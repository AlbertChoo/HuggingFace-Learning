{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "6Gjb2AVO1Zb-",
        "5OW5VLAI45-D",
        "UTVA5Kg7485U",
        "y9KD_jSk7Eol",
        "ozuoBa9N7Wr4",
        "XVz1R8aQ_TQ3",
        "J_-zl7O8_nJ_",
        "n2wnpcIJCRv2",
        "1tzR_S3OMBw-",
        "1voGHjXmPoSi",
        "3I4mAnYPTXTM"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0d99ccf47bc14bfc82bdd1615dc4b1af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2238d9add24f4f30b4fefc973a4bceac",
              "IPY_MODEL_f00ee6fe31c341a5a30025ff61ee0ceb",
              "IPY_MODEL_68cf701b61f94134a76723fe0cf4e8b2"
            ],
            "layout": "IPY_MODEL_e5a51ebcfc1b46a58b81b58351f3a376"
          }
        },
        "2238d9add24f4f30b4fefc973a4bceac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_19f1500156e24ad4b4393235e24e3b5b",
            "placeholder": "​",
            "style": "IPY_MODEL_6f12ec21a8654396bbd3b6c3366d6ac6",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "f00ee6fe31c341a5a30025ff61ee0ceb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_249574b21312417492d3aa92fd844b7f",
            "max": 48,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6263bfc41bb6449182fdaee12222371f",
            "value": 48
          }
        },
        "68cf701b61f94134a76723fe0cf4e8b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f6e403de869740279ec0d41db15869b9",
            "placeholder": "​",
            "style": "IPY_MODEL_7c93a3e766334501847149d047479728",
            "value": " 48.0/48.0 [00:00&lt;00:00, 2.70kB/s]"
          }
        },
        "e5a51ebcfc1b46a58b81b58351f3a376": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "19f1500156e24ad4b4393235e24e3b5b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6f12ec21a8654396bbd3b6c3366d6ac6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "249574b21312417492d3aa92fd844b7f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6263bfc41bb6449182fdaee12222371f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f6e403de869740279ec0d41db15869b9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7c93a3e766334501847149d047479728": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dbe2a36d8424418e94548f4c3a14765c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f4be6ad4171144f1b545068ab3b3d0bc",
              "IPY_MODEL_3a08cd192f15475d8c9155960e1a0490",
              "IPY_MODEL_f658ec6da65148fd90cf83e1b99e1853"
            ],
            "layout": "IPY_MODEL_70350178a4fb428db85c655994e859e7"
          }
        },
        "f4be6ad4171144f1b545068ab3b3d0bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7d6ec0542a0d4d2fb44fc23b49d35207",
            "placeholder": "​",
            "style": "IPY_MODEL_06016af164c84e2e8bfcc9bf8e517f65",
            "value": "vocab.txt: 100%"
          }
        },
        "3a08cd192f15475d8c9155960e1a0490": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3e78d9a6cd004b5482db1f8e8ff68f4b",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1fea61f680a84fb482dd5e9b3d73d23c",
            "value": 231508
          }
        },
        "f658ec6da65148fd90cf83e1b99e1853": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8cb6d8c67e804aa1b1430b214486c78f",
            "placeholder": "​",
            "style": "IPY_MODEL_a2d1fa75d7154a7980c96eaf7c17715a",
            "value": " 232k/232k [00:00&lt;00:00, 6.66MB/s]"
          }
        },
        "70350178a4fb428db85c655994e859e7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7d6ec0542a0d4d2fb44fc23b49d35207": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "06016af164c84e2e8bfcc9bf8e517f65": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3e78d9a6cd004b5482db1f8e8ff68f4b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1fea61f680a84fb482dd5e9b3d73d23c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8cb6d8c67e804aa1b1430b214486c78f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a2d1fa75d7154a7980c96eaf7c17715a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0734148601794077ac68625346a33e3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f75c998d013f4a94841659bd6f910c58",
              "IPY_MODEL_3f4d717681e64c4eaca3a509aee8809b",
              "IPY_MODEL_3bd42a5bc52c464788c2b4817c8a975b"
            ],
            "layout": "IPY_MODEL_b96b925c09da49ee9889429f46c4a3dc"
          }
        },
        "f75c998d013f4a94841659bd6f910c58": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4fce062cb6b846a1ab85d5e8599dd239",
            "placeholder": "​",
            "style": "IPY_MODEL_16e8264aff434cdeb0831a6e96116827",
            "value": "tokenizer.json: 100%"
          }
        },
        "3f4d717681e64c4eaca3a509aee8809b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0891076ea949460380dadb1eab354546",
            "max": 466062,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_68bca01415604410af32e816793a50a2",
            "value": 466062
          }
        },
        "3bd42a5bc52c464788c2b4817c8a975b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2c4ec2b7fae44b4bb88a086b907d9184",
            "placeholder": "​",
            "style": "IPY_MODEL_dd8717f36b2e4d09916402ea9413ecfa",
            "value": " 466k/466k [00:00&lt;00:00, 11.2MB/s]"
          }
        },
        "b96b925c09da49ee9889429f46c4a3dc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4fce062cb6b846a1ab85d5e8599dd239": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "16e8264aff434cdeb0831a6e96116827": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0891076ea949460380dadb1eab354546": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "68bca01415604410af32e816793a50a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2c4ec2b7fae44b4bb88a086b907d9184": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dd8717f36b2e4d09916402ea9413ecfa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c53bf2d3959248f48765f0d553424d53": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a814ed4d8f0149838104e99081aaf38e",
              "IPY_MODEL_c0968fe67109400fa462c34bbebcde40",
              "IPY_MODEL_a484923a10a04020b00e42babcad06f7"
            ],
            "layout": "IPY_MODEL_34efb49814ac4c0281fba39fd23c1b35"
          }
        },
        "a814ed4d8f0149838104e99081aaf38e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7eae0a33482c4dc1bcecde5cd630f519",
            "placeholder": "​",
            "style": "IPY_MODEL_fa830e0f8b8c463da40796dee5b08f1b",
            "value": "config.json: 100%"
          }
        },
        "c0968fe67109400fa462c34bbebcde40": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b342f8adc6a74ee9bda100c2bcad017f",
            "max": 570,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a577c732a51d449dab855915fa8a554d",
            "value": 570
          }
        },
        "a484923a10a04020b00e42babcad06f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d58b15612c294732933a9e56a7cd09c6",
            "placeholder": "​",
            "style": "IPY_MODEL_e065e324f1cb42f0a855b68d2c65de3e",
            "value": " 570/570 [00:00&lt;00:00, 21.8kB/s]"
          }
        },
        "34efb49814ac4c0281fba39fd23c1b35": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7eae0a33482c4dc1bcecde5cd630f519": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fa830e0f8b8c463da40796dee5b08f1b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b342f8adc6a74ee9bda100c2bcad017f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a577c732a51d449dab855915fa8a554d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d58b15612c294732933a9e56a7cd09c6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e065e324f1cb42f0a855b68d2c65de3e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "25ead5d631dd487ca2ff3a9f0ad2020b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9d974e9740ce40ddb152498571d7127b",
              "IPY_MODEL_541cb4a70c8844718aee05c27dfe0e5c",
              "IPY_MODEL_8d6579e0ba3e4e95b318febe32c4d1bc"
            ],
            "layout": "IPY_MODEL_d83f2c478e5d4bb5b85a30ae541eaa55"
          }
        },
        "9d974e9740ce40ddb152498571d7127b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_54786b392b894ec3b5f8349cc51c1554",
            "placeholder": "​",
            "style": "IPY_MODEL_fc84f4c59b474b4ca8f6fb6347e0f381",
            "value": "Map: 100%"
          }
        },
        "541cb4a70c8844718aee05c27dfe0e5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e2b35cb9aeb54a33b8c29d57778e1d0c",
            "max": 4204,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_9a6826cd622a4370a8c395a24d37e7a0",
            "value": 4204
          }
        },
        "8d6579e0ba3e4e95b318febe32c4d1bc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_49770acc727542d6b525a8438df10909",
            "placeholder": "​",
            "style": "IPY_MODEL_46bdc78c3f344856b72684bd0da15053",
            "value": " 4204/4204 [00:04&lt;00:00, 1000.04 examples/s]"
          }
        },
        "d83f2c478e5d4bb5b85a30ae541eaa55": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "54786b392b894ec3b5f8349cc51c1554": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fc84f4c59b474b4ca8f6fb6347e0f381": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e2b35cb9aeb54a33b8c29d57778e1d0c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9a6826cd622a4370a8c395a24d37e7a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "49770acc727542d6b525a8438df10909": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "46bdc78c3f344856b72684bd0da15053": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9005934ffeca491093c394dab1c85e81": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_dc27ac9fc396414db3dfe84acb8932c0",
              "IPY_MODEL_c397b57ee1b34c599522433435faceb7",
              "IPY_MODEL_bd45992f0996486fb9a853e4be8eaa39"
            ],
            "layout": "IPY_MODEL_9b703a20090b420b831f070fcda31c54"
          }
        },
        "dc27ac9fc396414db3dfe84acb8932c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d4c1e32fa5f148e58337b19e3d2c4f20",
            "placeholder": "​",
            "style": "IPY_MODEL_98355e935bc644b6989a547268707e69",
            "value": "Map: 100%"
          }
        },
        "c397b57ee1b34c599522433435faceb7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_245ea4463d844fb6a277495de71cfa08",
            "max": 1169,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_567caf3e27d0438fa13cf2d74ea3a1b6",
            "value": 1169
          }
        },
        "bd45992f0996486fb9a853e4be8eaa39": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e350d8e121094f4eae848d91eaaf09f9",
            "placeholder": "​",
            "style": "IPY_MODEL_35f746b7563c4c10be8327f115b77637",
            "value": " 1169/1169 [00:01&lt;00:00, 872.41 examples/s]"
          }
        },
        "9b703a20090b420b831f070fcda31c54": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d4c1e32fa5f148e58337b19e3d2c4f20": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "98355e935bc644b6989a547268707e69": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "245ea4463d844fb6a277495de71cfa08": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "567caf3e27d0438fa13cf2d74ea3a1b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e350d8e121094f4eae848d91eaaf09f9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "35f746b7563c4c10be8327f115b77637": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0f800aff70d543d099cfc7a2173bb021": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5d08ff3e89f243feb8f51a995451b834",
              "IPY_MODEL_a5f56e49fbf84c92af682ec6744557ed",
              "IPY_MODEL_0c1209b1497e40798ecbf627ebd599e9"
            ],
            "layout": "IPY_MODEL_64901768022143dd9c592882e017fe03"
          }
        },
        "5d08ff3e89f243feb8f51a995451b834": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d1e7089f69b94eeeb78b1fb6a503b78d",
            "placeholder": "​",
            "style": "IPY_MODEL_8084a8e0b31b452389fcdd8d697807b7",
            "value": "Map: 100%"
          }
        },
        "a5f56e49fbf84c92af682ec6744557ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fbcff3e931834882a70504b312347bca",
            "max": 468,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_dfba57c771634a518b1415127a4ec849",
            "value": 468
          }
        },
        "0c1209b1497e40798ecbf627ebd599e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_da638ada9b5d4cefbda3b3ab5e345eb5",
            "placeholder": "​",
            "style": "IPY_MODEL_2dd6195d72da49b19af78ce1d8f4b28d",
            "value": " 468/468 [00:00&lt;00:00, 1031.28 examples/s]"
          }
        },
        "64901768022143dd9c592882e017fe03": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d1e7089f69b94eeeb78b1fb6a503b78d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8084a8e0b31b452389fcdd8d697807b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fbcff3e931834882a70504b312347bca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dfba57c771634a518b1415127a4ec849": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "da638ada9b5d4cefbda3b3ab5e345eb5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2dd6195d72da49b19af78ce1d8f4b28d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b0c54b91e5714c6a99aa153651f18755": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fa600ad4f848443d9ffc156e298a868d",
              "IPY_MODEL_e31fcf8f082d4847ad4caa8fec99dabb",
              "IPY_MODEL_cb3c294692ad44e7aa07f212d68dbf72"
            ],
            "layout": "IPY_MODEL_7c5e9e17524c4d4b9c7ef673ba8e83ca"
          }
        },
        "fa600ad4f848443d9ffc156e298a868d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1c020a33ddf4408689e3a61d2d14d0cb",
            "placeholder": "​",
            "style": "IPY_MODEL_7895beb370094c2e8b9f4bee2c3b286b",
            "value": "Map: 100%"
          }
        },
        "e31fcf8f082d4847ad4caa8fec99dabb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d4d4755c8ab34e958dabe26b6ad818f8",
            "max": 4204,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a4e9699aef8b42fd969124b0d6ef8f4e",
            "value": 4204
          }
        },
        "cb3c294692ad44e7aa07f212d68dbf72": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4c955bf1be7f49e0a8539e3e581425fa",
            "placeholder": "​",
            "style": "IPY_MODEL_70165f2332084bfe8e56b84b1ca746c8",
            "value": " 4204/4204 [00:00&lt;00:00, 49341.16 examples/s]"
          }
        },
        "7c5e9e17524c4d4b9c7ef673ba8e83ca": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1c020a33ddf4408689e3a61d2d14d0cb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7895beb370094c2e8b9f4bee2c3b286b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d4d4755c8ab34e958dabe26b6ad818f8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a4e9699aef8b42fd969124b0d6ef8f4e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4c955bf1be7f49e0a8539e3e581425fa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "70165f2332084bfe8e56b84b1ca746c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d093a9b859504645820f379989d19245": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_db498bbbfa23432f9d0712eaa7895692",
              "IPY_MODEL_06bead29edf841b997d40cd4a86027c9",
              "IPY_MODEL_ec34489cf1c3421ca7f1b489912b57dd"
            ],
            "layout": "IPY_MODEL_028d249c5f3e49f991d7a2193c944279"
          }
        },
        "db498bbbfa23432f9d0712eaa7895692": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_13c93a3294a14185a7d416053609faa5",
            "placeholder": "​",
            "style": "IPY_MODEL_4c6846c599d74a90b18d99bc2b95b6be",
            "value": "Map: 100%"
          }
        },
        "06bead29edf841b997d40cd4a86027c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_52d3d52cc1d64b79b6f7c965b056a95d",
            "max": 1169,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c3afe3b9fb9f40d79f04c1bc48660e07",
            "value": 1169
          }
        },
        "ec34489cf1c3421ca7f1b489912b57dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d758dab5b6e94b45a4609b2caf666939",
            "placeholder": "​",
            "style": "IPY_MODEL_fbe1db74798e4883a0b1e03f9a0cc6ca",
            "value": " 1169/1169 [00:00&lt;00:00, 19779.66 examples/s]"
          }
        },
        "028d249c5f3e49f991d7a2193c944279": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "13c93a3294a14185a7d416053609faa5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4c6846c599d74a90b18d99bc2b95b6be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "52d3d52cc1d64b79b6f7c965b056a95d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c3afe3b9fb9f40d79f04c1bc48660e07": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d758dab5b6e94b45a4609b2caf666939": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fbe1db74798e4883a0b1e03f9a0cc6ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a5267fda695e4574b18c1a5e64d0469d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_dd619a65f74e490ca31596c24a798716",
              "IPY_MODEL_a46bdce4ff264072a017af01297cbc41",
              "IPY_MODEL_a1451942fa144105b9cd4ae292edc1e6"
            ],
            "layout": "IPY_MODEL_0124dcd404044346a7928fc4141d32ce"
          }
        },
        "dd619a65f74e490ca31596c24a798716": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f74e16304757488f8b78a540677ebd73",
            "placeholder": "​",
            "style": "IPY_MODEL_7f890285a9e646bebd2f2453630f311c",
            "value": "Map: 100%"
          }
        },
        "a46bdce4ff264072a017af01297cbc41": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_17877dd9c77d493d88a69212cfa4dad5",
            "max": 468,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f985acbea9154a5db0538b2a915bcdb8",
            "value": 468
          }
        },
        "a1451942fa144105b9cd4ae292edc1e6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a488872590e84240b09f29070c3e1ecc",
            "placeholder": "​",
            "style": "IPY_MODEL_912fad821c50452bb5df694b7fe6319b",
            "value": " 468/468 [00:00&lt;00:00, 9624.92 examples/s]"
          }
        },
        "0124dcd404044346a7928fc4141d32ce": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f74e16304757488f8b78a540677ebd73": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7f890285a9e646bebd2f2453630f311c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "17877dd9c77d493d88a69212cfa4dad5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f985acbea9154a5db0538b2a915bcdb8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a488872590e84240b09f29070c3e1ecc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "912fad821c50452bb5df694b7fe6319b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e7b6dea41d7d4d0ca41744352f1f610a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_85755f38f0ec440986487cc985f99b23",
              "IPY_MODEL_e70e4e0a1b904a749975edb82ef2c9c2",
              "IPY_MODEL_30fffd837fb04c489f26e0906a066d51"
            ],
            "layout": "IPY_MODEL_59a06f6630674a91a6feb7159689cc23"
          }
        },
        "85755f38f0ec440986487cc985f99b23": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6319f71fab3244199205e03dabfb5340",
            "placeholder": "​",
            "style": "IPY_MODEL_f7714dc1eccc4958b2d589999efb81f9",
            "value": "model.safetensors: 100%"
          }
        },
        "e70e4e0a1b904a749975edb82ef2c9c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e869d856290a426cb08ccf01ec7d6ac7",
            "max": 440449768,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_82f0bfda75994397b2bf15655bf55734",
            "value": 440449768
          }
        },
        "30fffd837fb04c489f26e0906a066d51": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_76affdc5ac5942108a9850dd821e9cd7",
            "placeholder": "​",
            "style": "IPY_MODEL_5d53210b40de477ca6ff0be6b4a52b2b",
            "value": " 440M/440M [00:04&lt;00:00, 94.8MB/s]"
          }
        },
        "59a06f6630674a91a6feb7159689cc23": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6319f71fab3244199205e03dabfb5340": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f7714dc1eccc4958b2d589999efb81f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e869d856290a426cb08ccf01ec7d6ac7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "82f0bfda75994397b2bf15655bf55734": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "76affdc5ac5942108a9850dd821e9cd7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5d53210b40de477ca6ff0be6b4a52b2b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Dependencies and Dataset"
      ],
      "metadata": {
        "id": "6Gjb2AVO1Zb-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dependencies"
      ],
      "metadata": {
        "id": "5OW5VLAI45-D"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B98SR6yrxfbZ",
        "outputId": "3296aea6-5a45-4c81-8b9c-a8248f5f530e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.48.3)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.49.0-py3-none-any.whl.metadata (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.17.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.28.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (2024.10.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Downloading transformers-4.49.0-py3-none-any.whl (10.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.0/10.0 MB\u001b[0m \u001b[31m22.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: transformers\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.48.3\n",
            "    Uninstalling transformers-4.48.3:\n",
            "      Successfully uninstalled transformers-4.48.3\n",
            "Successfully installed transformers-4.49.0\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.3.0)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.11/dist-packages (from accelerate) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (24.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate) (6.0.2)\n",
            "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (2.5.1+cu124)\n",
            "Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.28.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate) (0.5.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (3.17.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2024.10.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.5)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2.0.0->accelerate)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate) (2025.1.31)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m111.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m86.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m59.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m56.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ],
      "source": [
        "!pip install -U transformers\n",
        "!pip install -U accelerate"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Parsing and checking issues"
      ],
      "metadata": {
        "id": "UTVA5Kg7485U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Parse XML file"
      ],
      "metadata": {
        "id": "orWU_EMy3XlV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import xml.etree.ElementTree as ET\n",
        "import pandas as pd\n",
        "\n",
        "def parse_absa_xml(file_path):\n",
        "    tree = ET.parse(file_path)\n",
        "    root = tree.getroot()\n",
        "\n",
        "    data = []\n",
        "\n",
        "    for sentence in root.findall(\"sentence\"):\n",
        "        text = sentence.find(\"text\").text\n",
        "        aspects = sentence.find(\"aspectTerms\")\n",
        "\n",
        "        if aspects is not None:\n",
        "            for aspect in aspects.findall(\"aspectTerm\"):\n",
        "                aspect_term = aspect.get(\"term\")\n",
        "                polarity = aspect.get(\"polarity\")\n",
        "                data.append([text, aspect_term, polarity])\n",
        "\n",
        "    return pd.DataFrame(data, columns=[\"sentence\", \"aspect\", \"sentiment\"])\n",
        "\n",
        "# Load both datasets\n",
        "laptop_df = parse_absa_xml(\"/content/Laptop_Train_v2.xml\")\n",
        "restaurant_df = parse_absa_xml(\"/content/Restaurants_Train_v2.xml\")\n",
        "\n",
        "# Combine both datasets\n",
        "absa_df = pd.concat([laptop_df, restaurant_df], ignore_index=True)\n",
        "\n",
        "absa_df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dvpDVH5x25Es",
        "outputId": "d6f3aef7-55a6-4606-aa85-8279da2d8bbc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                            sentence          aspect sentiment\n",
              "0  I charge it at night and skip taking the cord ...            cord   neutral\n",
              "1  I charge it at night and skip taking the cord ...    battery life  positive\n",
              "2  The tech guy then said the service center does...  service center  negative\n",
              "3  The tech guy then said the service center does...    \"sales\" team  negative\n",
              "4  The tech guy then said the service center does...        tech guy   neutral"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-41113fb4-b275-4553-afbe-24e3f96de644\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>aspect</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>I charge it at night and skip taking the cord ...</td>\n",
              "      <td>cord</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>I charge it at night and skip taking the cord ...</td>\n",
              "      <td>battery life</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>The tech guy then said the service center does...</td>\n",
              "      <td>service center</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>The tech guy then said the service center does...</td>\n",
              "      <td>\"sales\" team</td>\n",
              "      <td>negative</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>The tech guy then said the service center does...</td>\n",
              "      <td>tech guy</td>\n",
              "      <td>neutral</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-41113fb4-b275-4553-afbe-24e3f96de644')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-41113fb4-b275-4553-afbe-24e3f96de644 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-41113fb4-b275-4553-afbe-24e3f96de644');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-f0a7e269-b4d0-4b0d-bb6e-15276c491bcd\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-f0a7e269-b4d0-4b0d-bb6e-15276c491bcd')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-f0a7e269-b4d0-4b0d-bb6e-15276c491bcd button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "absa_df",
              "summary": "{\n  \"name\": \"absa_df\",\n  \"rows\": 6051,\n  \"fields\": [\n    {\n      \"column\": \"sentence\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3501,\n        \"samples\": [\n          \"the programs are esay to use and are quick to process this computer works like a charm.\",\n          \"I've also had to have the keyboard replaced at my expense.\",\n          \"The food is amazing!!!!\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"aspect\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2304,\n        \"samples\": [\n          \"selecion of wines\",\n          \"repair service\",\n          \"flour\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sentiment\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          \"positive\",\n          \"conflict\",\n          \"neutral\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check For Issues (Imbalanced datasets for missing values)"
      ],
      "metadata": {
        "id": "RUY12LHF3p7k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(absa_df[\"sentiment\"].value_counts())  # Check class distribution\n",
        "print(absa_df.isnull().sum())  # Check for missing values"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vYHWe5FR3iZR",
        "outputId": "23ae83e7-ea23-4e5d-8ada-a3cb9d2e774a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sentiment\n",
            "positive    3151\n",
            "negative    1671\n",
            "neutral     1093\n",
            "conflict     136\n",
            "Name: count, dtype: int64\n",
            "sentence     0\n",
            "aspect       0\n",
            "sentiment    0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "absa_df.describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bGGz7wo652QF",
        "outputId": "53114ead-7de6-4dfa-dd8a-7af526ba004d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                                 sentence aspect sentiment\n",
              "count                                                6051   6051      6051\n",
              "unique                                               3501   2304         4\n",
              "top     There are several programs for school or offic...   food  positive\n",
              "freq                                                   13    357      3151"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c9608c79-2196-41d5-8d81-e90cc66ae3b7\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>aspect</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>6051</td>\n",
              "      <td>6051</td>\n",
              "      <td>6051</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>unique</th>\n",
              "      <td>3501</td>\n",
              "      <td>2304</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>top</th>\n",
              "      <td>There are several programs for school or offic...</td>\n",
              "      <td>food</td>\n",
              "      <td>positive</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>freq</th>\n",
              "      <td>13</td>\n",
              "      <td>357</td>\n",
              "      <td>3151</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c9608c79-2196-41d5-8d81-e90cc66ae3b7')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-c9608c79-2196-41d5-8d81-e90cc66ae3b7 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-c9608c79-2196-41d5-8d81-e90cc66ae3b7');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-e299e675-8d13-4ec1-83d2-61e544a4e009\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e299e675-8d13-4ec1-83d2-61e544a4e009')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-e299e675-8d13-4ec1-83d2-61e544a4e009 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"absa_df\",\n  \"rows\": 4,\n  \"fields\": [\n    {\n      \"column\": \"sentence\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          3501,\n          \"13\",\n          \"6051\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"aspect\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          2304,\n          \"357\",\n          \"6051\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sentiment\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 4,\n        \"samples\": [\n          4,\n          \"3151\",\n          \"6051\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Preparation Pipeline"
      ],
      "metadata": {
        "id": "y9KD_jSk7Eol"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Cleaning & Normalization"
      ],
      "metadata": {
        "id": "ozuoBa9N7Wr4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "id": "lyuLFC1k_Olw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ecf27c66-9c11-4ed6-8223-1b1432bad757"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-3.3.1-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.17.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (17.0.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.10.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.12)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.28.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.4.6)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.1.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.2.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.18.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.3.1-py3-none-any.whl (484 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m484.9/484.9 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, dill, multiprocess, datasets\n",
            "Successfully installed datasets-3.3.1 dill-0.3.8 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import xml.etree.ElementTree as ET\n",
        "import pandas as pd\n",
        "import re\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import BertTokenizer\n",
        "from datasets import Dataset, DatasetDict\n",
        "\n",
        "# Replace XML entities (example: &quot; to \")\n",
        "absa_df['sentence'] = absa_df['sentence'].str.replace('&quot;', '\"', regex=False)\n",
        "\n",
        "# Remove extra whitespace\n",
        "absa_df['sentence'] = absa_df['sentence'].apply(lambda x: re.sub(r'\\s+', ' ', x).strip())\n",
        "\n",
        "# Convert text to lowercase\n",
        "absa_df['sentence'] = absa_df['sentence'].str.lower()\n",
        "\n",
        "# Drop rows with the 'conflict' sentiment\n",
        "absa_df = absa_df[absa_df['sentiment'] != 'conflict']\n",
        "\n",
        "# Drop duplicate rows\n",
        "absa_df = absa_df.drop_duplicates()\n",
        "\n",
        "print(\"\\nSentiment distribution after cleaning (conflict dropped):\")\n",
        "print(absa_df[\"sentiment\"].value_counts())\n",
        "\n",
        "print(\"\\nData description:\")\n",
        "print(absa_df.describe())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jd8WCyf8_atA",
        "outputId": "74fefae6-3eff-443c-af89-fdf7a43442b7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sentiment distribution after cleaning (conflict dropped):\n",
            "sentiment\n",
            "positive    3121\n",
            "negative    1633\n",
            "neutral     1087\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Data description:\n",
            "                                                 sentence aspect sentiment\n",
            "count                                                5841   5841      5841\n",
            "unique                                               3429   2273         3\n",
            "top     there are several programs for school or offic...   food  positive\n",
            "freq                                                   13    339      3121\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Label Encoding"
      ],
      "metadata": {
        "id": "XVz1R8aQ_TQ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentiment_mapping = {\n",
        "    'positive': 1,\n",
        "    'negative': 0,\n",
        "    'neutral': 2\n",
        "}\n",
        "absa_df['sentiment_encoded'] = absa_df['sentiment'].map(sentiment_mapping)\n",
        "\n",
        "print(\"\\nUnique sentiment labels after encoding:\")\n",
        "print(absa_df[['sentiment', 'sentiment_encoded']].drop_duplicates())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7tFZixIj_DaI",
        "outputId": "6ce33d37-22d6-4fef-845a-7ee3fcf6bd83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Unique sentiment labels after encoding:\n",
            "  sentiment  sentiment_encoded\n",
            "0   neutral                  2\n",
            "1  positive                  1\n",
            "2  negative                  0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Splitting and Tokenization"
      ],
      "metadata": {
        "id": "J_-zl7O8_nJ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the DataFrame into training and testing sets (stratified)\n",
        "train_df, test_df = train_test_split(\n",
        "    absa_df,\n",
        "    test_size=0.2,\n",
        "    stratify=absa_df['sentiment'],\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "train_df, val_df = train_test_split(\n",
        "    train_df,\n",
        "    test_size=0.1,\n",
        "    stratify=train_df['sentiment'],\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(\"\\nTrain shape:\", train_df.shape,\n",
        "      \"Test shape:\", test_df.shape,\n",
        "      \"Validation shape:\", val_df.shape)\n",
        "\n",
        "# Convert the train and test DataFrames to Hugging Face Datasets\n",
        "train_dataset = Dataset.from_pandas(train_df.reset_index(drop=True))\n",
        "test_dataset = Dataset.from_pandas(test_df.reset_index(drop=True))\n",
        "validation_dataset = Dataset.from_pandas(val_df.reset_index(drop=True))\n",
        "\n",
        "# Create a DatasetDict with train and test splits\n",
        "dataset_dict = DatasetDict({\n",
        "    \"train\": train_dataset,\n",
        "    \"test\": test_dataset,\n",
        "    \"validation\": validation_dataset\n",
        "})\n",
        "\n",
        "dataset_dict"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K-CK70E2_pEP",
        "outputId": "3252bdb8-c434-42ad-edf2-a37bbf67a65e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train shape: (4204, 4) Test shape: (1169, 4) Validation shape: (468, 4)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['sentence', 'aspect', 'sentiment', 'sentiment_encoded'],\n",
              "        num_rows: 4204\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['sentence', 'aspect', 'sentiment', 'sentiment_encoded'],\n",
              "        num_rows: 1169\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['sentence', 'aspect', 'sentiment', 'sentiment_encoded'],\n",
              "        num_rows: 468\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset_dict['train'][0], dataset_dict['test'][0], dataset_dict['validation'][0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lu5N4PR4BXf4",
        "outputId": "4cd32037-0830-4952-c862-9a6b5c60a030"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "({'sentence': 'i hope to edit this in the next few hours, i am going to try to install my own copy of windows 7.',\n",
              "  'aspect': 'Windows 7',\n",
              "  'sentiment': 'neutral',\n",
              "  'sentiment_encoded': 2},\n",
              " {'sentence': 'monday nights are a bargain at the $28 prix fix - this includes a three course meal plus *three* glasses of wine paired with each course.',\n",
              "  'aspect': 'course',\n",
              "  'sentiment': 'neutral',\n",
              "  'sentiment_encoded': 2},\n",
              " {'sentence': \"after 2 tries by the waiter to take it away (we hadn't even looked at it yet, we had full beers yet to drink), the manager approached and told us they needed the table for people with reservations.\",\n",
              "  'aspect': 'beers',\n",
              "  'sentiment': 'neutral',\n",
              "  'sentiment_encoded': 2})"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "def tokenize_function(example):\n",
        "    return tokenizer(\n",
        "        example[\"sentence\"],\n",
        "        padding=\"max_length\",\n",
        "        truncation=True,\n",
        "        max_length=128\n",
        "    )\n",
        "\n",
        "# Apply tokenization to the entire dataset using map (batched processing)\n",
        "tokenized_datasets = dataset_dict.map(tokenize_function, batched=True)\n",
        "\n",
        "# Add a function to map the 'sentiment_encoded' to 'labels'\n",
        "def add_labels(example):\n",
        "    example[\"labels\"] = example[\"sentiment_encoded\"]\n",
        "    return example\n",
        "\n",
        "# Map the function over the dataset\n",
        "tokenized_datasets = tokenized_datasets.map(add_labels, batched=True)\n",
        "\n",
        "# (Optional) Remove columns that are no longer needed\n",
        "# Here, you may want to remove the raw text and the original sentiment columns\n",
        "tokenized_datasets = tokenized_datasets.remove_columns([\"sentence\",\n",
        "                                                        \"aspect\",\n",
        "                                                        \"sentiment\",\n",
        "                                                        \"sentiment_encoded\"])\n",
        "\n",
        "print(\"\\nTokenized train example:\")\n",
        "print(tokenized_datasets[\"train\"][0])\n",
        "\n",
        "tokenized_datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 756,
          "referenced_widgets": [
            "0d99ccf47bc14bfc82bdd1615dc4b1af",
            "2238d9add24f4f30b4fefc973a4bceac",
            "f00ee6fe31c341a5a30025ff61ee0ceb",
            "68cf701b61f94134a76723fe0cf4e8b2",
            "e5a51ebcfc1b46a58b81b58351f3a376",
            "19f1500156e24ad4b4393235e24e3b5b",
            "6f12ec21a8654396bbd3b6c3366d6ac6",
            "249574b21312417492d3aa92fd844b7f",
            "6263bfc41bb6449182fdaee12222371f",
            "f6e403de869740279ec0d41db15869b9",
            "7c93a3e766334501847149d047479728",
            "dbe2a36d8424418e94548f4c3a14765c",
            "f4be6ad4171144f1b545068ab3b3d0bc",
            "3a08cd192f15475d8c9155960e1a0490",
            "f658ec6da65148fd90cf83e1b99e1853",
            "70350178a4fb428db85c655994e859e7",
            "7d6ec0542a0d4d2fb44fc23b49d35207",
            "06016af164c84e2e8bfcc9bf8e517f65",
            "3e78d9a6cd004b5482db1f8e8ff68f4b",
            "1fea61f680a84fb482dd5e9b3d73d23c",
            "8cb6d8c67e804aa1b1430b214486c78f",
            "a2d1fa75d7154a7980c96eaf7c17715a",
            "0734148601794077ac68625346a33e3e",
            "f75c998d013f4a94841659bd6f910c58",
            "3f4d717681e64c4eaca3a509aee8809b",
            "3bd42a5bc52c464788c2b4817c8a975b",
            "b96b925c09da49ee9889429f46c4a3dc",
            "4fce062cb6b846a1ab85d5e8599dd239",
            "16e8264aff434cdeb0831a6e96116827",
            "0891076ea949460380dadb1eab354546",
            "68bca01415604410af32e816793a50a2",
            "2c4ec2b7fae44b4bb88a086b907d9184",
            "dd8717f36b2e4d09916402ea9413ecfa",
            "c53bf2d3959248f48765f0d553424d53",
            "a814ed4d8f0149838104e99081aaf38e",
            "c0968fe67109400fa462c34bbebcde40",
            "a484923a10a04020b00e42babcad06f7",
            "34efb49814ac4c0281fba39fd23c1b35",
            "7eae0a33482c4dc1bcecde5cd630f519",
            "fa830e0f8b8c463da40796dee5b08f1b",
            "b342f8adc6a74ee9bda100c2bcad017f",
            "a577c732a51d449dab855915fa8a554d",
            "d58b15612c294732933a9e56a7cd09c6",
            "e065e324f1cb42f0a855b68d2c65de3e",
            "25ead5d631dd487ca2ff3a9f0ad2020b",
            "9d974e9740ce40ddb152498571d7127b",
            "541cb4a70c8844718aee05c27dfe0e5c",
            "8d6579e0ba3e4e95b318febe32c4d1bc",
            "d83f2c478e5d4bb5b85a30ae541eaa55",
            "54786b392b894ec3b5f8349cc51c1554",
            "fc84f4c59b474b4ca8f6fb6347e0f381",
            "e2b35cb9aeb54a33b8c29d57778e1d0c",
            "9a6826cd622a4370a8c395a24d37e7a0",
            "49770acc727542d6b525a8438df10909",
            "46bdc78c3f344856b72684bd0da15053",
            "9005934ffeca491093c394dab1c85e81",
            "dc27ac9fc396414db3dfe84acb8932c0",
            "c397b57ee1b34c599522433435faceb7",
            "bd45992f0996486fb9a853e4be8eaa39",
            "9b703a20090b420b831f070fcda31c54",
            "d4c1e32fa5f148e58337b19e3d2c4f20",
            "98355e935bc644b6989a547268707e69",
            "245ea4463d844fb6a277495de71cfa08",
            "567caf3e27d0438fa13cf2d74ea3a1b6",
            "e350d8e121094f4eae848d91eaaf09f9",
            "35f746b7563c4c10be8327f115b77637",
            "0f800aff70d543d099cfc7a2173bb021",
            "5d08ff3e89f243feb8f51a995451b834",
            "a5f56e49fbf84c92af682ec6744557ed",
            "0c1209b1497e40798ecbf627ebd599e9",
            "64901768022143dd9c592882e017fe03",
            "d1e7089f69b94eeeb78b1fb6a503b78d",
            "8084a8e0b31b452389fcdd8d697807b7",
            "fbcff3e931834882a70504b312347bca",
            "dfba57c771634a518b1415127a4ec849",
            "da638ada9b5d4cefbda3b3ab5e345eb5",
            "2dd6195d72da49b19af78ce1d8f4b28d",
            "b0c54b91e5714c6a99aa153651f18755",
            "fa600ad4f848443d9ffc156e298a868d",
            "e31fcf8f082d4847ad4caa8fec99dabb",
            "cb3c294692ad44e7aa07f212d68dbf72",
            "7c5e9e17524c4d4b9c7ef673ba8e83ca",
            "1c020a33ddf4408689e3a61d2d14d0cb",
            "7895beb370094c2e8b9f4bee2c3b286b",
            "d4d4755c8ab34e958dabe26b6ad818f8",
            "a4e9699aef8b42fd969124b0d6ef8f4e",
            "4c955bf1be7f49e0a8539e3e581425fa",
            "70165f2332084bfe8e56b84b1ca746c8",
            "d093a9b859504645820f379989d19245",
            "db498bbbfa23432f9d0712eaa7895692",
            "06bead29edf841b997d40cd4a86027c9",
            "ec34489cf1c3421ca7f1b489912b57dd",
            "028d249c5f3e49f991d7a2193c944279",
            "13c93a3294a14185a7d416053609faa5",
            "4c6846c599d74a90b18d99bc2b95b6be",
            "52d3d52cc1d64b79b6f7c965b056a95d",
            "c3afe3b9fb9f40d79f04c1bc48660e07",
            "d758dab5b6e94b45a4609b2caf666939",
            "fbe1db74798e4883a0b1e03f9a0cc6ca",
            "a5267fda695e4574b18c1a5e64d0469d",
            "dd619a65f74e490ca31596c24a798716",
            "a46bdce4ff264072a017af01297cbc41",
            "a1451942fa144105b9cd4ae292edc1e6",
            "0124dcd404044346a7928fc4141d32ce",
            "f74e16304757488f8b78a540677ebd73",
            "7f890285a9e646bebd2f2453630f311c",
            "17877dd9c77d493d88a69212cfa4dad5",
            "f985acbea9154a5db0538b2a915bcdb8",
            "a488872590e84240b09f29070c3e1ecc",
            "912fad821c50452bb5df694b7fe6319b"
          ]
        },
        "id": "6pRCcIFKAAwn",
        "outputId": "d106742a-cda7-45ba-e850-ef4b8d76224d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0d99ccf47bc14bfc82bdd1615dc4b1af"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dbe2a36d8424418e94548f4c3a14765c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0734148601794077ac68625346a33e3e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c53bf2d3959248f48765f0d553424d53"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/4204 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "25ead5d631dd487ca2ff3a9f0ad2020b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/1169 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9005934ffeca491093c394dab1c85e81"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/468 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0f800aff70d543d099cfc7a2173bb021"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/4204 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b0c54b91e5714c6a99aa153651f18755"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/1169 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d093a9b859504645820f379989d19245"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/468 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a5267fda695e4574b18c1a5e64d0469d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Tokenized train example:\n",
            "{'input_ids': [101, 1045, 3246, 2000, 10086, 2023, 1999, 1996, 2279, 2261, 2847, 1010, 1045, 2572, 2183, 2000, 3046, 2000, 16500, 2026, 2219, 6100, 1997, 3645, 1021, 1012, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 2}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
              "        num_rows: 4204\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
              "        num_rows: 1169\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
              "        num_rows: 468\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model loading and Configuration"
      ],
      "metadata": {
        "id": "n2wnpcIJCRv2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertForSequenceClassification, TrainingArguments, Trainer\n",
        "from peft import get_peft_model, LoraConfig\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\",\n",
        "                                                      num_labels=3)\n",
        "\n",
        "# Configure LoRA parameters\n",
        "lora_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16, # a = r*2\n",
        "    target_modules=[\"query\", \"value\"],\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    task_type=\"SEQ_CLS\"\n",
        ")\n",
        "\n",
        "# Wrap your model with LoRA layers\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 101,
          "referenced_widgets": [
            "e7b6dea41d7d4d0ca41744352f1f610a",
            "85755f38f0ec440986487cc985f99b23",
            "e70e4e0a1b904a749975edb82ef2c9c2",
            "30fffd837fb04c489f26e0906a066d51",
            "59a06f6630674a91a6feb7159689cc23",
            "6319f71fab3244199205e03dabfb5340",
            "f7714dc1eccc4958b2d589999efb81f9",
            "e869d856290a426cb08ccf01ec7d6ac7",
            "82f0bfda75994397b2bf15655bf55734",
            "76affdc5ac5942108a9850dd821e9cd7",
            "5d53210b40de477ca6ff0be6b4a52b2b"
          ]
        },
        "id": "oarjnSHxCUdF",
        "outputId": "7eb9571c-0e16-40ba-fd53-21c43c90a4a1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e7b6dea41d7d4d0ca41744352f1f610a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 297,219 || all params: 109,781,766 || trainable%: 0.2707\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    # If logits are shape (num_samples, num_labels), get the predicted label by taking the argmax.\n",
        "    predictions = np.argmax(logits, axis=1)\n",
        "    acc = accuracy_score(labels, predictions)\n",
        "    prec = precision_score(labels, predictions, average='weighted')\n",
        "    rec = recall_score(labels, predictions, average='weighted')\n",
        "    f1 = f1_score(labels, predictions, average='weighted')\n",
        "    return {\"accuracy\": acc, \"precision\": prec, \"recall\": rec, \"f1\": f1}"
      ],
      "metadata": {
        "id": "wrC-5ygqEvvU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import DataCollatorWithPadding\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "# Define the training arguments\n",
        "training_args_lora = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=2, #mbs=2\n",
        "    gradient_accumulation_steps=16, # effective batch size 16*2\n",
        "    optim=\"adamw_torch\",\n",
        "    save_steps=200, # checkpoint every 200 steps\n",
        "    logging_steps=1,\n",
        "    learning_rate=2e-4, # step size in the optimizer update\n",
        "    weight_decay=0.001,\n",
        "    fp16=True, # 16 bit\n",
        "    bf16=False, # not supported on V100\n",
        "    max_grad_norm=0.3, #gradient clipping improves convergence\n",
        "    max_steps=-1,\n",
        "    warmup_ratio=0.03, # learning rate warmup\n",
        "    group_by_length=True,\n",
        "    lr_scheduler_type=\"cosine\" # cosine lr scheduler\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args_lora,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "import gc # garbage collection\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache() # clean cache\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "zWjURCz-C44n",
        "outputId": "2289030e-30a1-48ec-b079-9f0d5951606e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-16-bb8de300ab75>:26: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer = Trainer(\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmahkotasteam\u001b[0m (\u001b[33mmahkotasteam-asia-pacific-university-of-technology-innov\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.6"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250216_141859-tx4bwkhk</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mahkotasteam-asia-pacific-university-of-technology-innov/huggingface/runs/tx4bwkhk' target=\"_blank\">./results</a></strong> to <a href='https://wandb.ai/mahkotasteam-asia-pacific-university-of-technology-innov/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/mahkotasteam-asia-pacific-university-of-technology-innov/huggingface' target=\"_blank\">https://wandb.ai/mahkotasteam-asia-pacific-university-of-technology-innov/huggingface</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/mahkotasteam-asia-pacific-university-of-technology-innov/huggingface/runs/tx4bwkhk' target=\"_blank\">https://wandb.ai/mahkotasteam-asia-pacific-university-of-technology-innov/huggingface/runs/tx4bwkhk</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='393' max='393' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [393/393 04:43, Epoch 2/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.074100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.995100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.982800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.039500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.944200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>1.065000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.975400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>1.035600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>0.933100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>0.929700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>1.013600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>1.043800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>0.941800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>0.980300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>1.154700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>1.137300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>0.979600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>0.941700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>1.048800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.988100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>1.024400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>1.004000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>0.838400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>0.975900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>1.110100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>0.940400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>0.965300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>0.949700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>0.953700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.948900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>0.925500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>1.104700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>0.912900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>1.019800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>1.095900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>1.025800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>0.942600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>1.095500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>0.883100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.988200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41</td>\n",
              "      <td>0.939200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>0.797600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43</td>\n",
              "      <td>1.001600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>0.868600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>1.109200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46</td>\n",
              "      <td>0.999100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47</td>\n",
              "      <td>1.064800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>0.955900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49</td>\n",
              "      <td>0.878300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.958600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>51</td>\n",
              "      <td>0.914000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52</td>\n",
              "      <td>0.913500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>53</td>\n",
              "      <td>0.868600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>54</td>\n",
              "      <td>0.955100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55</td>\n",
              "      <td>0.920900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>0.734100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>57</td>\n",
              "      <td>0.786800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>58</td>\n",
              "      <td>0.889000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>59</td>\n",
              "      <td>0.850700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>1.064000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>61</td>\n",
              "      <td>0.903400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>62</td>\n",
              "      <td>0.878400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>63</td>\n",
              "      <td>0.887400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>64</td>\n",
              "      <td>0.749700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>65</td>\n",
              "      <td>0.914600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>66</td>\n",
              "      <td>0.810000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>67</td>\n",
              "      <td>0.931900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>68</td>\n",
              "      <td>0.683200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>69</td>\n",
              "      <td>0.969600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>0.858100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>71</td>\n",
              "      <td>0.972300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>72</td>\n",
              "      <td>0.865700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>73</td>\n",
              "      <td>0.807200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>74</td>\n",
              "      <td>0.964300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>75</td>\n",
              "      <td>0.694900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>76</td>\n",
              "      <td>0.842700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>77</td>\n",
              "      <td>0.896300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>78</td>\n",
              "      <td>0.692300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>79</td>\n",
              "      <td>0.892300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>0.815200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>81</td>\n",
              "      <td>1.073100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>82</td>\n",
              "      <td>0.730800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>83</td>\n",
              "      <td>0.766100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>84</td>\n",
              "      <td>0.741800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>85</td>\n",
              "      <td>0.960600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>86</td>\n",
              "      <td>0.744200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>87</td>\n",
              "      <td>0.738900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>88</td>\n",
              "      <td>0.561700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>89</td>\n",
              "      <td>0.760500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>0.770500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>91</td>\n",
              "      <td>0.992200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>92</td>\n",
              "      <td>0.979600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>93</td>\n",
              "      <td>0.952600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>94</td>\n",
              "      <td>0.773200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>95</td>\n",
              "      <td>0.712700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>96</td>\n",
              "      <td>0.771700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>97</td>\n",
              "      <td>0.696000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>98</td>\n",
              "      <td>1.029000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>99</td>\n",
              "      <td>0.666000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.781500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>101</td>\n",
              "      <td>0.933400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>102</td>\n",
              "      <td>0.690000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>103</td>\n",
              "      <td>0.846200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>104</td>\n",
              "      <td>0.926400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>105</td>\n",
              "      <td>0.561400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>106</td>\n",
              "      <td>0.826800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>107</td>\n",
              "      <td>0.732700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>108</td>\n",
              "      <td>0.478200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>109</td>\n",
              "      <td>0.719600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>0.596900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>111</td>\n",
              "      <td>0.779800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>112</td>\n",
              "      <td>0.859500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>113</td>\n",
              "      <td>0.803600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>114</td>\n",
              "      <td>0.485500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>115</td>\n",
              "      <td>0.855500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>116</td>\n",
              "      <td>0.790900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>117</td>\n",
              "      <td>0.816000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>118</td>\n",
              "      <td>0.448500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>119</td>\n",
              "      <td>0.474800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>0.782100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>121</td>\n",
              "      <td>0.667000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>122</td>\n",
              "      <td>0.584000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>123</td>\n",
              "      <td>0.571300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>124</td>\n",
              "      <td>0.711700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>125</td>\n",
              "      <td>0.642100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>126</td>\n",
              "      <td>0.820800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>127</td>\n",
              "      <td>0.788300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>128</td>\n",
              "      <td>0.751600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>129</td>\n",
              "      <td>0.733700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>0.719400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>131</td>\n",
              "      <td>0.681100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>132</td>\n",
              "      <td>0.319800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>133</td>\n",
              "      <td>0.749000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>134</td>\n",
              "      <td>0.804600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>135</td>\n",
              "      <td>0.835600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>136</td>\n",
              "      <td>0.796400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>137</td>\n",
              "      <td>0.945400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>138</td>\n",
              "      <td>0.409800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>139</td>\n",
              "      <td>0.801800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>0.594600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>141</td>\n",
              "      <td>0.839300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>142</td>\n",
              "      <td>0.716700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>143</td>\n",
              "      <td>0.817200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>144</td>\n",
              "      <td>0.635100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>145</td>\n",
              "      <td>0.558300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>146</td>\n",
              "      <td>0.650600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>147</td>\n",
              "      <td>0.676700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>148</td>\n",
              "      <td>0.334500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>149</td>\n",
              "      <td>0.768100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.501000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>151</td>\n",
              "      <td>0.812400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>152</td>\n",
              "      <td>0.533400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>153</td>\n",
              "      <td>0.702500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>154</td>\n",
              "      <td>0.394500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>155</td>\n",
              "      <td>0.458200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>156</td>\n",
              "      <td>0.857100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>157</td>\n",
              "      <td>0.656400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>158</td>\n",
              "      <td>0.399900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>159</td>\n",
              "      <td>0.518200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>0.642400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>161</td>\n",
              "      <td>0.719400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>162</td>\n",
              "      <td>0.564500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>163</td>\n",
              "      <td>0.474100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>164</td>\n",
              "      <td>0.646700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>165</td>\n",
              "      <td>0.776400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>166</td>\n",
              "      <td>0.830000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>167</td>\n",
              "      <td>0.656600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>168</td>\n",
              "      <td>0.638000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>169</td>\n",
              "      <td>1.048300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>0.588100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>171</td>\n",
              "      <td>0.604000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>172</td>\n",
              "      <td>0.593800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>173</td>\n",
              "      <td>0.779600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>174</td>\n",
              "      <td>0.727200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>175</td>\n",
              "      <td>0.635000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>176</td>\n",
              "      <td>0.544600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>177</td>\n",
              "      <td>0.565600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>178</td>\n",
              "      <td>0.487100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>179</td>\n",
              "      <td>0.541200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>0.689800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>181</td>\n",
              "      <td>0.738900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>182</td>\n",
              "      <td>0.607800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>183</td>\n",
              "      <td>0.661900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>184</td>\n",
              "      <td>0.595900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>185</td>\n",
              "      <td>0.898600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>186</td>\n",
              "      <td>0.688900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>187</td>\n",
              "      <td>0.486500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>188</td>\n",
              "      <td>0.925300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>189</td>\n",
              "      <td>0.585000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>190</td>\n",
              "      <td>0.728600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>191</td>\n",
              "      <td>0.884500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>192</td>\n",
              "      <td>0.903100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>193</td>\n",
              "      <td>0.590000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>194</td>\n",
              "      <td>0.741200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>195</td>\n",
              "      <td>0.387800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>196</td>\n",
              "      <td>0.542400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>197</td>\n",
              "      <td>0.982900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>198</td>\n",
              "      <td>0.760600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>199</td>\n",
              "      <td>0.841200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.788200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>201</td>\n",
              "      <td>0.621400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>202</td>\n",
              "      <td>0.742600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>203</td>\n",
              "      <td>0.725200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>204</td>\n",
              "      <td>0.929500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>205</td>\n",
              "      <td>0.625800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>206</td>\n",
              "      <td>0.675500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>207</td>\n",
              "      <td>0.659000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>208</td>\n",
              "      <td>0.781900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>209</td>\n",
              "      <td>0.642000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>210</td>\n",
              "      <td>0.556500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>211</td>\n",
              "      <td>0.677200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>212</td>\n",
              "      <td>0.460400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>213</td>\n",
              "      <td>0.738300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>214</td>\n",
              "      <td>0.515900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>215</td>\n",
              "      <td>0.745300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>216</td>\n",
              "      <td>0.708300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>217</td>\n",
              "      <td>0.617000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>218</td>\n",
              "      <td>1.008700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>219</td>\n",
              "      <td>0.521600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>220</td>\n",
              "      <td>0.730800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>221</td>\n",
              "      <td>0.443400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>222</td>\n",
              "      <td>0.768500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>223</td>\n",
              "      <td>0.741600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>224</td>\n",
              "      <td>0.721000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>225</td>\n",
              "      <td>0.620100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>226</td>\n",
              "      <td>0.594400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>227</td>\n",
              "      <td>0.623400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>228</td>\n",
              "      <td>0.479800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>229</td>\n",
              "      <td>0.503700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>230</td>\n",
              "      <td>0.694000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>231</td>\n",
              "      <td>0.598900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>232</td>\n",
              "      <td>0.644200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>233</td>\n",
              "      <td>0.907200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>234</td>\n",
              "      <td>0.393800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>235</td>\n",
              "      <td>0.592000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>236</td>\n",
              "      <td>0.615500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>237</td>\n",
              "      <td>0.615300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>238</td>\n",
              "      <td>0.682800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>239</td>\n",
              "      <td>0.629400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>240</td>\n",
              "      <td>0.738500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>241</td>\n",
              "      <td>0.799500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>242</td>\n",
              "      <td>0.537600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>243</td>\n",
              "      <td>0.501900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>244</td>\n",
              "      <td>0.613100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>245</td>\n",
              "      <td>0.496200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>246</td>\n",
              "      <td>0.636500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>247</td>\n",
              "      <td>0.670000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>248</td>\n",
              "      <td>0.632600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>249</td>\n",
              "      <td>0.711300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>0.664000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>251</td>\n",
              "      <td>1.033600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>252</td>\n",
              "      <td>0.633900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>253</td>\n",
              "      <td>0.602700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>254</td>\n",
              "      <td>0.783500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>255</td>\n",
              "      <td>0.609100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>256</td>\n",
              "      <td>0.615800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>257</td>\n",
              "      <td>0.797600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>258</td>\n",
              "      <td>0.641400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>259</td>\n",
              "      <td>0.751700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>260</td>\n",
              "      <td>0.706500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>261</td>\n",
              "      <td>0.710500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>262</td>\n",
              "      <td>0.556000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>263</td>\n",
              "      <td>0.747300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>264</td>\n",
              "      <td>0.322000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>265</td>\n",
              "      <td>0.764000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>266</td>\n",
              "      <td>0.469500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>267</td>\n",
              "      <td>0.793700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>268</td>\n",
              "      <td>0.590500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>269</td>\n",
              "      <td>0.750400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>270</td>\n",
              "      <td>0.721000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>271</td>\n",
              "      <td>0.656800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>272</td>\n",
              "      <td>0.785100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>273</td>\n",
              "      <td>0.767900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>274</td>\n",
              "      <td>0.759400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>275</td>\n",
              "      <td>0.767900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>276</td>\n",
              "      <td>0.607800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>277</td>\n",
              "      <td>0.568100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>278</td>\n",
              "      <td>0.537200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>279</td>\n",
              "      <td>0.589700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>280</td>\n",
              "      <td>0.767300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>281</td>\n",
              "      <td>0.723100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>282</td>\n",
              "      <td>0.549800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>283</td>\n",
              "      <td>0.675900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>284</td>\n",
              "      <td>0.670200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>285</td>\n",
              "      <td>0.738800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>286</td>\n",
              "      <td>0.551900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>287</td>\n",
              "      <td>0.767200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>288</td>\n",
              "      <td>0.637500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>289</td>\n",
              "      <td>0.594000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>290</td>\n",
              "      <td>0.782500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>291</td>\n",
              "      <td>0.617000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>292</td>\n",
              "      <td>0.678100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>293</td>\n",
              "      <td>0.429800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>294</td>\n",
              "      <td>0.485600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>295</td>\n",
              "      <td>0.787100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>296</td>\n",
              "      <td>0.555500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>297</td>\n",
              "      <td>0.724800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>298</td>\n",
              "      <td>0.610800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>299</td>\n",
              "      <td>0.805100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.582500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>301</td>\n",
              "      <td>1.127200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>302</td>\n",
              "      <td>0.463100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>303</td>\n",
              "      <td>0.745600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>304</td>\n",
              "      <td>0.595100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>305</td>\n",
              "      <td>0.837100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>306</td>\n",
              "      <td>0.733300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>307</td>\n",
              "      <td>0.310600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>308</td>\n",
              "      <td>0.795000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>309</td>\n",
              "      <td>0.510500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>310</td>\n",
              "      <td>0.651700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>311</td>\n",
              "      <td>0.608600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>312</td>\n",
              "      <td>0.758900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>313</td>\n",
              "      <td>0.591900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>314</td>\n",
              "      <td>0.388900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>315</td>\n",
              "      <td>0.598800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>316</td>\n",
              "      <td>0.460300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>317</td>\n",
              "      <td>0.586900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>318</td>\n",
              "      <td>0.538400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>319</td>\n",
              "      <td>0.538400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>320</td>\n",
              "      <td>0.766100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>321</td>\n",
              "      <td>0.731600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>322</td>\n",
              "      <td>0.306100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>323</td>\n",
              "      <td>0.702900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>324</td>\n",
              "      <td>0.585700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>325</td>\n",
              "      <td>0.619300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>326</td>\n",
              "      <td>0.647700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>327</td>\n",
              "      <td>0.337600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>328</td>\n",
              "      <td>0.785000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>329</td>\n",
              "      <td>0.693700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>330</td>\n",
              "      <td>0.653000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>331</td>\n",
              "      <td>0.520200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>332</td>\n",
              "      <td>0.884000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>333</td>\n",
              "      <td>0.654500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>334</td>\n",
              "      <td>0.519200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>335</td>\n",
              "      <td>0.615700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>336</td>\n",
              "      <td>0.573000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>337</td>\n",
              "      <td>0.626900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>338</td>\n",
              "      <td>0.573200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>339</td>\n",
              "      <td>0.682800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>340</td>\n",
              "      <td>0.702400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>341</td>\n",
              "      <td>0.619900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>342</td>\n",
              "      <td>0.548500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>343</td>\n",
              "      <td>0.685500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>344</td>\n",
              "      <td>0.462200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>345</td>\n",
              "      <td>0.514000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>346</td>\n",
              "      <td>0.824300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>347</td>\n",
              "      <td>0.595400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>348</td>\n",
              "      <td>0.585800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>349</td>\n",
              "      <td>0.516100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>0.739600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>351</td>\n",
              "      <td>0.573500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>352</td>\n",
              "      <td>0.766500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>353</td>\n",
              "      <td>0.697000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>354</td>\n",
              "      <td>0.524200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>355</td>\n",
              "      <td>0.606500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>356</td>\n",
              "      <td>0.646200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>357</td>\n",
              "      <td>0.616900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>358</td>\n",
              "      <td>0.473800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>359</td>\n",
              "      <td>0.598500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>360</td>\n",
              "      <td>0.785200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>361</td>\n",
              "      <td>0.631100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>362</td>\n",
              "      <td>0.651000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>363</td>\n",
              "      <td>0.465100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>364</td>\n",
              "      <td>0.393100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>365</td>\n",
              "      <td>0.698400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>366</td>\n",
              "      <td>0.765800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>367</td>\n",
              "      <td>0.592700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>368</td>\n",
              "      <td>0.730100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>369</td>\n",
              "      <td>0.873000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>370</td>\n",
              "      <td>0.409500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>371</td>\n",
              "      <td>0.687300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>372</td>\n",
              "      <td>0.414000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>373</td>\n",
              "      <td>0.695400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>374</td>\n",
              "      <td>0.683600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>375</td>\n",
              "      <td>0.772500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>376</td>\n",
              "      <td>0.741000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>377</td>\n",
              "      <td>0.676800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>378</td>\n",
              "      <td>0.560100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>379</td>\n",
              "      <td>0.877000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>380</td>\n",
              "      <td>0.415700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>381</td>\n",
              "      <td>0.691900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>382</td>\n",
              "      <td>0.409100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>383</td>\n",
              "      <td>0.528400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>384</td>\n",
              "      <td>0.548400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>385</td>\n",
              "      <td>0.881100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>386</td>\n",
              "      <td>0.517800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>387</td>\n",
              "      <td>0.716000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>388</td>\n",
              "      <td>0.421300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>389</td>\n",
              "      <td>0.577300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>390</td>\n",
              "      <td>0.533600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>391</td>\n",
              "      <td>0.783900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>392</td>\n",
              "      <td>0.499100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>393</td>\n",
              "      <td>0.728900</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=393, training_loss=0.7220549801833757, metrics={'train_runtime': 305.4572, 'train_samples_per_second': 41.289, 'train_steps_per_second': 1.287, 'total_flos': 827458971070464.0, 'train_loss': 0.7220549801833757, 'epoch': 2.981921979067555})"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.evaluate()"
      ],
      "metadata": {
        "id": "z2hH2BdQFM1m",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "outputId": "d5edefeb-6f46-4bc3-9c1b-bc8605bfe976"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='59' max='59' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [59/59 00:01]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'eval_loss': 0.6324132084846497,\n",
              " 'eval_accuracy': 0.7371794871794872,\n",
              " 'eval_precision': 0.7216208342884588,\n",
              " 'eval_recall': 0.7371794871794872,\n",
              " 'eval_f1': 0.6919703755352902,\n",
              " 'eval_runtime': 1.3264,\n",
              " 'eval_samples_per_second': 352.837,\n",
              " 'eval_steps_per_second': 44.482,\n",
              " 'epoch': 2.981921979067555}"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LoRA-2 with r and a increased"
      ],
      "metadata": {
        "id": "1tzR_S3OMBw-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_2 = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\",\n",
        "                                                      num_labels=3)\n",
        "\n",
        "# Configure LoRA parameters\n",
        "lora_config_2 = LoraConfig(\n",
        "    r=16,\n",
        "    lora_alpha=32, # a = r*2\n",
        "    target_modules=[\"query\", \"value\"],\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    task_type=\"SEQ_CLS\"\n",
        ")\n",
        "\n",
        "# Wrap your model with LoRA layers\n",
        "model_2 = get_peft_model(model_2, lora_config_2)\n",
        "model_2.print_trainable_parameters()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M0RtPsvTLzly",
        "outputId": "1f0b9c9e-3afc-441c-fdf1-909ac6c135ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 592,131 || all params: 110,076,678 || trainable%: 0.5379\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "# Define the training arguments\n",
        "training_args_lora_2 = TrainingArguments(\n",
        "    output_dir=\"./results-model_2\",\n",
        "    num_train_epochs=5,\n",
        "    per_device_train_batch_size=2, #mbs=2\n",
        "    gradient_accumulation_steps=16, # effective batch size 16*2\n",
        "    optim=\"adamw_torch\",\n",
        "    save_steps=200, # checkpoint every 200 steps\n",
        "    logging_steps=1,\n",
        "    learning_rate=2e-4, # step size in the optimizer update\n",
        "    weight_decay=0.001,\n",
        "    fp16=True, # 16 bit\n",
        "    bf16=False, # not supported on V100\n",
        "    max_grad_norm=0.3, #gradient clipping improves convergence\n",
        "    max_steps=-1,\n",
        "    warmup_ratio=0.03, # learning rate warmup\n",
        "    group_by_length=True,\n",
        "    lr_scheduler_type=\"cosine\" # cosine lr scheduler\n",
        ")\n",
        "\n",
        "trainer_2 = Trainer(\n",
        "    model=model_2,\n",
        "    args=training_args_lora_2,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "import gc # garbage collection\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache() # clean cache\n",
        "\n",
        "trainer_2.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "GX4A9WszMAxO",
        "outputId": "edf9ecef-430a-4fc8-dae7-0481b3d738e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-20-ef82125499c9>:23: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer_2 = Trainer(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='655' max='655' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [655/655 08:17, Epoch 4/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.185400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.207900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.147400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.120600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>1.217000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>1.144200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>1.202800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>1.128400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>1.140700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.136000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>1.128100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>1.135300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>1.070100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>1.055700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>1.082500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>1.084900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>1.018500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>0.977200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>1.051500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>1.043800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>1.022200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>1.008200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>0.895900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>0.987600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>1.140900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>0.940100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>0.908500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>0.918400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>0.975700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.899900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>0.923800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>1.141300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>0.879700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>1.043800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>1.102500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>1.024700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>0.923800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>1.061700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>0.930600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>1.006700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41</td>\n",
              "      <td>0.928800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>0.802000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43</td>\n",
              "      <td>0.997900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>0.833200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>1.096700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46</td>\n",
              "      <td>1.002300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47</td>\n",
              "      <td>0.976800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>0.952800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49</td>\n",
              "      <td>0.856800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.927300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>51</td>\n",
              "      <td>0.918400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52</td>\n",
              "      <td>0.926300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>53</td>\n",
              "      <td>0.785900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>54</td>\n",
              "      <td>0.978400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55</td>\n",
              "      <td>0.882800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>0.760300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>57</td>\n",
              "      <td>0.730400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>58</td>\n",
              "      <td>0.893100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>59</td>\n",
              "      <td>0.872600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>1.177000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>61</td>\n",
              "      <td>0.933000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>62</td>\n",
              "      <td>0.823100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>63</td>\n",
              "      <td>0.818200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>64</td>\n",
              "      <td>0.800000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>65</td>\n",
              "      <td>0.852600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>66</td>\n",
              "      <td>0.834000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>67</td>\n",
              "      <td>0.908500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>68</td>\n",
              "      <td>0.713400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>69</td>\n",
              "      <td>0.938900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>0.865200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>71</td>\n",
              "      <td>0.877900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>72</td>\n",
              "      <td>0.855200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>73</td>\n",
              "      <td>0.824600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>74</td>\n",
              "      <td>1.047300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>75</td>\n",
              "      <td>0.632100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>76</td>\n",
              "      <td>0.850500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>77</td>\n",
              "      <td>0.721000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>78</td>\n",
              "      <td>0.612000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>79</td>\n",
              "      <td>0.850700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>0.799200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>81</td>\n",
              "      <td>0.989600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>82</td>\n",
              "      <td>0.737000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>83</td>\n",
              "      <td>0.710000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>84</td>\n",
              "      <td>0.734500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>85</td>\n",
              "      <td>0.895600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>86</td>\n",
              "      <td>0.725700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>87</td>\n",
              "      <td>0.603900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>88</td>\n",
              "      <td>0.534100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>89</td>\n",
              "      <td>0.692200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>0.743100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>91</td>\n",
              "      <td>0.959600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>92</td>\n",
              "      <td>0.937600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>93</td>\n",
              "      <td>0.850000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>94</td>\n",
              "      <td>0.748600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>95</td>\n",
              "      <td>0.670500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>96</td>\n",
              "      <td>0.752500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>97</td>\n",
              "      <td>0.639100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>98</td>\n",
              "      <td>0.943100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>99</td>\n",
              "      <td>0.565500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.766200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>101</td>\n",
              "      <td>0.919600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>102</td>\n",
              "      <td>0.629100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>103</td>\n",
              "      <td>0.888300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>104</td>\n",
              "      <td>1.002600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>105</td>\n",
              "      <td>0.413300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>106</td>\n",
              "      <td>0.813000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>107</td>\n",
              "      <td>0.727400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>108</td>\n",
              "      <td>0.414700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>109</td>\n",
              "      <td>0.667300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>0.543700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>111</td>\n",
              "      <td>0.757100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>112</td>\n",
              "      <td>0.843000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>113</td>\n",
              "      <td>0.748100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>114</td>\n",
              "      <td>0.481500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>115</td>\n",
              "      <td>0.853000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>116</td>\n",
              "      <td>0.763800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>117</td>\n",
              "      <td>0.654300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>118</td>\n",
              "      <td>0.443000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>119</td>\n",
              "      <td>0.527600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>0.864700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>121</td>\n",
              "      <td>0.704700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>122</td>\n",
              "      <td>0.526200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>123</td>\n",
              "      <td>0.519100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>124</td>\n",
              "      <td>0.798500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>125</td>\n",
              "      <td>0.678700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>126</td>\n",
              "      <td>0.744500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>127</td>\n",
              "      <td>0.814500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>128</td>\n",
              "      <td>0.628000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>129</td>\n",
              "      <td>0.763100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>0.713400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>131</td>\n",
              "      <td>0.668800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>132</td>\n",
              "      <td>0.313000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>133</td>\n",
              "      <td>0.699800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>134</td>\n",
              "      <td>0.809100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>135</td>\n",
              "      <td>0.824200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>136</td>\n",
              "      <td>0.838600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>137</td>\n",
              "      <td>0.809900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>138</td>\n",
              "      <td>0.399900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>139</td>\n",
              "      <td>0.774600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>0.557600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>141</td>\n",
              "      <td>0.767500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>142</td>\n",
              "      <td>0.748100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>143</td>\n",
              "      <td>0.777700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>144</td>\n",
              "      <td>0.632600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>145</td>\n",
              "      <td>0.579700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>146</td>\n",
              "      <td>0.639700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>147</td>\n",
              "      <td>0.714100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>148</td>\n",
              "      <td>0.363800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>149</td>\n",
              "      <td>0.767600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.506000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>151</td>\n",
              "      <td>0.801000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>152</td>\n",
              "      <td>0.588300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>153</td>\n",
              "      <td>0.707200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>154</td>\n",
              "      <td>0.373400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>155</td>\n",
              "      <td>0.450300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>156</td>\n",
              "      <td>0.863400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>157</td>\n",
              "      <td>0.640800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>158</td>\n",
              "      <td>0.405900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>159</td>\n",
              "      <td>0.550100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>0.596800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>161</td>\n",
              "      <td>0.657300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>162</td>\n",
              "      <td>0.596000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>163</td>\n",
              "      <td>0.495100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>164</td>\n",
              "      <td>0.651000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>165</td>\n",
              "      <td>0.842900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>166</td>\n",
              "      <td>0.833300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>167</td>\n",
              "      <td>0.684500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>168</td>\n",
              "      <td>0.599100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>169</td>\n",
              "      <td>0.874200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>0.540100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>171</td>\n",
              "      <td>0.628900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>172</td>\n",
              "      <td>0.589400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>173</td>\n",
              "      <td>0.710700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>174</td>\n",
              "      <td>0.712100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>175</td>\n",
              "      <td>0.620100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>176</td>\n",
              "      <td>0.618000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>177</td>\n",
              "      <td>0.612100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>178</td>\n",
              "      <td>0.497100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>179</td>\n",
              "      <td>0.494300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>0.645500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>181</td>\n",
              "      <td>0.736500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>182</td>\n",
              "      <td>0.570700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>183</td>\n",
              "      <td>0.633100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>184</td>\n",
              "      <td>0.645500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>185</td>\n",
              "      <td>0.952100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>186</td>\n",
              "      <td>0.667500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>187</td>\n",
              "      <td>0.394700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>188</td>\n",
              "      <td>0.916800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>189</td>\n",
              "      <td>0.565000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>190</td>\n",
              "      <td>0.650000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>191</td>\n",
              "      <td>0.791400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>192</td>\n",
              "      <td>0.847700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>193</td>\n",
              "      <td>0.586000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>194</td>\n",
              "      <td>0.786200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>195</td>\n",
              "      <td>0.441000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>196</td>\n",
              "      <td>0.588100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>197</td>\n",
              "      <td>1.064000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>198</td>\n",
              "      <td>0.765900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>199</td>\n",
              "      <td>0.798800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.799600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>201</td>\n",
              "      <td>0.641900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>202</td>\n",
              "      <td>0.743900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>203</td>\n",
              "      <td>0.792700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>204</td>\n",
              "      <td>0.833700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>205</td>\n",
              "      <td>0.649400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>206</td>\n",
              "      <td>0.692200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>207</td>\n",
              "      <td>0.591000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>208</td>\n",
              "      <td>0.710900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>209</td>\n",
              "      <td>0.650200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>210</td>\n",
              "      <td>0.575600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>211</td>\n",
              "      <td>0.691200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>212</td>\n",
              "      <td>0.489900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>213</td>\n",
              "      <td>0.755500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>214</td>\n",
              "      <td>0.509500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>215</td>\n",
              "      <td>0.656500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>216</td>\n",
              "      <td>0.678800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>217</td>\n",
              "      <td>0.597700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>218</td>\n",
              "      <td>1.015100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>219</td>\n",
              "      <td>0.540700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>220</td>\n",
              "      <td>0.676700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>221</td>\n",
              "      <td>0.444900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>222</td>\n",
              "      <td>0.741600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>223</td>\n",
              "      <td>0.696000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>224</td>\n",
              "      <td>0.687100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>225</td>\n",
              "      <td>0.638000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>226</td>\n",
              "      <td>0.597000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>227</td>\n",
              "      <td>0.551700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>228</td>\n",
              "      <td>0.426500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>229</td>\n",
              "      <td>0.514200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>230</td>\n",
              "      <td>0.697500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>231</td>\n",
              "      <td>0.519100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>232</td>\n",
              "      <td>0.655600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>233</td>\n",
              "      <td>0.945800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>234</td>\n",
              "      <td>0.412100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>235</td>\n",
              "      <td>0.585000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>236</td>\n",
              "      <td>0.664900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>237</td>\n",
              "      <td>0.579800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>238</td>\n",
              "      <td>0.678600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>239</td>\n",
              "      <td>0.649300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>240</td>\n",
              "      <td>0.712300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>241</td>\n",
              "      <td>0.813100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>242</td>\n",
              "      <td>0.497500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>243</td>\n",
              "      <td>0.494400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>244</td>\n",
              "      <td>0.628500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>245</td>\n",
              "      <td>0.421700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>246</td>\n",
              "      <td>0.689100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>247</td>\n",
              "      <td>0.688300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>248</td>\n",
              "      <td>0.661200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>249</td>\n",
              "      <td>0.693500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>0.663600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>251</td>\n",
              "      <td>0.944100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>252</td>\n",
              "      <td>0.664700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>253</td>\n",
              "      <td>0.591200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>254</td>\n",
              "      <td>0.728300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>255</td>\n",
              "      <td>0.625000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>256</td>\n",
              "      <td>0.605300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>257</td>\n",
              "      <td>0.748800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>258</td>\n",
              "      <td>0.594000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>259</td>\n",
              "      <td>0.727200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>260</td>\n",
              "      <td>0.721100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>261</td>\n",
              "      <td>0.657900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>262</td>\n",
              "      <td>0.574600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>263</td>\n",
              "      <td>0.744000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>264</td>\n",
              "      <td>0.301800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>265</td>\n",
              "      <td>0.738600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>266</td>\n",
              "      <td>0.486400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>267</td>\n",
              "      <td>0.683800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>268</td>\n",
              "      <td>0.644600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>269</td>\n",
              "      <td>0.717800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>270</td>\n",
              "      <td>0.704500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>271</td>\n",
              "      <td>0.558100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>272</td>\n",
              "      <td>0.722300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>273</td>\n",
              "      <td>0.727400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>274</td>\n",
              "      <td>0.716300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>275</td>\n",
              "      <td>0.765300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>276</td>\n",
              "      <td>0.554500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>277</td>\n",
              "      <td>0.537800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>278</td>\n",
              "      <td>0.507200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>279</td>\n",
              "      <td>0.522400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>280</td>\n",
              "      <td>0.816900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>281</td>\n",
              "      <td>0.796300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>282</td>\n",
              "      <td>0.479000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>283</td>\n",
              "      <td>0.628300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>284</td>\n",
              "      <td>0.668900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>285</td>\n",
              "      <td>0.762100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>286</td>\n",
              "      <td>0.497800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>287</td>\n",
              "      <td>0.658000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>288</td>\n",
              "      <td>0.574100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>289</td>\n",
              "      <td>0.532600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>290</td>\n",
              "      <td>0.671400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>291</td>\n",
              "      <td>0.649000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>292</td>\n",
              "      <td>0.653500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>293</td>\n",
              "      <td>0.364700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>294</td>\n",
              "      <td>0.409800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>295</td>\n",
              "      <td>0.755800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>296</td>\n",
              "      <td>0.489300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>297</td>\n",
              "      <td>0.702900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>298</td>\n",
              "      <td>0.572900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>299</td>\n",
              "      <td>0.764400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.563900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>301</td>\n",
              "      <td>1.109800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>302</td>\n",
              "      <td>0.424100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>303</td>\n",
              "      <td>0.631100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>304</td>\n",
              "      <td>0.598800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>305</td>\n",
              "      <td>0.804300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>306</td>\n",
              "      <td>0.724100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>307</td>\n",
              "      <td>0.245500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>308</td>\n",
              "      <td>0.750200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>309</td>\n",
              "      <td>0.407300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>310</td>\n",
              "      <td>0.652700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>311</td>\n",
              "      <td>0.518500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>312</td>\n",
              "      <td>0.742100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>313</td>\n",
              "      <td>0.540700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>314</td>\n",
              "      <td>0.345800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>315</td>\n",
              "      <td>0.556700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>316</td>\n",
              "      <td>0.378800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>317</td>\n",
              "      <td>0.535300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>318</td>\n",
              "      <td>0.567000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>319</td>\n",
              "      <td>0.470400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>320</td>\n",
              "      <td>0.771000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>321</td>\n",
              "      <td>0.799700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>322</td>\n",
              "      <td>0.273600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>323</td>\n",
              "      <td>0.666600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>324</td>\n",
              "      <td>0.543700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>325</td>\n",
              "      <td>0.610600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>326</td>\n",
              "      <td>0.740800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>327</td>\n",
              "      <td>0.250700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>328</td>\n",
              "      <td>0.728100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>329</td>\n",
              "      <td>0.619600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>330</td>\n",
              "      <td>0.641300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>331</td>\n",
              "      <td>0.479700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>332</td>\n",
              "      <td>0.809900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>333</td>\n",
              "      <td>0.599700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>334</td>\n",
              "      <td>0.540300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>335</td>\n",
              "      <td>0.531700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>336</td>\n",
              "      <td>0.498700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>337</td>\n",
              "      <td>0.662800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>338</td>\n",
              "      <td>0.533800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>339</td>\n",
              "      <td>0.677100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>340</td>\n",
              "      <td>0.708700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>341</td>\n",
              "      <td>0.534800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>342</td>\n",
              "      <td>0.466000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>343</td>\n",
              "      <td>0.660500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>344</td>\n",
              "      <td>0.431600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>345</td>\n",
              "      <td>0.416400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>346</td>\n",
              "      <td>0.808000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>347</td>\n",
              "      <td>0.520400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>348</td>\n",
              "      <td>0.528000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>349</td>\n",
              "      <td>0.441500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>0.641700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>351</td>\n",
              "      <td>0.518300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>352</td>\n",
              "      <td>0.753700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>353</td>\n",
              "      <td>0.548300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>354</td>\n",
              "      <td>0.476100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>355</td>\n",
              "      <td>0.557500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>356</td>\n",
              "      <td>0.551600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>357</td>\n",
              "      <td>0.628100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>358</td>\n",
              "      <td>0.389000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>359</td>\n",
              "      <td>0.460600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>360</td>\n",
              "      <td>0.814300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>361</td>\n",
              "      <td>0.596700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>362</td>\n",
              "      <td>0.602600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>363</td>\n",
              "      <td>0.467400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>364</td>\n",
              "      <td>0.304800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>365</td>\n",
              "      <td>0.637700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>366</td>\n",
              "      <td>0.681300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>367</td>\n",
              "      <td>0.597500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>368</td>\n",
              "      <td>0.702000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>369</td>\n",
              "      <td>0.792000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>370</td>\n",
              "      <td>0.269100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>371</td>\n",
              "      <td>0.656300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>372</td>\n",
              "      <td>0.367700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>373</td>\n",
              "      <td>0.677800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>374</td>\n",
              "      <td>0.618700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>375</td>\n",
              "      <td>0.772400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>376</td>\n",
              "      <td>0.733200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>377</td>\n",
              "      <td>0.649000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>378</td>\n",
              "      <td>0.449400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>379</td>\n",
              "      <td>0.853200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>380</td>\n",
              "      <td>0.397300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>381</td>\n",
              "      <td>0.605700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>382</td>\n",
              "      <td>0.372500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>383</td>\n",
              "      <td>0.519300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>384</td>\n",
              "      <td>0.534000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>385</td>\n",
              "      <td>0.693500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>386</td>\n",
              "      <td>0.459200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>387</td>\n",
              "      <td>0.603900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>388</td>\n",
              "      <td>0.380800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>389</td>\n",
              "      <td>0.546300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>390</td>\n",
              "      <td>0.506300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>391</td>\n",
              "      <td>0.718500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>392</td>\n",
              "      <td>0.479900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>393</td>\n",
              "      <td>0.754200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>394</td>\n",
              "      <td>0.473900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>395</td>\n",
              "      <td>0.627700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>396</td>\n",
              "      <td>0.349600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>397</td>\n",
              "      <td>0.492800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>398</td>\n",
              "      <td>0.606500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>399</td>\n",
              "      <td>0.361900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.661300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>401</td>\n",
              "      <td>0.504400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>402</td>\n",
              "      <td>0.573800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>403</td>\n",
              "      <td>0.635400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>404</td>\n",
              "      <td>0.632800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>405</td>\n",
              "      <td>0.616000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>406</td>\n",
              "      <td>0.655900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>407</td>\n",
              "      <td>0.494200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>408</td>\n",
              "      <td>0.602800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>409</td>\n",
              "      <td>0.633800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>410</td>\n",
              "      <td>0.627100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>411</td>\n",
              "      <td>0.679100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>412</td>\n",
              "      <td>0.427300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>413</td>\n",
              "      <td>0.521000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>414</td>\n",
              "      <td>0.514600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>415</td>\n",
              "      <td>0.908600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>416</td>\n",
              "      <td>0.423800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>417</td>\n",
              "      <td>0.760300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>418</td>\n",
              "      <td>0.436200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>419</td>\n",
              "      <td>0.476500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>420</td>\n",
              "      <td>0.523800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>421</td>\n",
              "      <td>0.771100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>422</td>\n",
              "      <td>0.653900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>423</td>\n",
              "      <td>0.478100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>424</td>\n",
              "      <td>0.700700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>425</td>\n",
              "      <td>0.637400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>426</td>\n",
              "      <td>0.672200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>427</td>\n",
              "      <td>0.690300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>428</td>\n",
              "      <td>0.799400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>429</td>\n",
              "      <td>0.355400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>430</td>\n",
              "      <td>0.453400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>431</td>\n",
              "      <td>0.477800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>432</td>\n",
              "      <td>0.619800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>433</td>\n",
              "      <td>0.399100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>434</td>\n",
              "      <td>0.709400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>435</td>\n",
              "      <td>0.423700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>436</td>\n",
              "      <td>0.655200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>437</td>\n",
              "      <td>0.678700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>438</td>\n",
              "      <td>0.537900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>439</td>\n",
              "      <td>0.543200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>440</td>\n",
              "      <td>0.391900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>441</td>\n",
              "      <td>0.517300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>442</td>\n",
              "      <td>0.511700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>443</td>\n",
              "      <td>0.361700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>444</td>\n",
              "      <td>0.808100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>445</td>\n",
              "      <td>0.608400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>446</td>\n",
              "      <td>0.530000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>447</td>\n",
              "      <td>0.730500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>448</td>\n",
              "      <td>0.541500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>449</td>\n",
              "      <td>0.571100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>0.489200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>451</td>\n",
              "      <td>0.698600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>452</td>\n",
              "      <td>0.857800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>453</td>\n",
              "      <td>0.612400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>454</td>\n",
              "      <td>0.514700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>455</td>\n",
              "      <td>0.375800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>456</td>\n",
              "      <td>0.417000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>457</td>\n",
              "      <td>0.656200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>458</td>\n",
              "      <td>0.591500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>459</td>\n",
              "      <td>0.629600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>460</td>\n",
              "      <td>0.558800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>461</td>\n",
              "      <td>0.554700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>462</td>\n",
              "      <td>0.311700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>463</td>\n",
              "      <td>0.330600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>464</td>\n",
              "      <td>0.559300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>465</td>\n",
              "      <td>0.833000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>466</td>\n",
              "      <td>0.617300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>467</td>\n",
              "      <td>0.519200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>468</td>\n",
              "      <td>0.650300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>469</td>\n",
              "      <td>0.669500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>470</td>\n",
              "      <td>0.750800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>471</td>\n",
              "      <td>0.637600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>472</td>\n",
              "      <td>0.440300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>473</td>\n",
              "      <td>0.691400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>474</td>\n",
              "      <td>0.571400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>475</td>\n",
              "      <td>0.468000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>476</td>\n",
              "      <td>0.563300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>477</td>\n",
              "      <td>0.337500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>478</td>\n",
              "      <td>0.578300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>479</td>\n",
              "      <td>0.502000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>480</td>\n",
              "      <td>0.558500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>481</td>\n",
              "      <td>0.397000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>482</td>\n",
              "      <td>0.361400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>483</td>\n",
              "      <td>0.832300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>484</td>\n",
              "      <td>0.459200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>485</td>\n",
              "      <td>0.562800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>486</td>\n",
              "      <td>0.454300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>487</td>\n",
              "      <td>0.588500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>488</td>\n",
              "      <td>0.516200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>489</td>\n",
              "      <td>0.326800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>490</td>\n",
              "      <td>0.582200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>491</td>\n",
              "      <td>0.566200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>492</td>\n",
              "      <td>0.758400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>493</td>\n",
              "      <td>0.597000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>494</td>\n",
              "      <td>0.441000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>495</td>\n",
              "      <td>0.792800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>496</td>\n",
              "      <td>0.503500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>497</td>\n",
              "      <td>0.564100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>498</td>\n",
              "      <td>0.493800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>499</td>\n",
              "      <td>0.626000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.473600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>501</td>\n",
              "      <td>0.454300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>502</td>\n",
              "      <td>0.748800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>503</td>\n",
              "      <td>0.399800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>504</td>\n",
              "      <td>0.703100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>505</td>\n",
              "      <td>0.508100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>506</td>\n",
              "      <td>0.536300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>507</td>\n",
              "      <td>0.311900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>508</td>\n",
              "      <td>0.564400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>509</td>\n",
              "      <td>0.441200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>510</td>\n",
              "      <td>0.606700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>511</td>\n",
              "      <td>0.503600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>512</td>\n",
              "      <td>0.535200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>513</td>\n",
              "      <td>0.442300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>514</td>\n",
              "      <td>0.654300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>515</td>\n",
              "      <td>0.476200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>516</td>\n",
              "      <td>0.514800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>517</td>\n",
              "      <td>0.417500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>518</td>\n",
              "      <td>0.352100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>519</td>\n",
              "      <td>0.620300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>520</td>\n",
              "      <td>0.706000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>521</td>\n",
              "      <td>0.625700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>522</td>\n",
              "      <td>0.678600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>523</td>\n",
              "      <td>0.560400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>524</td>\n",
              "      <td>0.495800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>525</td>\n",
              "      <td>0.841900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>526</td>\n",
              "      <td>0.727900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>527</td>\n",
              "      <td>0.253500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>528</td>\n",
              "      <td>0.365000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>529</td>\n",
              "      <td>0.941300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>530</td>\n",
              "      <td>0.752800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>531</td>\n",
              "      <td>0.516900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>532</td>\n",
              "      <td>0.602800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>533</td>\n",
              "      <td>0.701400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>534</td>\n",
              "      <td>0.680100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>535</td>\n",
              "      <td>0.463700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>536</td>\n",
              "      <td>0.404200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>537</td>\n",
              "      <td>0.769700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>538</td>\n",
              "      <td>0.623900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>539</td>\n",
              "      <td>0.613400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>540</td>\n",
              "      <td>0.575000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>541</td>\n",
              "      <td>0.577700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>542</td>\n",
              "      <td>0.687900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>543</td>\n",
              "      <td>0.520800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>544</td>\n",
              "      <td>0.434900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>545</td>\n",
              "      <td>0.502600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>546</td>\n",
              "      <td>0.526800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>547</td>\n",
              "      <td>0.531900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>548</td>\n",
              "      <td>0.475100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>549</td>\n",
              "      <td>0.402900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>550</td>\n",
              "      <td>0.612600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>551</td>\n",
              "      <td>0.472800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>552</td>\n",
              "      <td>0.402100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>553</td>\n",
              "      <td>0.604500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>554</td>\n",
              "      <td>0.479000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>555</td>\n",
              "      <td>0.429100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>556</td>\n",
              "      <td>0.344100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>557</td>\n",
              "      <td>0.550500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>558</td>\n",
              "      <td>0.413200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>559</td>\n",
              "      <td>0.544000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>560</td>\n",
              "      <td>0.561000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>561</td>\n",
              "      <td>0.623900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>562</td>\n",
              "      <td>0.733800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>563</td>\n",
              "      <td>0.647600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>564</td>\n",
              "      <td>0.782800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>565</td>\n",
              "      <td>0.494600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>566</td>\n",
              "      <td>0.430700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>567</td>\n",
              "      <td>0.506200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>568</td>\n",
              "      <td>0.504500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>569</td>\n",
              "      <td>0.345200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>570</td>\n",
              "      <td>0.497400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>571</td>\n",
              "      <td>0.677500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>572</td>\n",
              "      <td>0.413600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>573</td>\n",
              "      <td>0.472100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>574</td>\n",
              "      <td>0.866100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>575</td>\n",
              "      <td>0.613100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>576</td>\n",
              "      <td>0.597500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>577</td>\n",
              "      <td>0.341100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>578</td>\n",
              "      <td>0.367700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>579</td>\n",
              "      <td>0.455300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>580</td>\n",
              "      <td>0.489600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>581</td>\n",
              "      <td>0.537800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>582</td>\n",
              "      <td>0.294900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>583</td>\n",
              "      <td>0.453900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>584</td>\n",
              "      <td>0.635600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>585</td>\n",
              "      <td>0.572000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>586</td>\n",
              "      <td>0.523900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>587</td>\n",
              "      <td>0.650000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>588</td>\n",
              "      <td>0.576600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>589</td>\n",
              "      <td>0.484300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>590</td>\n",
              "      <td>0.664400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>591</td>\n",
              "      <td>0.351200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>592</td>\n",
              "      <td>0.580700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>593</td>\n",
              "      <td>0.384100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>594</td>\n",
              "      <td>0.470000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>595</td>\n",
              "      <td>0.589000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>596</td>\n",
              "      <td>0.625900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>597</td>\n",
              "      <td>0.446500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>598</td>\n",
              "      <td>0.627500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>599</td>\n",
              "      <td>0.749500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.394400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>601</td>\n",
              "      <td>0.331400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>602</td>\n",
              "      <td>0.584000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>603</td>\n",
              "      <td>0.501700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>604</td>\n",
              "      <td>0.525100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>605</td>\n",
              "      <td>0.494800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>606</td>\n",
              "      <td>0.771700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>607</td>\n",
              "      <td>0.496200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>608</td>\n",
              "      <td>0.583500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>609</td>\n",
              "      <td>0.647700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>610</td>\n",
              "      <td>0.549300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>611</td>\n",
              "      <td>0.550300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>612</td>\n",
              "      <td>0.542200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>613</td>\n",
              "      <td>0.635200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>614</td>\n",
              "      <td>0.582700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>615</td>\n",
              "      <td>0.686400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>616</td>\n",
              "      <td>0.550200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>617</td>\n",
              "      <td>0.650400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>618</td>\n",
              "      <td>0.495700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>619</td>\n",
              "      <td>0.414000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>620</td>\n",
              "      <td>0.363300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>621</td>\n",
              "      <td>0.886400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>622</td>\n",
              "      <td>0.909400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>623</td>\n",
              "      <td>0.392400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>624</td>\n",
              "      <td>0.563600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>625</td>\n",
              "      <td>0.603600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>626</td>\n",
              "      <td>0.477800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>627</td>\n",
              "      <td>0.705500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>628</td>\n",
              "      <td>0.619300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>629</td>\n",
              "      <td>0.569000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>630</td>\n",
              "      <td>0.698600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>631</td>\n",
              "      <td>0.602000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>632</td>\n",
              "      <td>0.652400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>633</td>\n",
              "      <td>0.735300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>634</td>\n",
              "      <td>0.696400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>635</td>\n",
              "      <td>0.512100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>636</td>\n",
              "      <td>0.626100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>637</td>\n",
              "      <td>0.373700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>638</td>\n",
              "      <td>0.467400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>639</td>\n",
              "      <td>0.367300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>640</td>\n",
              "      <td>0.609000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>641</td>\n",
              "      <td>0.579700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>642</td>\n",
              "      <td>0.571300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>643</td>\n",
              "      <td>0.358300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>644</td>\n",
              "      <td>0.438800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>645</td>\n",
              "      <td>0.384600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>646</td>\n",
              "      <td>0.710700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>647</td>\n",
              "      <td>0.516500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>648</td>\n",
              "      <td>0.570200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>649</td>\n",
              "      <td>0.868900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>650</td>\n",
              "      <td>0.279700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>651</td>\n",
              "      <td>0.549400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>652</td>\n",
              "      <td>0.384700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>653</td>\n",
              "      <td>0.374500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>654</td>\n",
              "      <td>0.482500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>655</td>\n",
              "      <td>0.546100</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=655, training_loss=0.6448268460863419, metrics={'train_runtime': 498.0747, 'train_samples_per_second': 42.203, 'train_steps_per_second': 1.315, 'total_flos': 1382947358515200.0, 'train_loss': 0.6448268460863419, 'epoch': 4.966698382492864})"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer_2.evaluate()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "v3tNsLUrMpCf",
        "outputId": "c870fb2c-22a4-44be-e96f-844822d59f22"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='59' max='59' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [59/59 00:01]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'eval_loss': 0.5918135046958923,\n",
              " 'eval_accuracy': 0.7713675213675214,\n",
              " 'eval_precision': 0.7598617945840168,\n",
              " 'eval_recall': 0.7713675213675214,\n",
              " 'eval_f1': 0.7631507634389456,\n",
              " 'eval_runtime': 1.3631,\n",
              " 'eval_samples_per_second': 343.344,\n",
              " 'eval_steps_per_second': 43.285,\n",
              " 'epoch': 4.966698382492864}"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LoRA-3 with r and a increased, to 32 and 64"
      ],
      "metadata": {
        "id": "1voGHjXmPoSi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_3 = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\",\n",
        "                                                        num_labels=3)\n",
        "\n",
        "# Configure LoRA parameters\n",
        "lora_config_3 = LoraConfig(\n",
        "    r=32,\n",
        "    lora_alpha=64, # a = r*2\n",
        "    target_modules=[\"query\", \"value\"],\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    task_type=\"SEQ_CLS\"\n",
        ")\n",
        "\n",
        "# Wrap your model with LoRA layers\n",
        "model_3 = get_peft_model(model_3, lora_config_3)\n",
        "model_3.print_trainable_parameters()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vbzOUHRqMtQb",
        "outputId": "ae79943f-5219-42fe-99a3-5b6307ee2c0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 1,181,955 || all params: 110,666,502 || trainable%: 1.0680\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "# Define the training arguments\n",
        "training_args_lora_3 = TrainingArguments(\n",
        "    output_dir=\"./results-model_3\",\n",
        "    num_train_epochs=6,\n",
        "    per_device_train_batch_size=2, #mbs=2\n",
        "    gradient_accumulation_steps=16, # effective batch size 16*2\n",
        "    optim=\"adamw_torch\",\n",
        "    save_steps=200, # checkpoint every 200 steps\n",
        "    logging_steps=1,\n",
        "    learning_rate=2e-4, # step size in the optimizer update\n",
        "    weight_decay=0.001,\n",
        "    fp16=True, # 16 bit\n",
        "    bf16=False, # not supported on V100\n",
        "    max_grad_norm=0.3, #gradient clipping improves convergence\n",
        "    max_steps=-1,\n",
        "    warmup_ratio=0.03, # learning rate warmup\n",
        "    group_by_length=True,\n",
        "    lr_scheduler_type=\"cosine\" # cosine lr scheduler\n",
        ")\n",
        "\n",
        "trainer_3 = Trainer(\n",
        "    model=model_3,\n",
        "    args=training_args_lora_3,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "import gc # garbage collection\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache() # clean cache\n",
        "\n",
        "trainer_3.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ipPoq616P4_U",
        "outputId": "19f37a5c-f346-4ec2-dc92-413fb75337ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-23-d44ce753de20>:23: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer_3 = Trainer(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='786' max='786' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [786/786 09:34, Epoch 5/6]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.175800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.163400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.098200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.073000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>1.126000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>1.101100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>1.157200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>1.118100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>1.132100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.102400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>1.044200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>1.100300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>1.017200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>1.021200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>1.072800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>1.137800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>0.993800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>1.016400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>1.067600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>1.007700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>0.991100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>1.003100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>0.858300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>1.029700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>1.122800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>0.926300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>0.942700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>0.956700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>0.960500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.949100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>0.884500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>1.107200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>0.836600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>1.016300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>1.036700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>1.036500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>0.894300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>1.019600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>0.860000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.941100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41</td>\n",
              "      <td>0.905900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>0.781900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43</td>\n",
              "      <td>0.904400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>0.761800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>1.025700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46</td>\n",
              "      <td>0.959000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47</td>\n",
              "      <td>1.008700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>0.955000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49</td>\n",
              "      <td>0.851100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.812700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>51</td>\n",
              "      <td>0.795600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52</td>\n",
              "      <td>0.851100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>53</td>\n",
              "      <td>0.702600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>54</td>\n",
              "      <td>0.955100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55</td>\n",
              "      <td>0.842200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>0.721300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>57</td>\n",
              "      <td>0.634500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>58</td>\n",
              "      <td>0.856800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>59</td>\n",
              "      <td>0.858800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>1.170000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>61</td>\n",
              "      <td>0.878000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>62</td>\n",
              "      <td>0.810700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>63</td>\n",
              "      <td>0.825300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>64</td>\n",
              "      <td>0.646200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>65</td>\n",
              "      <td>0.739200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>66</td>\n",
              "      <td>0.854100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>67</td>\n",
              "      <td>0.775500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>68</td>\n",
              "      <td>0.661200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>69</td>\n",
              "      <td>0.943500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>0.828400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>71</td>\n",
              "      <td>0.809900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>72</td>\n",
              "      <td>0.805200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>73</td>\n",
              "      <td>0.768700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>74</td>\n",
              "      <td>0.829900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>75</td>\n",
              "      <td>0.620200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>76</td>\n",
              "      <td>0.785200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>77</td>\n",
              "      <td>0.699800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>78</td>\n",
              "      <td>0.509100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>79</td>\n",
              "      <td>0.720600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>0.676300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>81</td>\n",
              "      <td>1.002600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>82</td>\n",
              "      <td>0.759600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>83</td>\n",
              "      <td>0.727900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>84</td>\n",
              "      <td>0.819700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>85</td>\n",
              "      <td>0.779000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>86</td>\n",
              "      <td>0.717200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>87</td>\n",
              "      <td>0.478300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>88</td>\n",
              "      <td>0.517100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>89</td>\n",
              "      <td>0.666600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>0.764600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>91</td>\n",
              "      <td>0.909000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>92</td>\n",
              "      <td>0.960500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>93</td>\n",
              "      <td>0.818800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>94</td>\n",
              "      <td>0.752600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>95</td>\n",
              "      <td>0.578800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>96</td>\n",
              "      <td>0.678000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>97</td>\n",
              "      <td>0.715000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>98</td>\n",
              "      <td>0.887700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>99</td>\n",
              "      <td>0.521100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.705200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>101</td>\n",
              "      <td>0.862900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>102</td>\n",
              "      <td>0.622500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>103</td>\n",
              "      <td>0.853200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>104</td>\n",
              "      <td>0.963000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>105</td>\n",
              "      <td>0.415900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>106</td>\n",
              "      <td>0.759300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>107</td>\n",
              "      <td>0.665800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>108</td>\n",
              "      <td>0.452900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>109</td>\n",
              "      <td>0.652400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>0.469200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>111</td>\n",
              "      <td>0.772700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>112</td>\n",
              "      <td>0.749200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>113</td>\n",
              "      <td>0.751900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>114</td>\n",
              "      <td>0.479700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>115</td>\n",
              "      <td>0.718800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>116</td>\n",
              "      <td>0.768400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>117</td>\n",
              "      <td>0.612500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>118</td>\n",
              "      <td>0.406900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>119</td>\n",
              "      <td>0.555800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>0.738800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>121</td>\n",
              "      <td>0.637100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>122</td>\n",
              "      <td>0.502700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>123</td>\n",
              "      <td>0.515400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>124</td>\n",
              "      <td>0.701800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>125</td>\n",
              "      <td>0.696800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>126</td>\n",
              "      <td>0.752500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>127</td>\n",
              "      <td>0.743700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>128</td>\n",
              "      <td>0.591700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>129</td>\n",
              "      <td>0.692300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>0.635200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>131</td>\n",
              "      <td>0.610500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>132</td>\n",
              "      <td>0.306500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>133</td>\n",
              "      <td>0.689100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>134</td>\n",
              "      <td>0.749800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>135</td>\n",
              "      <td>0.901500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>136</td>\n",
              "      <td>0.828900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>137</td>\n",
              "      <td>0.714600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>138</td>\n",
              "      <td>0.427700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>139</td>\n",
              "      <td>0.705200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>0.587800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>141</td>\n",
              "      <td>0.699400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>142</td>\n",
              "      <td>0.684300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>143</td>\n",
              "      <td>0.689700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>144</td>\n",
              "      <td>0.575500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>145</td>\n",
              "      <td>0.545400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>146</td>\n",
              "      <td>0.580200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>147</td>\n",
              "      <td>0.697500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>148</td>\n",
              "      <td>0.338200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>149</td>\n",
              "      <td>0.689000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.407600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>151</td>\n",
              "      <td>0.829000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>152</td>\n",
              "      <td>0.537100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>153</td>\n",
              "      <td>0.688500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>154</td>\n",
              "      <td>0.388800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>155</td>\n",
              "      <td>0.393200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>156</td>\n",
              "      <td>0.827000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>157</td>\n",
              "      <td>0.626900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>158</td>\n",
              "      <td>0.423000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>159</td>\n",
              "      <td>0.434100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>0.553000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>161</td>\n",
              "      <td>0.648700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>162</td>\n",
              "      <td>0.551600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>163</td>\n",
              "      <td>0.396100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>164</td>\n",
              "      <td>0.590500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>165</td>\n",
              "      <td>0.824900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>166</td>\n",
              "      <td>0.749000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>167</td>\n",
              "      <td>0.666700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>168</td>\n",
              "      <td>0.553400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>169</td>\n",
              "      <td>0.929500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>0.529400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>171</td>\n",
              "      <td>0.556600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>172</td>\n",
              "      <td>0.599000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>173</td>\n",
              "      <td>0.644300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>174</td>\n",
              "      <td>0.723500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>175</td>\n",
              "      <td>0.606500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>176</td>\n",
              "      <td>0.563200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>177</td>\n",
              "      <td>0.566000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>178</td>\n",
              "      <td>0.461600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>179</td>\n",
              "      <td>0.460700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>0.646400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>181</td>\n",
              "      <td>0.669000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>182</td>\n",
              "      <td>0.551900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>183</td>\n",
              "      <td>0.684400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>184</td>\n",
              "      <td>0.561600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>185</td>\n",
              "      <td>0.944300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>186</td>\n",
              "      <td>0.628900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>187</td>\n",
              "      <td>0.360800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>188</td>\n",
              "      <td>0.752700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>189</td>\n",
              "      <td>0.522800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>190</td>\n",
              "      <td>0.561600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>191</td>\n",
              "      <td>0.736600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>192</td>\n",
              "      <td>0.797400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>193</td>\n",
              "      <td>0.552400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>194</td>\n",
              "      <td>0.743200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>195</td>\n",
              "      <td>0.374700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>196</td>\n",
              "      <td>0.491600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>197</td>\n",
              "      <td>0.958800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>198</td>\n",
              "      <td>0.727900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>199</td>\n",
              "      <td>0.647300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.718600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>201</td>\n",
              "      <td>0.630900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>202</td>\n",
              "      <td>0.656100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>203</td>\n",
              "      <td>0.777700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>204</td>\n",
              "      <td>0.866900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>205</td>\n",
              "      <td>0.558500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>206</td>\n",
              "      <td>0.589100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>207</td>\n",
              "      <td>0.535400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>208</td>\n",
              "      <td>0.648100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>209</td>\n",
              "      <td>0.667700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>210</td>\n",
              "      <td>0.550900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>211</td>\n",
              "      <td>0.591900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>212</td>\n",
              "      <td>0.427000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>213</td>\n",
              "      <td>0.684600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>214</td>\n",
              "      <td>0.493900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>215</td>\n",
              "      <td>0.493200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>216</td>\n",
              "      <td>0.561500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>217</td>\n",
              "      <td>0.592000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>218</td>\n",
              "      <td>1.006000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>219</td>\n",
              "      <td>0.454900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>220</td>\n",
              "      <td>0.671900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>221</td>\n",
              "      <td>0.333100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>222</td>\n",
              "      <td>0.766900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>223</td>\n",
              "      <td>0.686300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>224</td>\n",
              "      <td>0.648300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>225</td>\n",
              "      <td>0.641400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>226</td>\n",
              "      <td>0.661500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>227</td>\n",
              "      <td>0.590600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>228</td>\n",
              "      <td>0.463300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>229</td>\n",
              "      <td>0.476400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>230</td>\n",
              "      <td>0.717700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>231</td>\n",
              "      <td>0.499000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>232</td>\n",
              "      <td>0.667200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>233</td>\n",
              "      <td>1.016400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>234</td>\n",
              "      <td>0.331400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>235</td>\n",
              "      <td>0.563300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>236</td>\n",
              "      <td>0.686500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>237</td>\n",
              "      <td>0.575900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>238</td>\n",
              "      <td>0.657400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>239</td>\n",
              "      <td>0.552000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>240</td>\n",
              "      <td>0.692100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>241</td>\n",
              "      <td>0.690800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>242</td>\n",
              "      <td>0.507100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>243</td>\n",
              "      <td>0.438100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>244</td>\n",
              "      <td>0.647700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>245</td>\n",
              "      <td>0.372100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>246</td>\n",
              "      <td>0.594200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>247</td>\n",
              "      <td>0.724500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>248</td>\n",
              "      <td>0.557200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>249</td>\n",
              "      <td>0.725500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>0.611600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>251</td>\n",
              "      <td>0.951900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>252</td>\n",
              "      <td>0.570900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>253</td>\n",
              "      <td>0.521400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>254</td>\n",
              "      <td>0.726700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>255</td>\n",
              "      <td>0.590600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>256</td>\n",
              "      <td>0.528700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>257</td>\n",
              "      <td>0.709400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>258</td>\n",
              "      <td>0.650000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>259</td>\n",
              "      <td>0.801700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>260</td>\n",
              "      <td>0.639600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>261</td>\n",
              "      <td>0.566600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>262</td>\n",
              "      <td>0.462300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>263</td>\n",
              "      <td>0.677400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>264</td>\n",
              "      <td>0.275200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>265</td>\n",
              "      <td>0.663800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>266</td>\n",
              "      <td>0.421700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>267</td>\n",
              "      <td>0.636300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>268</td>\n",
              "      <td>0.575100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>269</td>\n",
              "      <td>0.606200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>270</td>\n",
              "      <td>0.749800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>271</td>\n",
              "      <td>0.522200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>272</td>\n",
              "      <td>0.628900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>273</td>\n",
              "      <td>0.675800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>274</td>\n",
              "      <td>0.660600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>275</td>\n",
              "      <td>0.763900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>276</td>\n",
              "      <td>0.507300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>277</td>\n",
              "      <td>0.430400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>278</td>\n",
              "      <td>0.468200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>279</td>\n",
              "      <td>0.479600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>280</td>\n",
              "      <td>0.781500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>281</td>\n",
              "      <td>0.738800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>282</td>\n",
              "      <td>0.475500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>283</td>\n",
              "      <td>0.466000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>284</td>\n",
              "      <td>0.549000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>285</td>\n",
              "      <td>0.645100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>286</td>\n",
              "      <td>0.494400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>287</td>\n",
              "      <td>0.666500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>288</td>\n",
              "      <td>0.565300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>289</td>\n",
              "      <td>0.518600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>290</td>\n",
              "      <td>0.580400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>291</td>\n",
              "      <td>0.568300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>292</td>\n",
              "      <td>0.653800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>293</td>\n",
              "      <td>0.309600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>294</td>\n",
              "      <td>0.332700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>295</td>\n",
              "      <td>0.775100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>296</td>\n",
              "      <td>0.461600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>297</td>\n",
              "      <td>0.708000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>298</td>\n",
              "      <td>0.475600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>299</td>\n",
              "      <td>0.674500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.577400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>301</td>\n",
              "      <td>1.203100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>302</td>\n",
              "      <td>0.341700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>303</td>\n",
              "      <td>0.536800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>304</td>\n",
              "      <td>0.654700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>305</td>\n",
              "      <td>0.812100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>306</td>\n",
              "      <td>0.765700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>307</td>\n",
              "      <td>0.249200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>308</td>\n",
              "      <td>0.638300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>309</td>\n",
              "      <td>0.384100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>310</td>\n",
              "      <td>0.684300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>311</td>\n",
              "      <td>0.399500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>312</td>\n",
              "      <td>0.539500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>313</td>\n",
              "      <td>0.570300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>314</td>\n",
              "      <td>0.386900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>315</td>\n",
              "      <td>0.484000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>316</td>\n",
              "      <td>0.329800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>317</td>\n",
              "      <td>0.456000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>318</td>\n",
              "      <td>0.563000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>319</td>\n",
              "      <td>0.499600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>320</td>\n",
              "      <td>0.826100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>321</td>\n",
              "      <td>0.860900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>322</td>\n",
              "      <td>0.196900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>323</td>\n",
              "      <td>0.688300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>324</td>\n",
              "      <td>0.499000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>325</td>\n",
              "      <td>0.547000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>326</td>\n",
              "      <td>0.832300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>327</td>\n",
              "      <td>0.211800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>328</td>\n",
              "      <td>0.856500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>329</td>\n",
              "      <td>0.669000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>330</td>\n",
              "      <td>0.617700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>331</td>\n",
              "      <td>0.528000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>332</td>\n",
              "      <td>0.745800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>333</td>\n",
              "      <td>0.472200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>334</td>\n",
              "      <td>0.488600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>335</td>\n",
              "      <td>0.510300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>336</td>\n",
              "      <td>0.396500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>337</td>\n",
              "      <td>0.610600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>338</td>\n",
              "      <td>0.475800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>339</td>\n",
              "      <td>0.548400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>340</td>\n",
              "      <td>0.714000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>341</td>\n",
              "      <td>0.509500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>342</td>\n",
              "      <td>0.395700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>343</td>\n",
              "      <td>0.467800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>344</td>\n",
              "      <td>0.440800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>345</td>\n",
              "      <td>0.280500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>346</td>\n",
              "      <td>0.782200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>347</td>\n",
              "      <td>0.414400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>348</td>\n",
              "      <td>0.476100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>349</td>\n",
              "      <td>0.410300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>0.565600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>351</td>\n",
              "      <td>0.498500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>352</td>\n",
              "      <td>0.699400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>353</td>\n",
              "      <td>0.461700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>354</td>\n",
              "      <td>0.465700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>355</td>\n",
              "      <td>0.576800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>356</td>\n",
              "      <td>0.529500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>357</td>\n",
              "      <td>0.658100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>358</td>\n",
              "      <td>0.327000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>359</td>\n",
              "      <td>0.441600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>360</td>\n",
              "      <td>0.689900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>361</td>\n",
              "      <td>0.576800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>362</td>\n",
              "      <td>0.615900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>363</td>\n",
              "      <td>0.463600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>364</td>\n",
              "      <td>0.279200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>365</td>\n",
              "      <td>0.651400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>366</td>\n",
              "      <td>0.583800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>367</td>\n",
              "      <td>0.588300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>368</td>\n",
              "      <td>0.618400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>369</td>\n",
              "      <td>0.730900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>370</td>\n",
              "      <td>0.260700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>371</td>\n",
              "      <td>0.600900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>372</td>\n",
              "      <td>0.293600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>373</td>\n",
              "      <td>0.643500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>374</td>\n",
              "      <td>0.681800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>375</td>\n",
              "      <td>0.752600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>376</td>\n",
              "      <td>0.685600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>377</td>\n",
              "      <td>0.556900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>378</td>\n",
              "      <td>0.477700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>379</td>\n",
              "      <td>0.795900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>380</td>\n",
              "      <td>0.378400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>381</td>\n",
              "      <td>0.493400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>382</td>\n",
              "      <td>0.394100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>383</td>\n",
              "      <td>0.430200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>384</td>\n",
              "      <td>0.488000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>385</td>\n",
              "      <td>0.662100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>386</td>\n",
              "      <td>0.404500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>387</td>\n",
              "      <td>0.604400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>388</td>\n",
              "      <td>0.325700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>389</td>\n",
              "      <td>0.535700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>390</td>\n",
              "      <td>0.501200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>391</td>\n",
              "      <td>0.637200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>392</td>\n",
              "      <td>0.446600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>393</td>\n",
              "      <td>0.793100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>394</td>\n",
              "      <td>0.455900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>395</td>\n",
              "      <td>0.710600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>396</td>\n",
              "      <td>0.259300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>397</td>\n",
              "      <td>0.347800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>398</td>\n",
              "      <td>0.434000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>399</td>\n",
              "      <td>0.346500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.563700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>401</td>\n",
              "      <td>0.451200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>402</td>\n",
              "      <td>0.452800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>403</td>\n",
              "      <td>0.547800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>404</td>\n",
              "      <td>0.584500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>405</td>\n",
              "      <td>0.564600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>406</td>\n",
              "      <td>0.620500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>407</td>\n",
              "      <td>0.434100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>408</td>\n",
              "      <td>0.473100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>409</td>\n",
              "      <td>0.490200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>410</td>\n",
              "      <td>0.660700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>411</td>\n",
              "      <td>0.664000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>412</td>\n",
              "      <td>0.330300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>413</td>\n",
              "      <td>0.472300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>414</td>\n",
              "      <td>0.440600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>415</td>\n",
              "      <td>0.930300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>416</td>\n",
              "      <td>0.350200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>417</td>\n",
              "      <td>0.636600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>418</td>\n",
              "      <td>0.386900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>419</td>\n",
              "      <td>0.377300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>420</td>\n",
              "      <td>0.519400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>421</td>\n",
              "      <td>0.603200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>422</td>\n",
              "      <td>0.593700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>423</td>\n",
              "      <td>0.464700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>424</td>\n",
              "      <td>0.653000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>425</td>\n",
              "      <td>0.552300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>426</td>\n",
              "      <td>0.610700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>427</td>\n",
              "      <td>0.669700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>428</td>\n",
              "      <td>0.657400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>429</td>\n",
              "      <td>0.351900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>430</td>\n",
              "      <td>0.451800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>431</td>\n",
              "      <td>0.418500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>432</td>\n",
              "      <td>0.555300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>433</td>\n",
              "      <td>0.302700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>434</td>\n",
              "      <td>0.614000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>435</td>\n",
              "      <td>0.321700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>436</td>\n",
              "      <td>0.658700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>437</td>\n",
              "      <td>0.637500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>438</td>\n",
              "      <td>0.487200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>439</td>\n",
              "      <td>0.579800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>440</td>\n",
              "      <td>0.438800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>441</td>\n",
              "      <td>0.431000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>442</td>\n",
              "      <td>0.351000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>443</td>\n",
              "      <td>0.314400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>444</td>\n",
              "      <td>0.821500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>445</td>\n",
              "      <td>0.538500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>446</td>\n",
              "      <td>0.544900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>447</td>\n",
              "      <td>0.645100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>448</td>\n",
              "      <td>0.471700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>449</td>\n",
              "      <td>0.541500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>0.391100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>451</td>\n",
              "      <td>0.616500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>452</td>\n",
              "      <td>0.710500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>453</td>\n",
              "      <td>0.450200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>454</td>\n",
              "      <td>0.463000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>455</td>\n",
              "      <td>0.328600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>456</td>\n",
              "      <td>0.414900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>457</td>\n",
              "      <td>0.606500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>458</td>\n",
              "      <td>0.541600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>459</td>\n",
              "      <td>0.462400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>460</td>\n",
              "      <td>0.477700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>461</td>\n",
              "      <td>0.469200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>462</td>\n",
              "      <td>0.291600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>463</td>\n",
              "      <td>0.326300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>464</td>\n",
              "      <td>0.557400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>465</td>\n",
              "      <td>0.819300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>466</td>\n",
              "      <td>0.560700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>467</td>\n",
              "      <td>0.465700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>468</td>\n",
              "      <td>0.484000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>469</td>\n",
              "      <td>0.658900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>470</td>\n",
              "      <td>0.696100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>471</td>\n",
              "      <td>0.667400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>472</td>\n",
              "      <td>0.421000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>473</td>\n",
              "      <td>0.662100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>474</td>\n",
              "      <td>0.496500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>475</td>\n",
              "      <td>0.445400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>476</td>\n",
              "      <td>0.553700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>477</td>\n",
              "      <td>0.371800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>478</td>\n",
              "      <td>0.537000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>479</td>\n",
              "      <td>0.426800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>480</td>\n",
              "      <td>0.508500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>481</td>\n",
              "      <td>0.360400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>482</td>\n",
              "      <td>0.303600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>483</td>\n",
              "      <td>0.769200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>484</td>\n",
              "      <td>0.371500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>485</td>\n",
              "      <td>0.435900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>486</td>\n",
              "      <td>0.438400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>487</td>\n",
              "      <td>0.506400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>488</td>\n",
              "      <td>0.487500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>489</td>\n",
              "      <td>0.283400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>490</td>\n",
              "      <td>0.494000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>491</td>\n",
              "      <td>0.416100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>492</td>\n",
              "      <td>0.696600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>493</td>\n",
              "      <td>0.610300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>494</td>\n",
              "      <td>0.332900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>495</td>\n",
              "      <td>0.862700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>496</td>\n",
              "      <td>0.387700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>497</td>\n",
              "      <td>0.538900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>498</td>\n",
              "      <td>0.402100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>499</td>\n",
              "      <td>0.520800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.475900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>501</td>\n",
              "      <td>0.370500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>502</td>\n",
              "      <td>0.788400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>503</td>\n",
              "      <td>0.243400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>504</td>\n",
              "      <td>0.774500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>505</td>\n",
              "      <td>0.428000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>506</td>\n",
              "      <td>0.444400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>507</td>\n",
              "      <td>0.343700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>508</td>\n",
              "      <td>0.544100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>509</td>\n",
              "      <td>0.466000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>510</td>\n",
              "      <td>0.569600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>511</td>\n",
              "      <td>0.496500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>512</td>\n",
              "      <td>0.527800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>513</td>\n",
              "      <td>0.350000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>514</td>\n",
              "      <td>0.498300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>515</td>\n",
              "      <td>0.365800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>516</td>\n",
              "      <td>0.551500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>517</td>\n",
              "      <td>0.403300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>518</td>\n",
              "      <td>0.338200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>519</td>\n",
              "      <td>0.487300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>520</td>\n",
              "      <td>0.740900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>521</td>\n",
              "      <td>0.679300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>522</td>\n",
              "      <td>0.742400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>523</td>\n",
              "      <td>0.479300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>524</td>\n",
              "      <td>0.489500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>525</td>\n",
              "      <td>0.739900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>526</td>\n",
              "      <td>0.648100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>527</td>\n",
              "      <td>0.188800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>528</td>\n",
              "      <td>0.295300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>529</td>\n",
              "      <td>0.841900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>530</td>\n",
              "      <td>0.633800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>531</td>\n",
              "      <td>0.450500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>532</td>\n",
              "      <td>0.507700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>533</td>\n",
              "      <td>0.708400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>534</td>\n",
              "      <td>0.585700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>535</td>\n",
              "      <td>0.409900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>536</td>\n",
              "      <td>0.348200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>537</td>\n",
              "      <td>0.567600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>538</td>\n",
              "      <td>0.559400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>539</td>\n",
              "      <td>0.567900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>540</td>\n",
              "      <td>0.505400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>541</td>\n",
              "      <td>0.520500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>542</td>\n",
              "      <td>0.487000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>543</td>\n",
              "      <td>0.392700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>544</td>\n",
              "      <td>0.396900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>545</td>\n",
              "      <td>0.474000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>546</td>\n",
              "      <td>0.469400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>547</td>\n",
              "      <td>0.447500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>548</td>\n",
              "      <td>0.412300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>549</td>\n",
              "      <td>0.365400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>550</td>\n",
              "      <td>0.547200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>551</td>\n",
              "      <td>0.376800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>552</td>\n",
              "      <td>0.338400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>553</td>\n",
              "      <td>0.487000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>554</td>\n",
              "      <td>0.481300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>555</td>\n",
              "      <td>0.392100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>556</td>\n",
              "      <td>0.282100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>557</td>\n",
              "      <td>0.471700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>558</td>\n",
              "      <td>0.372700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>559</td>\n",
              "      <td>0.479400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>560</td>\n",
              "      <td>0.460600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>561</td>\n",
              "      <td>0.669900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>562</td>\n",
              "      <td>0.615000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>563</td>\n",
              "      <td>0.474900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>564</td>\n",
              "      <td>0.742600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>565</td>\n",
              "      <td>0.423800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>566</td>\n",
              "      <td>0.338900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>567</td>\n",
              "      <td>0.460500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>568</td>\n",
              "      <td>0.401700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>569</td>\n",
              "      <td>0.355700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>570</td>\n",
              "      <td>0.419300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>571</td>\n",
              "      <td>0.565200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>572</td>\n",
              "      <td>0.327500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>573</td>\n",
              "      <td>0.369200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>574</td>\n",
              "      <td>0.646200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>575</td>\n",
              "      <td>0.464400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>576</td>\n",
              "      <td>0.460900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>577</td>\n",
              "      <td>0.346300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>578</td>\n",
              "      <td>0.257800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>579</td>\n",
              "      <td>0.371600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>580</td>\n",
              "      <td>0.386500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>581</td>\n",
              "      <td>0.447000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>582</td>\n",
              "      <td>0.178800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>583</td>\n",
              "      <td>0.384600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>584</td>\n",
              "      <td>0.472800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>585</td>\n",
              "      <td>0.537400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>586</td>\n",
              "      <td>0.538300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>587</td>\n",
              "      <td>0.692700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>588</td>\n",
              "      <td>0.542100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>589</td>\n",
              "      <td>0.407300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>590</td>\n",
              "      <td>0.562000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>591</td>\n",
              "      <td>0.285900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>592</td>\n",
              "      <td>0.580300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>593</td>\n",
              "      <td>0.446500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>594</td>\n",
              "      <td>0.509200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>595</td>\n",
              "      <td>0.609600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>596</td>\n",
              "      <td>0.554800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>597</td>\n",
              "      <td>0.392400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>598</td>\n",
              "      <td>0.528300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>599</td>\n",
              "      <td>0.704800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.276400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>601</td>\n",
              "      <td>0.310700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>602</td>\n",
              "      <td>0.484100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>603</td>\n",
              "      <td>0.399700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>604</td>\n",
              "      <td>0.380800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>605</td>\n",
              "      <td>0.451400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>606</td>\n",
              "      <td>0.674400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>607</td>\n",
              "      <td>0.408400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>608</td>\n",
              "      <td>0.506900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>609</td>\n",
              "      <td>0.638000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>610</td>\n",
              "      <td>0.519200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>611</td>\n",
              "      <td>0.436700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>612</td>\n",
              "      <td>0.445100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>613</td>\n",
              "      <td>0.566100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>614</td>\n",
              "      <td>0.492900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>615</td>\n",
              "      <td>0.589100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>616</td>\n",
              "      <td>0.496600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>617</td>\n",
              "      <td>0.525200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>618</td>\n",
              "      <td>0.559300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>619</td>\n",
              "      <td>0.332100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>620</td>\n",
              "      <td>0.327200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>621</td>\n",
              "      <td>0.666900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>622</td>\n",
              "      <td>0.881000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>623</td>\n",
              "      <td>0.308300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>624</td>\n",
              "      <td>0.428600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>625</td>\n",
              "      <td>0.571100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>626</td>\n",
              "      <td>0.494200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>627</td>\n",
              "      <td>0.624600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>628</td>\n",
              "      <td>0.643900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>629</td>\n",
              "      <td>0.432700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>630</td>\n",
              "      <td>0.625600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>631</td>\n",
              "      <td>0.529700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>632</td>\n",
              "      <td>0.585700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>633</td>\n",
              "      <td>0.718000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>634</td>\n",
              "      <td>0.637500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>635</td>\n",
              "      <td>0.380800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>636</td>\n",
              "      <td>0.587200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>637</td>\n",
              "      <td>0.355100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>638</td>\n",
              "      <td>0.434800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>639</td>\n",
              "      <td>0.359600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>640</td>\n",
              "      <td>0.522700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>641</td>\n",
              "      <td>0.541300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>642</td>\n",
              "      <td>0.525000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>643</td>\n",
              "      <td>0.301000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>644</td>\n",
              "      <td>0.463000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>645</td>\n",
              "      <td>0.290200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>646</td>\n",
              "      <td>0.610500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>647</td>\n",
              "      <td>0.509000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>648</td>\n",
              "      <td>0.497800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>649</td>\n",
              "      <td>0.805800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>650</td>\n",
              "      <td>0.309800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>651</td>\n",
              "      <td>0.518800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>652</td>\n",
              "      <td>0.365100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>653</td>\n",
              "      <td>0.328000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>654</td>\n",
              "      <td>0.430900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>655</td>\n",
              "      <td>0.552000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>656</td>\n",
              "      <td>0.549500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>657</td>\n",
              "      <td>0.624800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>658</td>\n",
              "      <td>0.205300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>659</td>\n",
              "      <td>0.659300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>660</td>\n",
              "      <td>0.136200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>661</td>\n",
              "      <td>0.358700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>662</td>\n",
              "      <td>0.430500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>663</td>\n",
              "      <td>0.260100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>664</td>\n",
              "      <td>0.448300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>665</td>\n",
              "      <td>0.350200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>666</td>\n",
              "      <td>0.525600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>667</td>\n",
              "      <td>0.376800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>668</td>\n",
              "      <td>0.379700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>669</td>\n",
              "      <td>0.445400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>670</td>\n",
              "      <td>0.843900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>671</td>\n",
              "      <td>0.392300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>672</td>\n",
              "      <td>0.744700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>673</td>\n",
              "      <td>0.464600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>674</td>\n",
              "      <td>0.578400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>675</td>\n",
              "      <td>0.497100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>676</td>\n",
              "      <td>0.609100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>677</td>\n",
              "      <td>0.377800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>678</td>\n",
              "      <td>0.492600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>679</td>\n",
              "      <td>0.408000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>680</td>\n",
              "      <td>0.595800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>681</td>\n",
              "      <td>0.425600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>682</td>\n",
              "      <td>0.363400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>683</td>\n",
              "      <td>0.665300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>684</td>\n",
              "      <td>0.635900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>685</td>\n",
              "      <td>0.359800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>686</td>\n",
              "      <td>0.430000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>687</td>\n",
              "      <td>0.511900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>688</td>\n",
              "      <td>0.589500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>689</td>\n",
              "      <td>0.738500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>690</td>\n",
              "      <td>0.329000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>691</td>\n",
              "      <td>0.438600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>692</td>\n",
              "      <td>0.523500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>693</td>\n",
              "      <td>0.506300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>694</td>\n",
              "      <td>0.442900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>695</td>\n",
              "      <td>0.482400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>696</td>\n",
              "      <td>0.668000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>697</td>\n",
              "      <td>0.494200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>698</td>\n",
              "      <td>0.435600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>699</td>\n",
              "      <td>0.412800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>0.268000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>701</td>\n",
              "      <td>0.779000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>702</td>\n",
              "      <td>0.395400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>703</td>\n",
              "      <td>0.385300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>704</td>\n",
              "      <td>0.435000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>705</td>\n",
              "      <td>0.509300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>706</td>\n",
              "      <td>0.681400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>707</td>\n",
              "      <td>0.511600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>708</td>\n",
              "      <td>0.365500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>709</td>\n",
              "      <td>0.373400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>710</td>\n",
              "      <td>0.482400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>711</td>\n",
              "      <td>0.406300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>712</td>\n",
              "      <td>0.553000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>713</td>\n",
              "      <td>0.402300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>714</td>\n",
              "      <td>0.577600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>715</td>\n",
              "      <td>0.456000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>716</td>\n",
              "      <td>0.315200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>717</td>\n",
              "      <td>0.633800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>718</td>\n",
              "      <td>0.616100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>719</td>\n",
              "      <td>0.351500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>720</td>\n",
              "      <td>0.490600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>721</td>\n",
              "      <td>0.494500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>722</td>\n",
              "      <td>0.469100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>723</td>\n",
              "      <td>0.529300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>724</td>\n",
              "      <td>0.223700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>725</td>\n",
              "      <td>0.493100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>726</td>\n",
              "      <td>0.419600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>727</td>\n",
              "      <td>0.325500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>728</td>\n",
              "      <td>0.351500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>729</td>\n",
              "      <td>0.690200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>730</td>\n",
              "      <td>0.493700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>731</td>\n",
              "      <td>0.312000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>732</td>\n",
              "      <td>0.430500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>733</td>\n",
              "      <td>0.399200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>734</td>\n",
              "      <td>0.661000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>735</td>\n",
              "      <td>0.274400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>736</td>\n",
              "      <td>0.483100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>737</td>\n",
              "      <td>0.322100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>738</td>\n",
              "      <td>0.272200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>739</td>\n",
              "      <td>0.330600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>740</td>\n",
              "      <td>0.350000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>741</td>\n",
              "      <td>0.532500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>742</td>\n",
              "      <td>0.540900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>743</td>\n",
              "      <td>0.532900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>744</td>\n",
              "      <td>0.278100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>745</td>\n",
              "      <td>0.467200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>746</td>\n",
              "      <td>0.362600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>747</td>\n",
              "      <td>0.317700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>748</td>\n",
              "      <td>0.443700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>749</td>\n",
              "      <td>0.439000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>750</td>\n",
              "      <td>0.747600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>751</td>\n",
              "      <td>0.423900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>752</td>\n",
              "      <td>0.390600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>753</td>\n",
              "      <td>0.375100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>754</td>\n",
              "      <td>0.351800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>755</td>\n",
              "      <td>0.731700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>756</td>\n",
              "      <td>0.454300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>757</td>\n",
              "      <td>0.397900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>758</td>\n",
              "      <td>0.626200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>759</td>\n",
              "      <td>0.430900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>760</td>\n",
              "      <td>0.400800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>761</td>\n",
              "      <td>0.396800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>762</td>\n",
              "      <td>0.663600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>763</td>\n",
              "      <td>0.455000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>764</td>\n",
              "      <td>0.260200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>765</td>\n",
              "      <td>0.484600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>766</td>\n",
              "      <td>0.460000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>767</td>\n",
              "      <td>0.420700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>768</td>\n",
              "      <td>0.763300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>769</td>\n",
              "      <td>0.366100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>770</td>\n",
              "      <td>0.546600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>771</td>\n",
              "      <td>0.618100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>772</td>\n",
              "      <td>0.392200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>773</td>\n",
              "      <td>0.343300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>774</td>\n",
              "      <td>0.633600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>775</td>\n",
              "      <td>0.338700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>776</td>\n",
              "      <td>0.694500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>777</td>\n",
              "      <td>0.291100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>778</td>\n",
              "      <td>0.278400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>779</td>\n",
              "      <td>0.429000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>780</td>\n",
              "      <td>0.441700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>781</td>\n",
              "      <td>0.581100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>782</td>\n",
              "      <td>0.436700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>783</td>\n",
              "      <td>0.405700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>784</td>\n",
              "      <td>0.297400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>785</td>\n",
              "      <td>0.243100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>786</td>\n",
              "      <td>0.263500</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=786, training_loss=0.5756967399866526, metrics={'train_runtime': 576.2492, 'train_samples_per_second': 43.773, 'train_steps_per_second': 1.364, 'total_flos': 1670620073785344.0, 'train_loss': 0.5756967399866526, 'epoch': 5.959086584205519})"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer_3.evaluate()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "WyF37zj1QDmM",
        "outputId": "05d40e87-d643-4572-949e-a928e614b873"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='59' max='59' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [59/59 00:01]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'eval_loss': 0.5980727076530457,\n",
              " 'eval_accuracy': 0.7777777777777778,\n",
              " 'eval_precision': 0.7657031243115098,\n",
              " 'eval_recall': 0.7777777777777778,\n",
              " 'eval_f1': 0.7674804274690797,\n",
              " 'eval_runtime': 1.3268,\n",
              " 'eval_samples_per_second': 352.721,\n",
              " 'eval_steps_per_second': 44.467,\n",
              " 'epoch': 5.959086584205519}"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LoRA-4 with add-on targeted modules, `value` and `dense`"
      ],
      "metadata": {
        "id": "3I4mAnYPTXTM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_4 = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\",\n",
        "                                                        num_labels=3)\n",
        "\n",
        "# Configure LoRA parameters\n",
        "lora_config_4 = LoraConfig(\n",
        "    r=32,\n",
        "    lora_alpha=64, # a = r*2\n",
        "    target_modules=[\"query\", \"key\", \"value\", \"dense\"],\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    task_type=\"SEQ_CLS\"\n",
        ")\n",
        "\n",
        "# Wrap your model with LoRA layers\n",
        "model_4 = get_peft_model(model_4, lora_config_4)\n",
        "model_4.print_trainable_parameters()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GdMEj4rpTbFu",
        "outputId": "09af9e33-e240-4fa2-9e23-fafe29740f31"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 5,359,875 || all params: 114,844,422 || trainable%: 4.6671\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "# Define the training arguments\n",
        "training_args_lora_4 = TrainingArguments(\n",
        "    output_dir=\"./results-model_4\",\n",
        "    num_train_epochs=6,\n",
        "    per_device_train_batch_size=2, #mbs=2\n",
        "    gradient_accumulation_steps=16, # effective batch size 16*2\n",
        "    optim=\"adamw_torch\",\n",
        "    save_steps=200, # checkpoint every 200 steps\n",
        "    logging_steps=1,\n",
        "    learning_rate=2e-4, # step size in the optimizer update\n",
        "    weight_decay=0.001,\n",
        "    fp16=True, # 16 bit\n",
        "    bf16=False, # not supported on V100\n",
        "    max_grad_norm=0.3, #gradient clipping improves convergence\n",
        "    max_steps=-1,\n",
        "    warmup_ratio=0.03, # learning rate warmup\n",
        "    group_by_length=True,\n",
        "    lr_scheduler_type=\"cosine\" # cosine lr scheduler\n",
        ")\n",
        "\n",
        "trainer_4 = Trainer(\n",
        "    model=model_4,\n",
        "    args=training_args_lora_4,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "import gc # garbage collection\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache() # clean cache\n",
        "\n",
        "trainer_4.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "4Z_itfpcU7xl",
        "outputId": "549b2a4f-333e-473e-a6e7-3d4d227b9ef7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-26-e040854d55e0>:23: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer_4 = Trainer(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='786' max='786' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [786/786 16:29, Epoch 5/6]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.221000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.247000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.293500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.236800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>1.224400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>1.172200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>1.133100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>1.165400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>1.108300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.032400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>1.072000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>1.036800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>1.062700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>1.003000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>1.187300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>1.088300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>0.952100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>0.926200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>1.053200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>1.002400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>0.982700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>0.974200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>0.848400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>0.912900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>1.094600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>0.900200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>0.922800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>0.851000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>0.870500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.812800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>0.771000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>1.044800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>0.708400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>0.886000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>0.947000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>0.819100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>0.695800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>0.967800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>0.723900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.789300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41</td>\n",
              "      <td>0.807000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>0.591300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43</td>\n",
              "      <td>0.833300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>0.571200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>0.814600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46</td>\n",
              "      <td>0.791100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47</td>\n",
              "      <td>0.816800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>0.835500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49</td>\n",
              "      <td>0.744500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.626100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>51</td>\n",
              "      <td>0.667200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52</td>\n",
              "      <td>0.705600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>53</td>\n",
              "      <td>0.631200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>54</td>\n",
              "      <td>0.580100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55</td>\n",
              "      <td>0.672500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>0.638300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>57</td>\n",
              "      <td>0.405900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>58</td>\n",
              "      <td>0.854300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>59</td>\n",
              "      <td>0.854500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.944000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>61</td>\n",
              "      <td>0.730500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>62</td>\n",
              "      <td>1.106600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>63</td>\n",
              "      <td>0.854400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>64</td>\n",
              "      <td>0.752800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>65</td>\n",
              "      <td>0.702200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>66</td>\n",
              "      <td>0.739700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>67</td>\n",
              "      <td>0.738600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>68</td>\n",
              "      <td>0.698900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>69</td>\n",
              "      <td>1.015400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>0.704200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>71</td>\n",
              "      <td>0.800700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>72</td>\n",
              "      <td>0.749600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>73</td>\n",
              "      <td>0.790500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>74</td>\n",
              "      <td>0.777400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>75</td>\n",
              "      <td>0.624500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>76</td>\n",
              "      <td>0.658700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>77</td>\n",
              "      <td>0.628700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>78</td>\n",
              "      <td>0.530100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>79</td>\n",
              "      <td>0.617200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>0.650700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>81</td>\n",
              "      <td>0.953500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>82</td>\n",
              "      <td>0.601700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>83</td>\n",
              "      <td>0.692500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>84</td>\n",
              "      <td>0.943500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>85</td>\n",
              "      <td>0.861900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>86</td>\n",
              "      <td>0.732300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>87</td>\n",
              "      <td>0.494500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>88</td>\n",
              "      <td>0.411200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>89</td>\n",
              "      <td>0.437500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>0.618200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>91</td>\n",
              "      <td>0.801200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>92</td>\n",
              "      <td>0.961300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>93</td>\n",
              "      <td>0.698200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>94</td>\n",
              "      <td>0.587800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>95</td>\n",
              "      <td>0.543100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>96</td>\n",
              "      <td>0.661300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>97</td>\n",
              "      <td>0.654500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>98</td>\n",
              "      <td>0.843200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>99</td>\n",
              "      <td>0.444000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.651000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>101</td>\n",
              "      <td>0.738200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>102</td>\n",
              "      <td>0.514000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>103</td>\n",
              "      <td>0.612100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>104</td>\n",
              "      <td>1.039900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>105</td>\n",
              "      <td>0.324500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>106</td>\n",
              "      <td>0.778800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>107</td>\n",
              "      <td>0.628500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>108</td>\n",
              "      <td>0.345500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>109</td>\n",
              "      <td>0.657700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>0.470800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>111</td>\n",
              "      <td>0.643900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>112</td>\n",
              "      <td>0.571200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>113</td>\n",
              "      <td>0.648900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>114</td>\n",
              "      <td>0.581000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>115</td>\n",
              "      <td>0.649000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>116</td>\n",
              "      <td>0.798400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>117</td>\n",
              "      <td>0.507900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>118</td>\n",
              "      <td>0.348700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>119</td>\n",
              "      <td>0.594100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>0.703500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>121</td>\n",
              "      <td>0.690800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>122</td>\n",
              "      <td>0.381200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>123</td>\n",
              "      <td>0.434700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>124</td>\n",
              "      <td>0.650900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>125</td>\n",
              "      <td>0.622900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>126</td>\n",
              "      <td>0.647500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>127</td>\n",
              "      <td>0.770600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>128</td>\n",
              "      <td>0.551300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>129</td>\n",
              "      <td>0.624700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>0.572900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>131</td>\n",
              "      <td>0.597100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>132</td>\n",
              "      <td>0.285000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>133</td>\n",
              "      <td>0.632100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>134</td>\n",
              "      <td>0.654100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>135</td>\n",
              "      <td>0.789100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>136</td>\n",
              "      <td>0.741700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>137</td>\n",
              "      <td>0.623100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>138</td>\n",
              "      <td>0.367700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>139</td>\n",
              "      <td>0.538000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>0.483800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>141</td>\n",
              "      <td>0.612500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>142</td>\n",
              "      <td>0.529700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>143</td>\n",
              "      <td>0.558900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>144</td>\n",
              "      <td>0.569500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>145</td>\n",
              "      <td>0.272200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>146</td>\n",
              "      <td>0.424900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>147</td>\n",
              "      <td>0.565800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>148</td>\n",
              "      <td>0.361600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>149</td>\n",
              "      <td>0.640700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.267300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>151</td>\n",
              "      <td>0.796100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>152</td>\n",
              "      <td>0.355300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>153</td>\n",
              "      <td>0.926900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>154</td>\n",
              "      <td>0.316100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>155</td>\n",
              "      <td>0.344400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>156</td>\n",
              "      <td>0.796800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>157</td>\n",
              "      <td>0.611400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>158</td>\n",
              "      <td>0.371800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>159</td>\n",
              "      <td>0.362700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>0.531400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>161</td>\n",
              "      <td>0.495600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>162</td>\n",
              "      <td>0.551800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>163</td>\n",
              "      <td>0.464000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>164</td>\n",
              "      <td>0.551500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>165</td>\n",
              "      <td>0.722900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>166</td>\n",
              "      <td>0.613000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>167</td>\n",
              "      <td>0.555600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>168</td>\n",
              "      <td>0.492700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>169</td>\n",
              "      <td>0.885500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>0.390200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>171</td>\n",
              "      <td>0.462600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>172</td>\n",
              "      <td>0.545300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>173</td>\n",
              "      <td>0.517600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>174</td>\n",
              "      <td>0.751800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>175</td>\n",
              "      <td>0.563600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>176</td>\n",
              "      <td>0.334100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>177</td>\n",
              "      <td>0.476300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>178</td>\n",
              "      <td>0.433100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>179</td>\n",
              "      <td>0.312300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>0.658100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>181</td>\n",
              "      <td>0.527200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>182</td>\n",
              "      <td>0.455100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>183</td>\n",
              "      <td>0.557000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>184</td>\n",
              "      <td>0.524700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>185</td>\n",
              "      <td>0.719200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>186</td>\n",
              "      <td>0.431400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>187</td>\n",
              "      <td>0.275800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>188</td>\n",
              "      <td>0.686100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>189</td>\n",
              "      <td>0.472700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>190</td>\n",
              "      <td>0.484100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>191</td>\n",
              "      <td>0.589900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>192</td>\n",
              "      <td>0.671600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>193</td>\n",
              "      <td>0.348400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>194</td>\n",
              "      <td>0.417000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>195</td>\n",
              "      <td>0.259800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>196</td>\n",
              "      <td>0.391700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>197</td>\n",
              "      <td>0.721600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>198</td>\n",
              "      <td>0.667300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>199</td>\n",
              "      <td>0.367100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.802500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>201</td>\n",
              "      <td>0.622000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>202</td>\n",
              "      <td>0.529600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>203</td>\n",
              "      <td>0.726200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>204</td>\n",
              "      <td>0.946400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>205</td>\n",
              "      <td>0.423800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>206</td>\n",
              "      <td>0.480300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>207</td>\n",
              "      <td>0.481100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>208</td>\n",
              "      <td>0.487100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>209</td>\n",
              "      <td>0.641400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>210</td>\n",
              "      <td>0.532300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>211</td>\n",
              "      <td>0.536500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>212</td>\n",
              "      <td>0.435900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>213</td>\n",
              "      <td>0.558300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>214</td>\n",
              "      <td>0.363100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>215</td>\n",
              "      <td>0.489500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>216</td>\n",
              "      <td>0.386500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>217</td>\n",
              "      <td>0.477700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>218</td>\n",
              "      <td>1.064400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>219</td>\n",
              "      <td>0.409300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>220</td>\n",
              "      <td>0.581500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>221</td>\n",
              "      <td>0.294500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>222</td>\n",
              "      <td>0.741200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>223</td>\n",
              "      <td>0.483400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>224</td>\n",
              "      <td>0.736700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>225</td>\n",
              "      <td>0.671300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>226</td>\n",
              "      <td>0.660600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>227</td>\n",
              "      <td>0.491300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>228</td>\n",
              "      <td>0.526400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>229</td>\n",
              "      <td>0.437500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>230</td>\n",
              "      <td>0.746300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>231</td>\n",
              "      <td>0.406200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>232</td>\n",
              "      <td>0.554200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>233</td>\n",
              "      <td>0.831900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>234</td>\n",
              "      <td>0.401900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>235</td>\n",
              "      <td>0.516300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>236</td>\n",
              "      <td>0.507400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>237</td>\n",
              "      <td>0.535500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>238</td>\n",
              "      <td>0.654800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>239</td>\n",
              "      <td>0.373100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>240</td>\n",
              "      <td>0.800400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>241</td>\n",
              "      <td>0.777200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>242</td>\n",
              "      <td>0.339100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>243</td>\n",
              "      <td>0.432900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>244</td>\n",
              "      <td>0.511700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>245</td>\n",
              "      <td>0.368200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>246</td>\n",
              "      <td>0.660900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>247</td>\n",
              "      <td>0.640500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>248</td>\n",
              "      <td>0.461400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>249</td>\n",
              "      <td>0.667600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>0.630900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>251</td>\n",
              "      <td>0.760200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>252</td>\n",
              "      <td>0.609100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>253</td>\n",
              "      <td>0.588200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>254</td>\n",
              "      <td>0.539800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>255</td>\n",
              "      <td>0.535900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>256</td>\n",
              "      <td>0.555600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>257</td>\n",
              "      <td>0.536000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>258</td>\n",
              "      <td>0.566700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>259</td>\n",
              "      <td>0.639200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>260</td>\n",
              "      <td>0.528200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>261</td>\n",
              "      <td>0.567000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>262</td>\n",
              "      <td>0.402300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>263</td>\n",
              "      <td>0.611100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>264</td>\n",
              "      <td>0.296100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>265</td>\n",
              "      <td>0.547100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>266</td>\n",
              "      <td>0.306600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>267</td>\n",
              "      <td>0.416100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>268</td>\n",
              "      <td>0.439900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>269</td>\n",
              "      <td>0.479600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>270</td>\n",
              "      <td>0.636900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>271</td>\n",
              "      <td>0.531800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>272</td>\n",
              "      <td>0.627900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>273</td>\n",
              "      <td>0.590100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>274</td>\n",
              "      <td>0.448000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>275</td>\n",
              "      <td>0.743600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>276</td>\n",
              "      <td>0.472700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>277</td>\n",
              "      <td>0.355400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>278</td>\n",
              "      <td>0.335900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>279</td>\n",
              "      <td>0.359500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>280</td>\n",
              "      <td>0.562300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>281</td>\n",
              "      <td>0.545800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>282</td>\n",
              "      <td>0.426600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>283</td>\n",
              "      <td>0.395000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>284</td>\n",
              "      <td>0.403800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>285</td>\n",
              "      <td>0.607900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>286</td>\n",
              "      <td>0.368500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>287</td>\n",
              "      <td>0.453200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>288</td>\n",
              "      <td>0.591400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>289</td>\n",
              "      <td>0.421100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>290</td>\n",
              "      <td>0.436800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>291</td>\n",
              "      <td>0.550500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>292</td>\n",
              "      <td>0.584200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>293</td>\n",
              "      <td>0.198800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>294</td>\n",
              "      <td>0.231200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>295</td>\n",
              "      <td>0.784000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>296</td>\n",
              "      <td>0.313300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>297</td>\n",
              "      <td>0.352600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>298</td>\n",
              "      <td>0.396900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>299</td>\n",
              "      <td>0.704300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.489000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>301</td>\n",
              "      <td>0.934700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>302</td>\n",
              "      <td>0.314400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>303</td>\n",
              "      <td>0.360900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>304</td>\n",
              "      <td>0.503400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>305</td>\n",
              "      <td>0.583000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>306</td>\n",
              "      <td>0.507600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>307</td>\n",
              "      <td>0.357800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>308</td>\n",
              "      <td>0.578200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>309</td>\n",
              "      <td>0.329900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>310</td>\n",
              "      <td>0.520200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>311</td>\n",
              "      <td>0.314300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>312</td>\n",
              "      <td>0.474500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>313</td>\n",
              "      <td>0.454900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>314</td>\n",
              "      <td>0.239200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>315</td>\n",
              "      <td>0.322700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>316</td>\n",
              "      <td>0.244800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>317</td>\n",
              "      <td>0.340000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>318</td>\n",
              "      <td>0.452400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>319</td>\n",
              "      <td>0.231400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>320</td>\n",
              "      <td>0.506900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>321</td>\n",
              "      <td>0.694700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>322</td>\n",
              "      <td>0.104300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>323</td>\n",
              "      <td>0.645100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>324</td>\n",
              "      <td>0.482900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>325</td>\n",
              "      <td>0.315000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>326</td>\n",
              "      <td>0.665200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>327</td>\n",
              "      <td>0.075300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>328</td>\n",
              "      <td>0.483400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>329</td>\n",
              "      <td>0.678400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>330</td>\n",
              "      <td>0.498900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>331</td>\n",
              "      <td>0.398600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>332</td>\n",
              "      <td>0.647900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>333</td>\n",
              "      <td>0.502300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>334</td>\n",
              "      <td>0.351200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>335</td>\n",
              "      <td>0.460100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>336</td>\n",
              "      <td>0.377600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>337</td>\n",
              "      <td>0.441800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>338</td>\n",
              "      <td>0.528200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>339</td>\n",
              "      <td>0.474900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>340</td>\n",
              "      <td>0.754400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>341</td>\n",
              "      <td>0.451500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>342</td>\n",
              "      <td>0.358800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>343</td>\n",
              "      <td>0.476000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>344</td>\n",
              "      <td>0.453900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>345</td>\n",
              "      <td>0.349400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>346</td>\n",
              "      <td>0.557200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>347</td>\n",
              "      <td>0.459400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>348</td>\n",
              "      <td>0.316200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>349</td>\n",
              "      <td>0.261300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>0.415100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>351</td>\n",
              "      <td>0.501800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>352</td>\n",
              "      <td>0.547100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>353</td>\n",
              "      <td>0.354200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>354</td>\n",
              "      <td>0.375800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>355</td>\n",
              "      <td>0.460600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>356</td>\n",
              "      <td>0.404300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>357</td>\n",
              "      <td>0.592400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>358</td>\n",
              "      <td>0.337100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>359</td>\n",
              "      <td>0.351700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>360</td>\n",
              "      <td>0.549300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>361</td>\n",
              "      <td>0.428100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>362</td>\n",
              "      <td>0.625900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>363</td>\n",
              "      <td>0.463300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>364</td>\n",
              "      <td>0.268000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>365</td>\n",
              "      <td>0.392800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>366</td>\n",
              "      <td>0.447700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>367</td>\n",
              "      <td>0.469900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>368</td>\n",
              "      <td>0.602300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>369</td>\n",
              "      <td>0.612500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>370</td>\n",
              "      <td>0.238400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>371</td>\n",
              "      <td>0.522700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>372</td>\n",
              "      <td>0.241100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>373</td>\n",
              "      <td>0.526800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>374</td>\n",
              "      <td>0.430900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>375</td>\n",
              "      <td>0.600100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>376</td>\n",
              "      <td>0.577400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>377</td>\n",
              "      <td>0.431900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>378</td>\n",
              "      <td>0.373900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>379</td>\n",
              "      <td>0.700200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>380</td>\n",
              "      <td>0.283300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>381</td>\n",
              "      <td>0.371300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>382</td>\n",
              "      <td>0.313700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>383</td>\n",
              "      <td>0.388200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>384</td>\n",
              "      <td>0.448200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>385</td>\n",
              "      <td>0.582200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>386</td>\n",
              "      <td>0.311900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>387</td>\n",
              "      <td>0.545100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>388</td>\n",
              "      <td>0.357900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>389</td>\n",
              "      <td>0.525400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>390</td>\n",
              "      <td>0.369000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>391</td>\n",
              "      <td>0.475100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>392</td>\n",
              "      <td>0.396400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>393</td>\n",
              "      <td>0.651300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>394</td>\n",
              "      <td>0.400200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>395</td>\n",
              "      <td>0.633500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>396</td>\n",
              "      <td>0.190800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>397</td>\n",
              "      <td>0.249800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>398</td>\n",
              "      <td>0.372300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>399</td>\n",
              "      <td>0.281900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.393100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>401</td>\n",
              "      <td>0.297000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>402</td>\n",
              "      <td>0.514000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>403</td>\n",
              "      <td>0.366300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>404</td>\n",
              "      <td>0.435600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>405</td>\n",
              "      <td>0.395200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>406</td>\n",
              "      <td>0.454200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>407</td>\n",
              "      <td>0.236000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>408</td>\n",
              "      <td>0.306500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>409</td>\n",
              "      <td>0.329900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>410</td>\n",
              "      <td>0.422100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>411</td>\n",
              "      <td>0.513800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>412</td>\n",
              "      <td>0.246500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>413</td>\n",
              "      <td>0.324800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>414</td>\n",
              "      <td>0.288600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>415</td>\n",
              "      <td>0.693900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>416</td>\n",
              "      <td>0.246800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>417</td>\n",
              "      <td>0.550600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>418</td>\n",
              "      <td>0.326800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>419</td>\n",
              "      <td>0.244800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>420</td>\n",
              "      <td>0.333900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>421</td>\n",
              "      <td>0.380900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>422</td>\n",
              "      <td>0.443700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>423</td>\n",
              "      <td>0.254900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>424</td>\n",
              "      <td>0.471400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>425</td>\n",
              "      <td>0.400000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>426</td>\n",
              "      <td>0.484500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>427</td>\n",
              "      <td>0.421600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>428</td>\n",
              "      <td>0.379500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>429</td>\n",
              "      <td>0.235600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>430</td>\n",
              "      <td>0.303300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>431</td>\n",
              "      <td>0.155400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>432</td>\n",
              "      <td>0.412400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>433</td>\n",
              "      <td>0.210800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>434</td>\n",
              "      <td>0.272500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>435</td>\n",
              "      <td>0.226300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>436</td>\n",
              "      <td>0.392200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>437</td>\n",
              "      <td>0.617100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>438</td>\n",
              "      <td>0.319100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>439</td>\n",
              "      <td>0.542200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>440</td>\n",
              "      <td>0.356300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>441</td>\n",
              "      <td>0.312500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>442</td>\n",
              "      <td>0.258800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>443</td>\n",
              "      <td>0.193300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>444</td>\n",
              "      <td>0.593800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>445</td>\n",
              "      <td>0.518300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>446</td>\n",
              "      <td>0.261700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>447</td>\n",
              "      <td>0.511800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>448</td>\n",
              "      <td>0.387700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>449</td>\n",
              "      <td>0.458100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>0.330400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>451</td>\n",
              "      <td>0.505800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>452</td>\n",
              "      <td>0.442500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>453</td>\n",
              "      <td>0.371300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>454</td>\n",
              "      <td>0.430100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>455</td>\n",
              "      <td>0.180200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>456</td>\n",
              "      <td>0.309100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>457</td>\n",
              "      <td>0.379300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>458</td>\n",
              "      <td>0.363700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>459</td>\n",
              "      <td>0.330100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>460</td>\n",
              "      <td>0.318900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>461</td>\n",
              "      <td>0.246100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>462</td>\n",
              "      <td>0.214100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>463</td>\n",
              "      <td>0.348100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>464</td>\n",
              "      <td>0.553500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>465</td>\n",
              "      <td>0.506600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>466</td>\n",
              "      <td>0.416300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>467</td>\n",
              "      <td>0.370300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>468</td>\n",
              "      <td>0.332900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>469</td>\n",
              "      <td>0.394700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>470</td>\n",
              "      <td>0.560500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>471</td>\n",
              "      <td>0.693300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>472</td>\n",
              "      <td>0.322200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>473</td>\n",
              "      <td>0.381400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>474</td>\n",
              "      <td>0.361500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>475</td>\n",
              "      <td>0.260000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>476</td>\n",
              "      <td>0.293000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>477</td>\n",
              "      <td>0.201600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>478</td>\n",
              "      <td>0.343200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>479</td>\n",
              "      <td>0.321500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>480</td>\n",
              "      <td>0.493400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>481</td>\n",
              "      <td>0.288300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>482</td>\n",
              "      <td>0.247100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>483</td>\n",
              "      <td>0.576000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>484</td>\n",
              "      <td>0.318500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>485</td>\n",
              "      <td>0.380200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>486</td>\n",
              "      <td>0.375100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>487</td>\n",
              "      <td>0.428300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>488</td>\n",
              "      <td>0.360500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>489</td>\n",
              "      <td>0.394400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>490</td>\n",
              "      <td>0.376400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>491</td>\n",
              "      <td>0.278300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>492</td>\n",
              "      <td>0.544500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>493</td>\n",
              "      <td>0.495900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>494</td>\n",
              "      <td>0.319500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>495</td>\n",
              "      <td>1.003600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>496</td>\n",
              "      <td>0.344000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>497</td>\n",
              "      <td>0.406500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>498</td>\n",
              "      <td>0.279400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>499</td>\n",
              "      <td>0.350800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.343400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>501</td>\n",
              "      <td>0.377100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>502</td>\n",
              "      <td>0.552400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>503</td>\n",
              "      <td>0.158700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>504</td>\n",
              "      <td>0.454900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>505</td>\n",
              "      <td>0.321200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>506</td>\n",
              "      <td>0.342700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>507</td>\n",
              "      <td>0.318200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>508</td>\n",
              "      <td>0.538200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>509</td>\n",
              "      <td>0.432600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>510</td>\n",
              "      <td>0.424200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>511</td>\n",
              "      <td>0.267600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>512</td>\n",
              "      <td>0.367000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>513</td>\n",
              "      <td>0.183400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>514</td>\n",
              "      <td>0.319800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>515</td>\n",
              "      <td>0.292300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>516</td>\n",
              "      <td>0.362900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>517</td>\n",
              "      <td>0.271800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>518</td>\n",
              "      <td>0.269300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>519</td>\n",
              "      <td>0.364800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>520</td>\n",
              "      <td>0.508600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>521</td>\n",
              "      <td>0.437100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>522</td>\n",
              "      <td>0.472900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>523</td>\n",
              "      <td>0.359800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>524</td>\n",
              "      <td>0.337800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>525</td>\n",
              "      <td>0.516000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>526</td>\n",
              "      <td>0.319500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>527</td>\n",
              "      <td>0.146200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>528</td>\n",
              "      <td>0.239300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>529</td>\n",
              "      <td>0.485300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>530</td>\n",
              "      <td>0.464200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>531</td>\n",
              "      <td>0.443200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>532</td>\n",
              "      <td>0.244500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>533</td>\n",
              "      <td>0.394700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>534</td>\n",
              "      <td>0.384500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>535</td>\n",
              "      <td>0.277000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>536</td>\n",
              "      <td>0.263900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>537</td>\n",
              "      <td>0.450500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>538</td>\n",
              "      <td>0.409400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>539</td>\n",
              "      <td>0.266100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>540</td>\n",
              "      <td>0.198000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>541</td>\n",
              "      <td>0.315400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>542</td>\n",
              "      <td>0.200700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>543</td>\n",
              "      <td>0.308800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>544</td>\n",
              "      <td>0.239100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>545</td>\n",
              "      <td>0.365100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>546</td>\n",
              "      <td>0.368100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>547</td>\n",
              "      <td>0.301700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>548</td>\n",
              "      <td>0.280500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>549</td>\n",
              "      <td>0.234000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>550</td>\n",
              "      <td>0.483100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>551</td>\n",
              "      <td>0.198300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>552</td>\n",
              "      <td>0.208500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>553</td>\n",
              "      <td>0.473300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>554</td>\n",
              "      <td>0.345600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>555</td>\n",
              "      <td>0.157400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>556</td>\n",
              "      <td>0.109200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>557</td>\n",
              "      <td>0.258200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>558</td>\n",
              "      <td>0.367700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>559</td>\n",
              "      <td>0.384900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>560</td>\n",
              "      <td>0.138600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>561</td>\n",
              "      <td>0.580600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>562</td>\n",
              "      <td>0.388600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>563</td>\n",
              "      <td>0.268100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>564</td>\n",
              "      <td>0.501700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>565</td>\n",
              "      <td>0.194200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>566</td>\n",
              "      <td>0.297000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>567</td>\n",
              "      <td>0.315800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>568</td>\n",
              "      <td>0.259700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>569</td>\n",
              "      <td>0.268200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>570</td>\n",
              "      <td>0.247700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>571</td>\n",
              "      <td>0.429800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>572</td>\n",
              "      <td>0.243400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>573</td>\n",
              "      <td>0.159500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>574</td>\n",
              "      <td>0.380200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>575</td>\n",
              "      <td>0.078200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>576</td>\n",
              "      <td>0.171000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>577</td>\n",
              "      <td>0.496600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>578</td>\n",
              "      <td>0.104500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>579</td>\n",
              "      <td>0.161000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>580</td>\n",
              "      <td>0.359700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>581</td>\n",
              "      <td>0.201600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>582</td>\n",
              "      <td>0.072900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>583</td>\n",
              "      <td>0.251300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>584</td>\n",
              "      <td>0.397500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>585</td>\n",
              "      <td>0.364700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>586</td>\n",
              "      <td>0.229900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>587</td>\n",
              "      <td>0.544500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>588</td>\n",
              "      <td>0.297200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>589</td>\n",
              "      <td>0.411100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>590</td>\n",
              "      <td>0.394400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>591</td>\n",
              "      <td>0.172200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>592</td>\n",
              "      <td>0.426400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>593</td>\n",
              "      <td>0.179500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>594</td>\n",
              "      <td>0.147600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>595</td>\n",
              "      <td>0.349100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>596</td>\n",
              "      <td>0.258100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>597</td>\n",
              "      <td>0.260200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>598</td>\n",
              "      <td>0.266800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>599</td>\n",
              "      <td>0.322900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.211300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>601</td>\n",
              "      <td>0.348700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>602</td>\n",
              "      <td>0.220400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>603</td>\n",
              "      <td>0.254900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>604</td>\n",
              "      <td>0.233700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>605</td>\n",
              "      <td>0.321000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>606</td>\n",
              "      <td>0.634200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>607</td>\n",
              "      <td>0.448400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>608</td>\n",
              "      <td>0.520300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>609</td>\n",
              "      <td>0.590100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>610</td>\n",
              "      <td>0.503900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>611</td>\n",
              "      <td>0.375900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>612</td>\n",
              "      <td>0.339100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>613</td>\n",
              "      <td>0.335200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>614</td>\n",
              "      <td>0.325000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>615</td>\n",
              "      <td>0.321500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>616</td>\n",
              "      <td>0.311300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>617</td>\n",
              "      <td>0.329300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>618</td>\n",
              "      <td>0.567100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>619</td>\n",
              "      <td>0.462100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>620</td>\n",
              "      <td>0.071800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>621</td>\n",
              "      <td>0.374900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>622</td>\n",
              "      <td>0.747600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>623</td>\n",
              "      <td>0.191100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>624</td>\n",
              "      <td>0.209800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>625</td>\n",
              "      <td>0.386100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>626</td>\n",
              "      <td>0.498900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>627</td>\n",
              "      <td>0.411500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>628</td>\n",
              "      <td>0.401400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>629</td>\n",
              "      <td>0.162800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>630</td>\n",
              "      <td>0.323500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>631</td>\n",
              "      <td>0.343600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>632</td>\n",
              "      <td>0.279200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>633</td>\n",
              "      <td>0.453100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>634</td>\n",
              "      <td>0.557700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>635</td>\n",
              "      <td>0.236100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>636</td>\n",
              "      <td>0.397300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>637</td>\n",
              "      <td>0.269200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>638</td>\n",
              "      <td>0.308900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>639</td>\n",
              "      <td>0.321100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>640</td>\n",
              "      <td>0.492100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>641</td>\n",
              "      <td>0.341400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>642</td>\n",
              "      <td>0.280600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>643</td>\n",
              "      <td>0.087700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>644</td>\n",
              "      <td>0.253900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>645</td>\n",
              "      <td>0.161300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>646</td>\n",
              "      <td>0.265000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>647</td>\n",
              "      <td>0.360100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>648</td>\n",
              "      <td>0.208500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>649</td>\n",
              "      <td>0.565000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>650</td>\n",
              "      <td>0.150000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>651</td>\n",
              "      <td>0.438100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>652</td>\n",
              "      <td>0.259600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>653</td>\n",
              "      <td>0.249400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>654</td>\n",
              "      <td>0.319000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>655</td>\n",
              "      <td>0.412100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>656</td>\n",
              "      <td>0.313900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>657</td>\n",
              "      <td>0.418400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>658</td>\n",
              "      <td>0.110800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>659</td>\n",
              "      <td>0.309100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>660</td>\n",
              "      <td>0.064900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>661</td>\n",
              "      <td>0.256800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>662</td>\n",
              "      <td>0.284200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>663</td>\n",
              "      <td>0.073600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>664</td>\n",
              "      <td>0.234900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>665</td>\n",
              "      <td>0.127200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>666</td>\n",
              "      <td>0.397400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>667</td>\n",
              "      <td>0.201800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>668</td>\n",
              "      <td>0.239700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>669</td>\n",
              "      <td>0.286800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>670</td>\n",
              "      <td>0.649400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>671</td>\n",
              "      <td>0.215700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>672</td>\n",
              "      <td>0.348000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>673</td>\n",
              "      <td>0.236100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>674</td>\n",
              "      <td>0.529400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>675</td>\n",
              "      <td>0.257700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>676</td>\n",
              "      <td>0.551000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>677</td>\n",
              "      <td>0.285600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>678</td>\n",
              "      <td>0.145500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>679</td>\n",
              "      <td>0.242500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>680</td>\n",
              "      <td>0.377300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>681</td>\n",
              "      <td>0.288800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>682</td>\n",
              "      <td>0.194000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>683</td>\n",
              "      <td>0.340400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>684</td>\n",
              "      <td>0.532500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>685</td>\n",
              "      <td>0.199600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>686</td>\n",
              "      <td>0.186200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>687</td>\n",
              "      <td>0.192500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>688</td>\n",
              "      <td>0.294500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>689</td>\n",
              "      <td>0.452700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>690</td>\n",
              "      <td>0.157900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>691</td>\n",
              "      <td>0.412600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>692</td>\n",
              "      <td>0.212800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>693</td>\n",
              "      <td>0.325400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>694</td>\n",
              "      <td>0.297700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>695</td>\n",
              "      <td>0.254100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>696</td>\n",
              "      <td>0.353300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>697</td>\n",
              "      <td>0.318900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>698</td>\n",
              "      <td>0.178800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>699</td>\n",
              "      <td>0.150400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>0.143700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>701</td>\n",
              "      <td>0.483500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>702</td>\n",
              "      <td>0.185000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>703</td>\n",
              "      <td>0.148100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>704</td>\n",
              "      <td>0.396900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>705</td>\n",
              "      <td>0.358300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>706</td>\n",
              "      <td>0.464900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>707</td>\n",
              "      <td>0.352800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>708</td>\n",
              "      <td>0.191100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>709</td>\n",
              "      <td>0.196200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>710</td>\n",
              "      <td>0.199700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>711</td>\n",
              "      <td>0.335300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>712</td>\n",
              "      <td>0.397000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>713</td>\n",
              "      <td>0.208400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>714</td>\n",
              "      <td>0.339000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>715</td>\n",
              "      <td>0.321300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>716</td>\n",
              "      <td>0.143800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>717</td>\n",
              "      <td>0.318700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>718</td>\n",
              "      <td>0.392100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>719</td>\n",
              "      <td>0.188300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>720</td>\n",
              "      <td>0.259300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>721</td>\n",
              "      <td>0.390000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>722</td>\n",
              "      <td>0.403200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>723</td>\n",
              "      <td>0.342100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>724</td>\n",
              "      <td>0.136200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>725</td>\n",
              "      <td>0.288800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>726</td>\n",
              "      <td>0.204500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>727</td>\n",
              "      <td>0.359800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>728</td>\n",
              "      <td>0.097600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>729</td>\n",
              "      <td>0.364000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>730</td>\n",
              "      <td>0.206500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>731</td>\n",
              "      <td>0.244100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>732</td>\n",
              "      <td>0.198400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>733</td>\n",
              "      <td>0.227300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>734</td>\n",
              "      <td>0.497000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>735</td>\n",
              "      <td>0.262300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>736</td>\n",
              "      <td>0.391600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>737</td>\n",
              "      <td>0.150000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>738</td>\n",
              "      <td>0.184100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>739</td>\n",
              "      <td>0.153600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>740</td>\n",
              "      <td>0.216500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>741</td>\n",
              "      <td>0.286900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>742</td>\n",
              "      <td>0.320000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>743</td>\n",
              "      <td>0.145400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>744</td>\n",
              "      <td>0.095800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>745</td>\n",
              "      <td>0.271800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>746</td>\n",
              "      <td>0.274700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>747</td>\n",
              "      <td>0.085700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>748</td>\n",
              "      <td>0.364900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>749</td>\n",
              "      <td>0.368600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>750</td>\n",
              "      <td>0.419300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>751</td>\n",
              "      <td>0.334900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>752</td>\n",
              "      <td>0.235100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>753</td>\n",
              "      <td>0.147200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>754</td>\n",
              "      <td>0.278500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>755</td>\n",
              "      <td>0.540300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>756</td>\n",
              "      <td>0.381000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>757</td>\n",
              "      <td>0.253200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>758</td>\n",
              "      <td>0.524300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>759</td>\n",
              "      <td>0.187100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>760</td>\n",
              "      <td>0.227400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>761</td>\n",
              "      <td>0.256500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>762</td>\n",
              "      <td>0.530700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>763</td>\n",
              "      <td>0.295300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>764</td>\n",
              "      <td>0.230600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>765</td>\n",
              "      <td>0.486400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>766</td>\n",
              "      <td>0.372000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>767</td>\n",
              "      <td>0.256700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>768</td>\n",
              "      <td>0.452100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>769</td>\n",
              "      <td>0.110100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>770</td>\n",
              "      <td>0.373100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>771</td>\n",
              "      <td>0.272400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>772</td>\n",
              "      <td>0.315600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>773</td>\n",
              "      <td>0.216000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>774</td>\n",
              "      <td>0.492300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>775</td>\n",
              "      <td>0.116600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>776</td>\n",
              "      <td>0.501300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>777</td>\n",
              "      <td>0.222200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>778</td>\n",
              "      <td>0.189200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>779</td>\n",
              "      <td>0.270600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>780</td>\n",
              "      <td>0.180600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>781</td>\n",
              "      <td>0.265100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>782</td>\n",
              "      <td>0.300800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>783</td>\n",
              "      <td>0.155800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>784</td>\n",
              "      <td>0.212200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>785</td>\n",
              "      <td>0.226700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>786</td>\n",
              "      <td>0.156400</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=786, training_loss=0.4589926237216736, metrics={'train_runtime': 990.8968, 'train_samples_per_second': 25.456, 'train_steps_per_second': 0.793, 'total_flos': 1751002987198464.0, 'train_loss': 0.4589926237216736, 'epoch': 5.959086584205519})"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer_4.evaluate()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "YOctz5qBU9sG",
        "outputId": "b0084863-e3d8-4afd-9101-ad6c184f21ae"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='59' max='59' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [59/59 00:02]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'eval_loss': 0.7281752228736877,\n",
              " 'eval_accuracy': 0.7521367521367521,\n",
              " 'eval_precision': 0.745759536178415,\n",
              " 'eval_recall': 0.7521367521367521,\n",
              " 'eval_f1': 0.748593640087723,\n",
              " 'eval_runtime': 2.2541,\n",
              " 'eval_samples_per_second': 207.62,\n",
              " 'eval_steps_per_second': 26.174,\n",
              " 'epoch': 5.959086584205519}"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aKb4ZSqwcjU4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LoRA-5, targeting all linear layers with r32 a64"
      ],
      "metadata": {
        "id": "K5YJfIHocjd8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_5 = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\",\n",
        "                                                        num_labels=3)\n",
        "\n",
        "# Configure LoRA parameters\n",
        "lora_config_5 = LoraConfig(\n",
        "    r=32,\n",
        "    lora_alpha=64, # a = r*2\n",
        "    target_modules=[\"query\", \"key\", \"value\", \"dense\",\n",
        "                    \"intermediate.dense\", \"output.dense\"],\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    task_type=\"SEQ_CLS\"\n",
        ")\n",
        "\n",
        "# Wrap your model with LoRA layers\n",
        "model_5 = get_peft_model(model_5, lora_config_5)\n",
        "model_5.print_trainable_parameters()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "34ebe635-3f32-408b-de68-497977d4e14f",
        "id": "6yLEtHUJcjd9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 5,359,875 || all params: 114,844,422 || trainable%: 4.6671\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import DataCollatorWithPadding\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "# Define the training arguments\n",
        "training_args_lora_5 = TrainingArguments(\n",
        "    output_dir=\"./results-model_5\",\n",
        "    num_train_epochs=5,\n",
        "    per_device_train_batch_size=2, #mbs=2\n",
        "    gradient_accumulation_steps=16, # effective batch size 16*2\n",
        "    optim=\"adamw_torch\",\n",
        "    save_steps=200, # checkpoint every 200 steps\n",
        "    logging_steps=1,\n",
        "    learning_rate=2e-4, # step size in the optimizer update\n",
        "    weight_decay=0.001,\n",
        "    fp16=True, # 16 bit\n",
        "    bf16=False, # not supported on V100\n",
        "    max_grad_norm=0.3, #gradient clipping improves convergence\n",
        "    max_steps=-1,\n",
        "    warmup_ratio=0.03, # learning rate warmup\n",
        "    group_by_length=True,\n",
        "    lr_scheduler_type=\"cosine\" # cosine lr scheduler\n",
        ")\n",
        "\n",
        "trainer_5 = Trainer(\n",
        "    model=model_5,\n",
        "    args=training_args_lora_5,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "import gc # garbage collection\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache() # clean cache\n",
        "\n",
        "trainer_5.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "19e39ec0-b8d1-4fcc-96b4-8e72b907c462",
        "id": "ihl9f4zOcjd9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-16-5dfa36bf3e33>:26: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
            "  trainer_5 = Trainer(\n",
            "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "        window._wandbApiKey = new Promise((resolve, reject) => {\n",
              "            function loadScript(url) {\n",
              "            return new Promise(function(resolve, reject) {\n",
              "                let newScript = document.createElement(\"script\");\n",
              "                newScript.onerror = reject;\n",
              "                newScript.onload = resolve;\n",
              "                document.body.appendChild(newScript);\n",
              "                newScript.src = url;\n",
              "            });\n",
              "            }\n",
              "            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n",
              "            const iframe = document.createElement('iframe')\n",
              "            iframe.style.cssText = \"width:0;height:0;border:none\"\n",
              "            document.body.appendChild(iframe)\n",
              "            const handshake = new Postmate({\n",
              "                container: iframe,\n",
              "                url: 'https://wandb.ai/authorize'\n",
              "            });\n",
              "            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n",
              "            handshake.then(function(child) {\n",
              "                child.on('authorize', data => {\n",
              "                    clearTimeout(timeout)\n",
              "                    resolve(data)\n",
              "                });\n",
              "            });\n",
              "            })\n",
              "        });\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "wandb: Paste an API key from your profile and hit enter:"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " ··········\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmahkotasteam\u001b[0m (\u001b[33mmahkotasteam-asia-pacific-university-of-technology-innov\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Tracking run with wandb version 0.19.6"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20250217_155045-z6ti1ykr</code>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/mahkotasteam-asia-pacific-university-of-technology-innov/huggingface/runs/z6ti1ykr' target=\"_blank\">./results-model_5</a></strong> to <a href='https://wandb.ai/mahkotasteam-asia-pacific-university-of-technology-innov/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View project at <a href='https://wandb.ai/mahkotasteam-asia-pacific-university-of-technology-innov/huggingface' target=\"_blank\">https://wandb.ai/mahkotasteam-asia-pacific-university-of-technology-innov/huggingface</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              " View run at <a href='https://wandb.ai/mahkotasteam-asia-pacific-university-of-technology-innov/huggingface/runs/z6ti1ykr' target=\"_blank\">https://wandb.ai/mahkotasteam-asia-pacific-university-of-technology-innov/huggingface/runs/z6ti1ykr</a>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='655' max='655' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [655/655 14:21, Epoch 4/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.107700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.149200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.137900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.173600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>1.088200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>1.137400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>1.106000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>1.091800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>9</td>\n",
              "      <td>1.008200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.019600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>11</td>\n",
              "      <td>1.022000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>12</td>\n",
              "      <td>1.026000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>13</td>\n",
              "      <td>0.996700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>14</td>\n",
              "      <td>0.992600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>15</td>\n",
              "      <td>1.155000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>16</td>\n",
              "      <td>1.119100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>17</td>\n",
              "      <td>0.940100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>18</td>\n",
              "      <td>0.943300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>19</td>\n",
              "      <td>1.087700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>1.029000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>21</td>\n",
              "      <td>1.013200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>22</td>\n",
              "      <td>0.964500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>23</td>\n",
              "      <td>0.831400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>24</td>\n",
              "      <td>0.909400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>25</td>\n",
              "      <td>1.107700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>26</td>\n",
              "      <td>0.865400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>27</td>\n",
              "      <td>0.913100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28</td>\n",
              "      <td>0.826200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>29</td>\n",
              "      <td>0.857500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.799600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>31</td>\n",
              "      <td>0.766400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>32</td>\n",
              "      <td>0.990200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>33</td>\n",
              "      <td>0.697500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>34</td>\n",
              "      <td>0.862100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>35</td>\n",
              "      <td>0.907100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>36</td>\n",
              "      <td>0.765700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>37</td>\n",
              "      <td>0.608200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>38</td>\n",
              "      <td>1.042400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>39</td>\n",
              "      <td>0.641100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.745700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>41</td>\n",
              "      <td>0.871100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>42</td>\n",
              "      <td>0.491600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>43</td>\n",
              "      <td>1.033100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>44</td>\n",
              "      <td>0.712700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>45</td>\n",
              "      <td>0.890100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>46</td>\n",
              "      <td>0.756400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>47</td>\n",
              "      <td>0.826000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>48</td>\n",
              "      <td>1.029100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>49</td>\n",
              "      <td>0.721200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.650000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>51</td>\n",
              "      <td>0.774800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>52</td>\n",
              "      <td>0.701900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>53</td>\n",
              "      <td>0.627300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>54</td>\n",
              "      <td>0.630900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>55</td>\n",
              "      <td>0.705100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>56</td>\n",
              "      <td>0.735500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>57</td>\n",
              "      <td>0.519400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>58</td>\n",
              "      <td>0.831400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>59</td>\n",
              "      <td>0.745500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.898400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>61</td>\n",
              "      <td>0.786900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>62</td>\n",
              "      <td>0.895200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>63</td>\n",
              "      <td>0.810700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>64</td>\n",
              "      <td>0.732900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>65</td>\n",
              "      <td>0.743200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>66</td>\n",
              "      <td>0.709600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>67</td>\n",
              "      <td>0.830500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>68</td>\n",
              "      <td>0.641400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>69</td>\n",
              "      <td>1.006800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>0.806000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>71</td>\n",
              "      <td>0.838200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>72</td>\n",
              "      <td>0.762700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>73</td>\n",
              "      <td>0.775800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>74</td>\n",
              "      <td>0.782800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>75</td>\n",
              "      <td>0.581800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>76</td>\n",
              "      <td>0.695600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>77</td>\n",
              "      <td>0.626500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>78</td>\n",
              "      <td>0.567400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>79</td>\n",
              "      <td>0.717400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>0.734600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>81</td>\n",
              "      <td>0.924700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>82</td>\n",
              "      <td>0.669300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>83</td>\n",
              "      <td>0.706500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>84</td>\n",
              "      <td>0.706100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>85</td>\n",
              "      <td>0.738400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>86</td>\n",
              "      <td>0.731100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>87</td>\n",
              "      <td>0.462600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>88</td>\n",
              "      <td>0.480700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>89</td>\n",
              "      <td>0.464400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>0.649600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>91</td>\n",
              "      <td>0.951400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>92</td>\n",
              "      <td>1.001500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>93</td>\n",
              "      <td>0.804700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>94</td>\n",
              "      <td>0.610600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>95</td>\n",
              "      <td>0.530100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>96</td>\n",
              "      <td>0.712400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>97</td>\n",
              "      <td>0.668300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>98</td>\n",
              "      <td>0.825500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>99</td>\n",
              "      <td>0.460600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.690000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>101</td>\n",
              "      <td>0.769100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>102</td>\n",
              "      <td>0.525600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>103</td>\n",
              "      <td>0.663600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>104</td>\n",
              "      <td>0.960700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>105</td>\n",
              "      <td>0.417700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>106</td>\n",
              "      <td>0.777400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>107</td>\n",
              "      <td>0.712100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>108</td>\n",
              "      <td>0.441100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>109</td>\n",
              "      <td>0.591900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>0.513600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>111</td>\n",
              "      <td>0.661400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>112</td>\n",
              "      <td>0.566600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>113</td>\n",
              "      <td>0.662600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>114</td>\n",
              "      <td>0.487800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>115</td>\n",
              "      <td>0.666800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>116</td>\n",
              "      <td>0.827500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>117</td>\n",
              "      <td>0.497600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>118</td>\n",
              "      <td>0.302300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>119</td>\n",
              "      <td>0.695700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>0.821000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>121</td>\n",
              "      <td>0.636700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>122</td>\n",
              "      <td>0.422100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>123</td>\n",
              "      <td>0.495100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>124</td>\n",
              "      <td>0.668900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>125</td>\n",
              "      <td>0.794600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>126</td>\n",
              "      <td>0.608700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>127</td>\n",
              "      <td>0.728500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>128</td>\n",
              "      <td>0.618500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>129</td>\n",
              "      <td>0.657500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>0.589400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>131</td>\n",
              "      <td>0.602100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>132</td>\n",
              "      <td>0.264600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>133</td>\n",
              "      <td>0.678500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>134</td>\n",
              "      <td>0.652100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>135</td>\n",
              "      <td>0.770500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>136</td>\n",
              "      <td>0.783000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>137</td>\n",
              "      <td>0.668800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>138</td>\n",
              "      <td>0.407200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>139</td>\n",
              "      <td>0.572500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>0.494800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>141</td>\n",
              "      <td>0.585200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>142</td>\n",
              "      <td>0.569900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>143</td>\n",
              "      <td>0.621700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>144</td>\n",
              "      <td>0.623300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>145</td>\n",
              "      <td>0.341800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>146</td>\n",
              "      <td>0.419900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>147</td>\n",
              "      <td>0.675000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>148</td>\n",
              "      <td>0.385400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>149</td>\n",
              "      <td>0.706600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.307000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>151</td>\n",
              "      <td>0.788300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>152</td>\n",
              "      <td>0.443900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>153</td>\n",
              "      <td>0.683400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>154</td>\n",
              "      <td>0.346600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>155</td>\n",
              "      <td>0.268800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>156</td>\n",
              "      <td>0.805000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>157</td>\n",
              "      <td>0.623100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>158</td>\n",
              "      <td>0.396200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>159</td>\n",
              "      <td>0.380800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>0.610100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>161</td>\n",
              "      <td>0.462500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>162</td>\n",
              "      <td>0.576300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>163</td>\n",
              "      <td>0.500000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>164</td>\n",
              "      <td>0.503300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>165</td>\n",
              "      <td>0.741100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>166</td>\n",
              "      <td>0.641600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>167</td>\n",
              "      <td>0.544600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>168</td>\n",
              "      <td>0.501300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>169</td>\n",
              "      <td>0.885100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>170</td>\n",
              "      <td>0.368000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>171</td>\n",
              "      <td>0.458200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>172</td>\n",
              "      <td>0.549700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>173</td>\n",
              "      <td>0.523300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>174</td>\n",
              "      <td>0.665100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>175</td>\n",
              "      <td>0.527700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>176</td>\n",
              "      <td>0.364100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>177</td>\n",
              "      <td>0.503200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>178</td>\n",
              "      <td>0.447800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>179</td>\n",
              "      <td>0.336800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>180</td>\n",
              "      <td>0.603900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>181</td>\n",
              "      <td>0.626400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>182</td>\n",
              "      <td>0.409800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>183</td>\n",
              "      <td>0.582500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>184</td>\n",
              "      <td>0.517000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>185</td>\n",
              "      <td>0.851100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>186</td>\n",
              "      <td>0.404000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>187</td>\n",
              "      <td>0.325000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>188</td>\n",
              "      <td>0.695700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>189</td>\n",
              "      <td>0.490300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>190</td>\n",
              "      <td>0.487200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>191</td>\n",
              "      <td>0.606700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>192</td>\n",
              "      <td>0.672500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>193</td>\n",
              "      <td>0.414800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>194</td>\n",
              "      <td>0.520200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>195</td>\n",
              "      <td>0.267500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>196</td>\n",
              "      <td>0.448300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>197</td>\n",
              "      <td>0.790200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>198</td>\n",
              "      <td>0.641500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>199</td>\n",
              "      <td>0.436700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.836900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>201</td>\n",
              "      <td>0.655200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>202</td>\n",
              "      <td>0.556400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>203</td>\n",
              "      <td>0.750900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>204</td>\n",
              "      <td>0.897300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>205</td>\n",
              "      <td>0.440300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>206</td>\n",
              "      <td>0.543700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>207</td>\n",
              "      <td>0.430600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>208</td>\n",
              "      <td>0.479200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>209</td>\n",
              "      <td>0.638500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>210</td>\n",
              "      <td>0.507600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>211</td>\n",
              "      <td>0.628800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>212</td>\n",
              "      <td>0.479800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>213</td>\n",
              "      <td>0.549000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>214</td>\n",
              "      <td>0.368200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>215</td>\n",
              "      <td>0.456900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>216</td>\n",
              "      <td>0.483800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>217</td>\n",
              "      <td>0.446600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>218</td>\n",
              "      <td>1.090200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>219</td>\n",
              "      <td>0.355200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>220</td>\n",
              "      <td>0.593600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>221</td>\n",
              "      <td>0.259700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>222</td>\n",
              "      <td>0.851600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>223</td>\n",
              "      <td>0.514700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>224</td>\n",
              "      <td>0.654700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>225</td>\n",
              "      <td>0.630000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>226</td>\n",
              "      <td>0.692200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>227</td>\n",
              "      <td>0.508700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>228</td>\n",
              "      <td>0.431300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>229</td>\n",
              "      <td>0.383900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>230</td>\n",
              "      <td>0.721300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>231</td>\n",
              "      <td>0.394700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>232</td>\n",
              "      <td>0.561100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>233</td>\n",
              "      <td>0.870400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>234</td>\n",
              "      <td>0.459700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>235</td>\n",
              "      <td>0.552000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>236</td>\n",
              "      <td>0.532600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>237</td>\n",
              "      <td>0.518900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>238</td>\n",
              "      <td>0.716700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>239</td>\n",
              "      <td>0.361000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>240</td>\n",
              "      <td>0.712200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>241</td>\n",
              "      <td>0.690800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>242</td>\n",
              "      <td>0.335700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>243</td>\n",
              "      <td>0.439300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>244</td>\n",
              "      <td>0.568700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>245</td>\n",
              "      <td>0.338900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>246</td>\n",
              "      <td>0.682500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>247</td>\n",
              "      <td>0.746900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>248</td>\n",
              "      <td>0.467200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>249</td>\n",
              "      <td>0.704200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>250</td>\n",
              "      <td>0.590000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>251</td>\n",
              "      <td>0.819800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>252</td>\n",
              "      <td>0.558600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>253</td>\n",
              "      <td>0.515400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>254</td>\n",
              "      <td>0.530700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>255</td>\n",
              "      <td>0.591900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>256</td>\n",
              "      <td>0.520500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>257</td>\n",
              "      <td>0.540100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>258</td>\n",
              "      <td>0.645600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>259</td>\n",
              "      <td>0.629900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>260</td>\n",
              "      <td>0.541700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>261</td>\n",
              "      <td>0.608800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>262</td>\n",
              "      <td>0.407700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>263</td>\n",
              "      <td>0.670400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>264</td>\n",
              "      <td>0.333200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>265</td>\n",
              "      <td>0.580800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>266</td>\n",
              "      <td>0.279600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>267</td>\n",
              "      <td>0.429800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>268</td>\n",
              "      <td>0.474000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>269</td>\n",
              "      <td>0.537500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>270</td>\n",
              "      <td>0.688000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>271</td>\n",
              "      <td>0.531700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>272</td>\n",
              "      <td>0.623300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>273</td>\n",
              "      <td>0.579900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>274</td>\n",
              "      <td>0.486400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>275</td>\n",
              "      <td>0.749700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>276</td>\n",
              "      <td>0.455800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>277</td>\n",
              "      <td>0.438500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>278</td>\n",
              "      <td>0.369600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>279</td>\n",
              "      <td>0.344500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>280</td>\n",
              "      <td>0.537000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>281</td>\n",
              "      <td>0.608200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>282</td>\n",
              "      <td>0.392600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>283</td>\n",
              "      <td>0.371000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>284</td>\n",
              "      <td>0.430100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>285</td>\n",
              "      <td>0.595300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>286</td>\n",
              "      <td>0.393100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>287</td>\n",
              "      <td>0.431300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>288</td>\n",
              "      <td>0.510500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>289</td>\n",
              "      <td>0.437900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>290</td>\n",
              "      <td>0.460200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>291</td>\n",
              "      <td>0.505600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>292</td>\n",
              "      <td>0.570900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>293</td>\n",
              "      <td>0.161200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>294</td>\n",
              "      <td>0.202900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>295</td>\n",
              "      <td>0.841500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>296</td>\n",
              "      <td>0.343600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>297</td>\n",
              "      <td>0.444000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>298</td>\n",
              "      <td>0.384200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>299</td>\n",
              "      <td>0.746600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.565300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>301</td>\n",
              "      <td>1.033900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>302</td>\n",
              "      <td>0.274200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>303</td>\n",
              "      <td>0.351700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>304</td>\n",
              "      <td>0.521600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>305</td>\n",
              "      <td>0.611400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>306</td>\n",
              "      <td>0.530500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>307</td>\n",
              "      <td>0.373300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>308</td>\n",
              "      <td>0.612800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>309</td>\n",
              "      <td>0.300700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>310</td>\n",
              "      <td>0.560200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>311</td>\n",
              "      <td>0.314100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>312</td>\n",
              "      <td>0.458600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>313</td>\n",
              "      <td>0.483800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>314</td>\n",
              "      <td>0.323600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>315</td>\n",
              "      <td>0.308800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>316</td>\n",
              "      <td>0.275900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>317</td>\n",
              "      <td>0.368100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>318</td>\n",
              "      <td>0.508600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>319</td>\n",
              "      <td>0.286400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>320</td>\n",
              "      <td>0.577000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>321</td>\n",
              "      <td>0.660400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>322</td>\n",
              "      <td>0.107700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>323</td>\n",
              "      <td>0.557200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>324</td>\n",
              "      <td>0.456600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>325</td>\n",
              "      <td>0.351300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>326</td>\n",
              "      <td>0.774600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>327</td>\n",
              "      <td>0.136900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>328</td>\n",
              "      <td>0.647000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>329</td>\n",
              "      <td>0.715500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>330</td>\n",
              "      <td>0.589400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>331</td>\n",
              "      <td>0.454000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>332</td>\n",
              "      <td>0.514300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>333</td>\n",
              "      <td>0.445400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>334</td>\n",
              "      <td>0.354900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>335</td>\n",
              "      <td>0.415700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>336</td>\n",
              "      <td>0.338300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>337</td>\n",
              "      <td>0.361400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>338</td>\n",
              "      <td>0.528400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>339</td>\n",
              "      <td>0.458400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>340</td>\n",
              "      <td>0.620500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>341</td>\n",
              "      <td>0.476200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>342</td>\n",
              "      <td>0.347400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>343</td>\n",
              "      <td>0.516400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>344</td>\n",
              "      <td>0.494500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>345</td>\n",
              "      <td>0.331400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>346</td>\n",
              "      <td>0.533700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>347</td>\n",
              "      <td>0.452400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>348</td>\n",
              "      <td>0.329600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>349</td>\n",
              "      <td>0.248000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>350</td>\n",
              "      <td>0.418400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>351</td>\n",
              "      <td>0.483100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>352</td>\n",
              "      <td>0.587200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>353</td>\n",
              "      <td>0.368900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>354</td>\n",
              "      <td>0.407400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>355</td>\n",
              "      <td>0.453900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>356</td>\n",
              "      <td>0.429700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>357</td>\n",
              "      <td>0.543700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>358</td>\n",
              "      <td>0.369000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>359</td>\n",
              "      <td>0.316600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>360</td>\n",
              "      <td>0.490600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>361</td>\n",
              "      <td>0.448100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>362</td>\n",
              "      <td>0.666000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>363</td>\n",
              "      <td>0.474600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>364</td>\n",
              "      <td>0.258000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>365</td>\n",
              "      <td>0.407300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>366</td>\n",
              "      <td>0.419500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>367</td>\n",
              "      <td>0.472600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>368</td>\n",
              "      <td>0.638600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>369</td>\n",
              "      <td>0.616900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>370</td>\n",
              "      <td>0.205500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>371</td>\n",
              "      <td>0.479500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>372</td>\n",
              "      <td>0.275600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>373</td>\n",
              "      <td>0.570100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>374</td>\n",
              "      <td>0.395500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>375</td>\n",
              "      <td>0.604900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>376</td>\n",
              "      <td>0.570900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>377</td>\n",
              "      <td>0.436000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>378</td>\n",
              "      <td>0.417700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>379</td>\n",
              "      <td>0.692300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>380</td>\n",
              "      <td>0.263300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>381</td>\n",
              "      <td>0.327200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>382</td>\n",
              "      <td>0.404500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>383</td>\n",
              "      <td>0.401200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>384</td>\n",
              "      <td>0.432900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>385</td>\n",
              "      <td>0.621100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>386</td>\n",
              "      <td>0.391800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>387</td>\n",
              "      <td>0.444200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>388</td>\n",
              "      <td>0.316200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>389</td>\n",
              "      <td>0.442900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>390</td>\n",
              "      <td>0.380400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>391</td>\n",
              "      <td>0.478300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>392</td>\n",
              "      <td>0.339000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>393</td>\n",
              "      <td>0.664500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>394</td>\n",
              "      <td>0.393500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>395</td>\n",
              "      <td>0.666900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>396</td>\n",
              "      <td>0.239700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>397</td>\n",
              "      <td>0.281600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>398</td>\n",
              "      <td>0.291600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>399</td>\n",
              "      <td>0.229900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.371300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>401</td>\n",
              "      <td>0.285200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>402</td>\n",
              "      <td>0.457700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>403</td>\n",
              "      <td>0.373500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>404</td>\n",
              "      <td>0.451500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>405</td>\n",
              "      <td>0.417800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>406</td>\n",
              "      <td>0.414600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>407</td>\n",
              "      <td>0.235100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>408</td>\n",
              "      <td>0.337200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>409</td>\n",
              "      <td>0.267900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>410</td>\n",
              "      <td>0.411400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>411</td>\n",
              "      <td>0.532400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>412</td>\n",
              "      <td>0.280300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>413</td>\n",
              "      <td>0.398700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>414</td>\n",
              "      <td>0.288900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>415</td>\n",
              "      <td>0.661500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>416</td>\n",
              "      <td>0.192700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>417</td>\n",
              "      <td>0.505100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>418</td>\n",
              "      <td>0.282400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>419</td>\n",
              "      <td>0.188000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>420</td>\n",
              "      <td>0.331900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>421</td>\n",
              "      <td>0.413700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>422</td>\n",
              "      <td>0.401600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>423</td>\n",
              "      <td>0.275100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>424</td>\n",
              "      <td>0.531000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>425</td>\n",
              "      <td>0.379100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>426</td>\n",
              "      <td>0.395100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>427</td>\n",
              "      <td>0.421300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>428</td>\n",
              "      <td>0.496400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>429</td>\n",
              "      <td>0.323000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>430</td>\n",
              "      <td>0.252100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>431</td>\n",
              "      <td>0.174400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>432</td>\n",
              "      <td>0.390900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>433</td>\n",
              "      <td>0.195500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>434</td>\n",
              "      <td>0.321200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>435</td>\n",
              "      <td>0.213300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>436</td>\n",
              "      <td>0.419400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>437</td>\n",
              "      <td>0.548600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>438</td>\n",
              "      <td>0.272000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>439</td>\n",
              "      <td>0.442400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>440</td>\n",
              "      <td>0.329100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>441</td>\n",
              "      <td>0.252800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>442</td>\n",
              "      <td>0.229900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>443</td>\n",
              "      <td>0.251100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>444</td>\n",
              "      <td>0.627900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>445</td>\n",
              "      <td>0.495900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>446</td>\n",
              "      <td>0.347200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>447</td>\n",
              "      <td>0.597500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>448</td>\n",
              "      <td>0.390900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>449</td>\n",
              "      <td>0.464000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>450</td>\n",
              "      <td>0.312700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>451</td>\n",
              "      <td>0.636900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>452</td>\n",
              "      <td>0.441600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>453</td>\n",
              "      <td>0.272400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>454</td>\n",
              "      <td>0.408900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>455</td>\n",
              "      <td>0.102600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>456</td>\n",
              "      <td>0.339400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>457</td>\n",
              "      <td>0.438300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>458</td>\n",
              "      <td>0.387800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>459</td>\n",
              "      <td>0.338200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>460</td>\n",
              "      <td>0.328800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>461</td>\n",
              "      <td>0.271800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>462</td>\n",
              "      <td>0.223100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>463</td>\n",
              "      <td>0.356400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>464</td>\n",
              "      <td>0.552600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>465</td>\n",
              "      <td>0.493700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>466</td>\n",
              "      <td>0.364600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>467</td>\n",
              "      <td>0.466400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>468</td>\n",
              "      <td>0.321100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>469</td>\n",
              "      <td>0.418300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>470</td>\n",
              "      <td>0.555200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>471</td>\n",
              "      <td>0.637700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>472</td>\n",
              "      <td>0.301600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>473</td>\n",
              "      <td>0.420900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>474</td>\n",
              "      <td>0.344900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>475</td>\n",
              "      <td>0.243100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>476</td>\n",
              "      <td>0.358700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>477</td>\n",
              "      <td>0.256700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>478</td>\n",
              "      <td>0.320400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>479</td>\n",
              "      <td>0.288200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>480</td>\n",
              "      <td>0.475600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>481</td>\n",
              "      <td>0.286100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>482</td>\n",
              "      <td>0.235900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>483</td>\n",
              "      <td>0.522900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>484</td>\n",
              "      <td>0.290900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>485</td>\n",
              "      <td>0.310100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>486</td>\n",
              "      <td>0.400800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>487</td>\n",
              "      <td>0.354300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>488</td>\n",
              "      <td>0.337900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>489</td>\n",
              "      <td>0.273500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>490</td>\n",
              "      <td>0.452500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>491</td>\n",
              "      <td>0.297100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>492</td>\n",
              "      <td>0.571300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>493</td>\n",
              "      <td>0.465400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>494</td>\n",
              "      <td>0.362800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>495</td>\n",
              "      <td>0.853700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>496</td>\n",
              "      <td>0.343700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>497</td>\n",
              "      <td>0.385500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>498</td>\n",
              "      <td>0.318800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>499</td>\n",
              "      <td>0.313300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.464500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>501</td>\n",
              "      <td>0.301500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>502</td>\n",
              "      <td>0.579400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>503</td>\n",
              "      <td>0.190300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>504</td>\n",
              "      <td>0.528400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>505</td>\n",
              "      <td>0.318200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>506</td>\n",
              "      <td>0.368700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>507</td>\n",
              "      <td>0.342400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>508</td>\n",
              "      <td>0.481900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>509</td>\n",
              "      <td>0.370800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>510</td>\n",
              "      <td>0.490800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>511</td>\n",
              "      <td>0.285400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>512</td>\n",
              "      <td>0.349400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>513</td>\n",
              "      <td>0.224500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>514</td>\n",
              "      <td>0.381200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>515</td>\n",
              "      <td>0.259500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>516</td>\n",
              "      <td>0.289700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>517</td>\n",
              "      <td>0.264700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>518</td>\n",
              "      <td>0.275400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>519</td>\n",
              "      <td>0.358100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>520</td>\n",
              "      <td>0.531100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>521</td>\n",
              "      <td>0.475200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>522</td>\n",
              "      <td>0.558800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>523</td>\n",
              "      <td>0.321500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>524</td>\n",
              "      <td>0.351100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>525</td>\n",
              "      <td>0.467900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>526</td>\n",
              "      <td>0.340700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>527</td>\n",
              "      <td>0.113900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>528</td>\n",
              "      <td>0.229700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>529</td>\n",
              "      <td>0.527200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>530</td>\n",
              "      <td>0.513300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>531</td>\n",
              "      <td>0.441200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>532</td>\n",
              "      <td>0.367200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>533</td>\n",
              "      <td>0.422400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>534</td>\n",
              "      <td>0.404100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>535</td>\n",
              "      <td>0.299200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>536</td>\n",
              "      <td>0.209800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>537</td>\n",
              "      <td>0.513500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>538</td>\n",
              "      <td>0.416200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>539</td>\n",
              "      <td>0.321300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>540</td>\n",
              "      <td>0.302200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>541</td>\n",
              "      <td>0.319800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>542</td>\n",
              "      <td>0.237800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>543</td>\n",
              "      <td>0.273500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>544</td>\n",
              "      <td>0.299200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>545</td>\n",
              "      <td>0.392200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>546</td>\n",
              "      <td>0.330600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>547</td>\n",
              "      <td>0.351900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>548</td>\n",
              "      <td>0.256700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>549</td>\n",
              "      <td>0.253400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>550</td>\n",
              "      <td>0.649500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>551</td>\n",
              "      <td>0.232600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>552</td>\n",
              "      <td>0.167700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>553</td>\n",
              "      <td>0.470400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>554</td>\n",
              "      <td>0.304400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>555</td>\n",
              "      <td>0.154400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>556</td>\n",
              "      <td>0.144300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>557</td>\n",
              "      <td>0.303000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>558</td>\n",
              "      <td>0.334700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>559</td>\n",
              "      <td>0.380400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>560</td>\n",
              "      <td>0.230600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>561</td>\n",
              "      <td>0.504800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>562</td>\n",
              "      <td>0.366200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>563</td>\n",
              "      <td>0.325200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>564</td>\n",
              "      <td>0.577800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>565</td>\n",
              "      <td>0.282400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>566</td>\n",
              "      <td>0.305900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>567</td>\n",
              "      <td>0.304100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>568</td>\n",
              "      <td>0.388800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>569</td>\n",
              "      <td>0.208400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>570</td>\n",
              "      <td>0.254600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>571</td>\n",
              "      <td>0.440300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>572</td>\n",
              "      <td>0.237500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>573</td>\n",
              "      <td>0.194200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>574</td>\n",
              "      <td>0.311200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>575</td>\n",
              "      <td>0.241900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>576</td>\n",
              "      <td>0.251200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>577</td>\n",
              "      <td>0.472500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>578</td>\n",
              "      <td>0.119800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>579</td>\n",
              "      <td>0.184800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>580</td>\n",
              "      <td>0.341600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>581</td>\n",
              "      <td>0.291600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>582</td>\n",
              "      <td>0.117700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>583</td>\n",
              "      <td>0.318000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>584</td>\n",
              "      <td>0.289500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>585</td>\n",
              "      <td>0.380100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>586</td>\n",
              "      <td>0.245800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>587</td>\n",
              "      <td>0.534100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>588</td>\n",
              "      <td>0.296900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>589</td>\n",
              "      <td>0.401000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>590</td>\n",
              "      <td>0.397000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>591</td>\n",
              "      <td>0.209200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>592</td>\n",
              "      <td>0.375800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>593</td>\n",
              "      <td>0.289700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>594</td>\n",
              "      <td>0.200200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>595</td>\n",
              "      <td>0.325700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>596</td>\n",
              "      <td>0.312400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>597</td>\n",
              "      <td>0.271400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>598</td>\n",
              "      <td>0.259300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>599</td>\n",
              "      <td>0.321000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.165700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>601</td>\n",
              "      <td>0.319300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>602</td>\n",
              "      <td>0.252700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>603</td>\n",
              "      <td>0.222100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>604</td>\n",
              "      <td>0.313100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>605</td>\n",
              "      <td>0.311900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>606</td>\n",
              "      <td>0.574100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>607</td>\n",
              "      <td>0.336300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>608</td>\n",
              "      <td>0.431800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>609</td>\n",
              "      <td>0.528800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>610</td>\n",
              "      <td>0.478300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>611</td>\n",
              "      <td>0.442000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>612</td>\n",
              "      <td>0.324700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>613</td>\n",
              "      <td>0.330200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>614</td>\n",
              "      <td>0.301400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>615</td>\n",
              "      <td>0.279000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>616</td>\n",
              "      <td>0.323600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>617</td>\n",
              "      <td>0.344300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>618</td>\n",
              "      <td>0.571900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>619</td>\n",
              "      <td>0.453500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>620</td>\n",
              "      <td>0.130400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>621</td>\n",
              "      <td>0.472000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>622</td>\n",
              "      <td>0.824300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>623</td>\n",
              "      <td>0.222200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>624</td>\n",
              "      <td>0.269500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>625</td>\n",
              "      <td>0.411800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>626</td>\n",
              "      <td>0.469900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>627</td>\n",
              "      <td>0.363000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>628</td>\n",
              "      <td>0.479400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>629</td>\n",
              "      <td>0.224300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>630</td>\n",
              "      <td>0.323500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>631</td>\n",
              "      <td>0.313500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>632</td>\n",
              "      <td>0.247400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>633</td>\n",
              "      <td>0.491400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>634</td>\n",
              "      <td>0.597100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>635</td>\n",
              "      <td>0.236500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>636</td>\n",
              "      <td>0.452400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>637</td>\n",
              "      <td>0.260100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>638</td>\n",
              "      <td>0.271500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>639</td>\n",
              "      <td>0.253800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>640</td>\n",
              "      <td>0.403000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>641</td>\n",
              "      <td>0.345100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>642</td>\n",
              "      <td>0.284600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>643</td>\n",
              "      <td>0.126400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>644</td>\n",
              "      <td>0.262700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>645</td>\n",
              "      <td>0.158800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>646</td>\n",
              "      <td>0.254600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>647</td>\n",
              "      <td>0.360600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>648</td>\n",
              "      <td>0.230400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>649</td>\n",
              "      <td>0.416200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>650</td>\n",
              "      <td>0.189000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>651</td>\n",
              "      <td>0.472600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>652</td>\n",
              "      <td>0.260800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>653</td>\n",
              "      <td>0.292300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>654</td>\n",
              "      <td>0.344800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>655</td>\n",
              "      <td>0.438300</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=655, training_loss=0.5005192502309348, metrics={'train_runtime': 882.1958, 'train_samples_per_second': 23.827, 'train_steps_per_second': 0.742, 'total_flos': 1459402138460160.0, 'train_loss': 0.5005192502309348, 'epoch': 4.966698382492864})"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer_5.evaluate()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "outputId": "e224bf36-4527-4d08-8842-6314cc67bc86",
        "id": "1b3ieoQ9cjd-"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n",
            "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='59' max='59' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [59/59 00:02]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'eval_loss': 0.6837388277053833,\n",
              " 'eval_accuracy': 0.7606837606837606,\n",
              " 'eval_precision': 0.7555191514725055,\n",
              " 'eval_recall': 0.7606837606837606,\n",
              " 'eval_f1': 0.7578218750803828,\n",
              " 'eval_runtime': 2.5757,\n",
              " 'eval_samples_per_second': 181.698,\n",
              " 'eval_steps_per_second': 22.906,\n",
              " 'epoch': 4.966698382492864}"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implement `Temperature Scaling` Calibration Technique to calibrate `eval_loss` into model_5"
      ],
      "metadata": {
        "id": "E3PVaCRAm136"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "from scipy.special import softmax\n",
        "\n",
        "class ModelWithTemperature(nn.Module):\n",
        "    def __init__(self, model):\n",
        "        super(ModelWithTemperature, self).__init__()\n",
        "        self.model = model\n",
        "        self.temperature = nn.Parameter(torch.ones(1) * 1.5)  # Initialize T=1.5\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        logits = self.model(input_ids=input_ids, attention_mask=attention_mask).logits\n",
        "        return logits / self.temperature\n",
        "\n",
        "def calibrate_temperature(model, valid_loader, device='cuda'):\n",
        "    \"\"\"\n",
        "    Learn the optimal temperature on the validation set\n",
        "    \"\"\"\n",
        "    temperature_model = ModelWithTemperature(model)\n",
        "    temperature_model = temperature_model.to(device)\n",
        "\n",
        "    nll_criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.LBFGS([temperature_model.temperature], lr=0.01, max_iter=50)\n",
        "\n",
        "    def eval_step():\n",
        "        total_loss = torch.tensor(0., device=device)  # Create tensor for loss\n",
        "        num_samples = 0\n",
        "\n",
        "        for batch in valid_loader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            scaled_logits = temperature_model(input_ids, attention_mask)\n",
        "            loss = nll_criterion(scaled_logits, labels)\n",
        "            total_loss += loss * input_ids.size(0)\n",
        "            num_samples += input_ids.size(0)\n",
        "\n",
        "        loss = total_loss / num_samples\n",
        "        loss.backward()\n",
        "        return loss\n",
        "\n",
        "    optimizer.step(eval_step)\n",
        "    return temperature_model\n",
        "\n",
        "# Create validation dataloader\n",
        "valid_dataloader = DataLoader(\n",
        "    tokenized_datasets[\"validation\"],\n",
        "    batch_size=8,\n",
        "    collate_fn=data_collator\n",
        ")\n",
        "\n",
        "# Calibrate the model\n",
        "print(\"Starting temperature calibration...\")\n",
        "calibrated_model = calibrate_temperature(model_5, valid_dataloader)\n",
        "optimal_temperature = calibrated_model.temperature.item()\n",
        "print(f\"Optimal temperature: {optimal_temperature:.3f}\")\n",
        "\n",
        "# Function to get calibrated predictions\n",
        "def get_calibrated_predictions(model, dataloader, temperature, device='cuda'):\n",
        "    model.eval()\n",
        "    all_probs = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            logits = outputs.logits / temperature\n",
        "            probs = torch.softmax(logits, dim=-1)\n",
        "\n",
        "            all_probs.extend(probs.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    return np.array(all_probs), np.array(all_labels)\n",
        "\n",
        "# Get calibrated predictions\n",
        "calibrated_probs, true_labels = get_calibrated_predictions(\n",
        "    model_5,\n",
        "    valid_dataloader,\n",
        "    optimal_temperature\n",
        ")\n",
        "\n",
        "# Function to compute ECE\n",
        "def compute_ece(probs, labels, n_bins=15):\n",
        "    confidences = np.max(probs, axis=1)\n",
        "    predictions = np.argmax(probs, axis=1)\n",
        "    accuracies = predictions == labels\n",
        "\n",
        "    ece = 0\n",
        "    total_samples = len(labels)\n",
        "\n",
        "    for bin_lower in np.linspace(0, 1, n_bins+1)[:-1]:\n",
        "        bin_upper = bin_lower + 1/n_bins\n",
        "        bin_mask = (confidences >= bin_lower) & (confidences < bin_upper)\n",
        "        if np.any(bin_mask):\n",
        "            bin_accuracy = np.mean(accuracies[bin_mask])\n",
        "            bin_confidence = np.mean(confidences[bin_mask])\n",
        "            bin_samples = np.sum(bin_mask)\n",
        "            ece += (bin_samples / total_samples) * np.abs(bin_accuracy - bin_confidence)\n",
        "\n",
        "    return ece\n",
        "\n",
        "# Get uncalibrated predictions first\n",
        "def get_uncalibrated_predictions(model, dataloader, device='cuda'):\n",
        "    model.eval()\n",
        "    all_probs = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in dataloader:\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['labels'].to(device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            probs = torch.softmax(outputs.logits, dim=-1)\n",
        "\n",
        "            all_probs.extend(probs.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    return np.array(all_probs), np.array(all_labels)\n",
        "\n",
        "# Compute ECE before and after calibration\n",
        "uncalibrated_probs, _ = get_uncalibrated_predictions(model_5, valid_dataloader)\n",
        "ece_before = compute_ece(uncalibrated_probs, true_labels)\n",
        "ece_after = compute_ece(calibrated_probs, true_labels)\n",
        "\n",
        "print(f\"ECE Before Calibration: {ece_before:.3f}\")\n",
        "print(f\"ECE After Calibration: {ece_after:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "id": "IvNQ26vyu-FB",
        "outputId": "ba570ec4-4155-40ad-ad13-837ffc54a309"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting temperature calibration...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 16.12 MiB is free. Process 22430 has 14.72 GiB memory in use. Of the allocated memory 14.59 GiB is allocated by PyTorch, and 3.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-cc328a31f1e3>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;31m# Calibrate the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Starting temperature calibration...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m \u001b[0mcalibrated_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalibrate_temperature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0moptimal_temperature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalibrated_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtemperature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Optimal temperature: {optimal_temperature:.3f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-29-cc328a31f1e3>\u001b[0m in \u001b[0;36mcalibrate_temperature\u001b[0;34m(model, valid_loader, device)\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtemperature_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                             )\n\u001b[1;32m    486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    488\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/lbfgs.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m         \u001b[0;31m# evaluate initial f(x) and df/dx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 330\u001b[0;31m         \u001b[0morig_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    331\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m         \u001b[0mcurrent_evals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-29-cc328a31f1e3>\u001b[0m in \u001b[0;36meval_step\u001b[0;34m()\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'labels'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m             \u001b[0mscaled_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtemperature_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnll_criterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscaled_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-29-cc328a31f1e3>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlogits\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtemperature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/utils/operations.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m     \u001b[0;31m# To act like a decorator so that it can be popped when doing `extract_model_from_parallel`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/utils/operations.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    805\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mconvert_to_fp32\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/amp/autocast_mode.py\u001b[0m in \u001b[0;36mdecorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_autocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mautocast_instance\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mdecorate_autocast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__script_unsupported\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"@autocast() decorator is not supported in script mode\"\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/peft/peft_model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mpeft_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpeft_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mPeftType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPOLY\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"task_ids\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtask_ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1521\u001b[0;31m                 return self.base_model(\n\u001b[0m\u001b[1;32m   1522\u001b[0m                     \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1523\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/peft/tuners/tuners_utils.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_pre_injection_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mPeftConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madapter_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1671\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1672\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1673\u001b[0;31m         outputs = self.bert(\n\u001b[0m\u001b[1;32m   1674\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1675\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1140\u001b[0m         \u001b[0mhead_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_head_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_hidden_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1142\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m   1143\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1144\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    693\u001b[0m                 )\n\u001b[1;32m    694\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 695\u001b[0;31m                 layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m    696\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    697\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    625\u001b[0m             \u001b[0mpresent_key_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpresent_key_value\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcross_attn_present_key_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 627\u001b[0;31m         layer_output = apply_chunking_to_forward(\n\u001b[0m\u001b[1;32m    628\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_forward_chunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk_size_feed_forward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq_len_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/pytorch_utils.py\u001b[0m in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m    259\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_chunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunk_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mforward_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mfeed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    638\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfeed_forward_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m         \u001b[0mintermediate_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 640\u001b[0;31m         \u001b[0mlayer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintermediate_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    641\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlayer_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 552\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    553\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayerNorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/peft/tuners/lora/layer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m    619\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    620\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_dora\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mactive_adapter\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 621\u001b[0;31m                     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlora_B\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlora_A\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mscaling\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    622\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    623\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIdentity\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 20.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 16.12 MiB is free. Process 22430 has 14.72 GiB memory in use. Of the allocated memory 14.59 GiB is allocated by PyTorch, and 3.38 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "from scipy.special import softmax\n",
        "import gc\n",
        "\n",
        "class ModelWithTemperature(nn.Module):\n",
        "    def __init__(self, model):\n",
        "        super(ModelWithTemperature, self).__init__()\n",
        "        self.model = model\n",
        "        self.temperature = nn.Parameter(torch.ones(1) * 1.5)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        logits = self.model(input_ids=input_ids, attention_mask=attention_mask).logits\n",
        "        return logits / self.temperature\n",
        "\n",
        "def calibrate_temperature(model, valid_loader, device='cuda'):\n",
        "    \"\"\"\n",
        "    Memory-efficient temperature calibration\n",
        "    \"\"\"\n",
        "    # Clear GPU memory before starting\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    temperature_model = ModelWithTemperature(model)\n",
        "    temperature_model = temperature_model.to(device)\n",
        "    temperature_model.eval()  # Ensure model is in eval mode\n",
        "\n",
        "    nll_criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.LBFGS([temperature_model.temperature], lr=0.01, max_iter=50)\n",
        "\n",
        "    def eval_step():\n",
        "        optimizer.zero_grad()\n",
        "        total_loss = torch.tensor(0., device=device)\n",
        "        num_samples = 0\n",
        "\n",
        "        # Process in smaller chunks\n",
        "        for batch in valid_loader:\n",
        "            # Move batch to CPU initially\n",
        "            input_ids = batch['input_ids']\n",
        "            attention_mask = batch['attention_mask']\n",
        "            labels = batch['labels']\n",
        "\n",
        "            # Process in even smaller sub-batches if needed\n",
        "            sub_batch_size = 2  # Very small sub-batch size\n",
        "            for i in range(0, input_ids.size(0), sub_batch_size):\n",
        "                end_idx = min(i + sub_batch_size, input_ids.size(0))\n",
        "\n",
        "                sub_input_ids = input_ids[i:end_idx].to(device)\n",
        "                sub_attention_mask = attention_mask[i:end_idx].to(device)\n",
        "                sub_labels = labels[i:end_idx].to(device)\n",
        "\n",
        "                with torch.set_grad_enabled(True):\n",
        "                    scaled_logits = temperature_model(sub_input_ids, sub_attention_mask)\n",
        "                    loss = nll_criterion(scaled_logits, sub_labels)\n",
        "                    total_loss += loss * sub_input_ids.size(0)\n",
        "                    num_samples += sub_input_ids.size(0)\n",
        "\n",
        "                # Clear GPU memory after each sub-batch\n",
        "                del sub_input_ids, sub_attention_mask, sub_labels, scaled_logits\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "        avg_loss = total_loss / num_samples\n",
        "        avg_loss.backward()\n",
        "        return avg_loss\n",
        "\n",
        "    optimizer.step(eval_step)\n",
        "\n",
        "    return temperature_model\n",
        "\n",
        "# Create a smaller validation dataloader\n",
        "valid_dataloader = DataLoader(\n",
        "    tokenized_datasets[\"validation\"],\n",
        "    batch_size=4,  # Reduced batch size\n",
        "    collate_fn=data_collator,\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "# Clear GPU memory before starting\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "print(\"Starting temperature calibration...\")\n",
        "try:\n",
        "    calibrated_model = calibrate_temperature(model_5, valid_dataloader)\n",
        "    optimal_temperature = calibrated_model.temperature.item()\n",
        "    print(f\"Optimal temperature: {optimal_temperature:.3f}\")\n",
        "\n",
        "    # Function to get predictions in a memory-efficient way\n",
        "    def get_predictions(model, dataloader, temperature=None, device='cuda'):\n",
        "        model.eval()\n",
        "        all_probs = []\n",
        "        all_labels = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch in dataloader:\n",
        "                # Process in smaller sub-batches\n",
        "                sub_batch_size = 2\n",
        "                input_ids = batch['input_ids']\n",
        "                attention_mask = batch['attention_mask']\n",
        "                labels = batch['labels']\n",
        "\n",
        "                for i in range(0, input_ids.size(0), sub_batch_size):\n",
        "                    end_idx = min(i + sub_batch_size, input_ids.size(0))\n",
        "\n",
        "                    sub_input_ids = input_ids[i:end_idx].to(device)\n",
        "                    sub_attention_mask = attention_mask[i:end_idx].to(device)\n",
        "                    sub_labels = labels[i:end_idx]\n",
        "\n",
        "                    outputs = model(input_ids=sub_input_ids, attention_mask=sub_attention_mask)\n",
        "                    logits = outputs.logits\n",
        "\n",
        "                    if temperature is not None:\n",
        "                        logits = logits / temperature\n",
        "\n",
        "                    probs = torch.softmax(logits, dim=-1)\n",
        "\n",
        "                    all_probs.extend(probs.cpu().numpy())\n",
        "                    all_labels.extend(sub_labels.numpy())\n",
        "\n",
        "                    # Clear GPU memory\n",
        "                    del sub_input_ids, sub_attention_mask, logits, probs\n",
        "                    torch.cuda.empty_cache()\n",
        "\n",
        "        return np.array(all_probs), np.array(all_labels)\n",
        "\n",
        "    # Get calibrated and uncalibrated predictions\n",
        "    print(\"Computing predictions...\")\n",
        "    uncalibrated_probs, true_labels = get_predictions(model_5, valid_dataloader)\n",
        "    calibrated_probs, _ = get_predictions(model_5, valid_dataloader, optimal_temperature)\n",
        "\n",
        "    # Compute ECE\n",
        "    def compute_ece(probs, labels, n_bins=15):\n",
        "        confidences = np.max(probs, axis=1)\n",
        "        predictions = np.argmax(probs, axis=1)\n",
        "        accuracies = predictions == labels\n",
        "\n",
        "        ece = 0\n",
        "        total_samples = len(labels)\n",
        "\n",
        "        for bin_lower in np.linspace(0, 1, n_bins+1)[:-1]:\n",
        "            bin_upper = bin_lower + 1/n_bins\n",
        "            bin_mask = (confidences >= bin_lower) & (confidences < bin_upper)\n",
        "            if np.any(bin_mask):\n",
        "                bin_accuracy = np.mean(accuracies[bin_mask])\n",
        "                bin_confidence = np.mean(confidences[bin_mask])\n",
        "                bin_samples = np.sum(bin_mask)\n",
        "                ece += (bin_samples / total_samples) * np.abs(bin_accuracy - bin_confidence)\n",
        "\n",
        "        return ece\n",
        "\n",
        "    ece_before = compute_ece(uncalibrated_probs, true_labels)\n",
        "    ece_after = compute_ece(calibrated_probs, true_labels)\n",
        "\n",
        "    print(f\"ECE Before Calibration: {ece_before:.3f}\")\n",
        "    print(f\"ECE After Calibration: {ece_after:.3f}\")\n",
        "\n",
        "except RuntimeError as e:\n",
        "    if \"out of memory\" in str(e):\n",
        "        print(\"Still running into memory issues. Try these steps:\")\n",
        "        print(\"1. Reduce sub_batch_size further\")\n",
        "        print(\"2. Move part of validation set to CPU\")\n",
        "        print(\"3. Use gradient checkpointing\")\n",
        "    raise e"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 495
        },
        "id": "DxjJKoMYveu-",
        "outputId": "3e96704d-217d-422c-fd55-1d36bafa2f26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting temperature calibration...\n",
            "Still running into memory issues. Try these steps:\n",
            "1. Reduce sub_batch_size further\n",
            "2. Move part of validation set to CPU\n",
            "3. Use gradient checkpointing\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 2.12 MiB is free. Process 22430 has 14.74 GiB memory in use. Of the allocated memory 14.58 GiB is allocated by PyTorch, and 19.84 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-4700133ed20a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    164\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"2. Move part of validation set to CPU\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"3. Use gradient checkpointing\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 166\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-31-4700133ed20a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Starting temperature calibration...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m     \u001b[0mcalibrated_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalibrate_temperature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m     \u001b[0moptimal_temperature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalibrated_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtemperature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Optimal temperature: {optimal_temperature:.3f}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-4700133ed20a>\u001b[0m in \u001b[0;36mcalibrate_temperature\u001b[0;34m(model, valid_loader, device)\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mavg_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtemperature_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                             )\n\u001b[1;32m    486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    488\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/lbfgs.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m         \u001b[0;31m# evaluate initial f(x) and df/dx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 330\u001b[0;31m         \u001b[0morig_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    331\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m         \u001b[0mcurrent_evals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-4700133ed20a>\u001b[0m in \u001b[0;36meval_step\u001b[0;34m()\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m                     \u001b[0mscaled_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtemperature_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msub_input_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msub_attention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m                     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnll_criterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscaled_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msub_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m                     \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0msub_input_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-4700133ed20a>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlogits\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtemperature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/utils/operations.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m     \u001b[0;31m# To act like a decorator so that it can be popped when doing `extract_model_from_parallel`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/utils/operations.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    805\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mconvert_to_fp32\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/amp/autocast_mode.py\u001b[0m in \u001b[0;36mdecorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_autocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mautocast_instance\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mdecorate_autocast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__script_unsupported\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"@autocast() decorator is not supported in script mode\"\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/peft/peft_model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mpeft_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpeft_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mPeftType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPOLY\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"task_ids\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtask_ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1521\u001b[0;31m                 return self.base_model(\n\u001b[0m\u001b[1;32m   1522\u001b[0m                     \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1523\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/peft/tuners/tuners_utils.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_pre_injection_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mPeftConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madapter_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1671\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1672\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1673\u001b[0;31m         outputs = self.bert(\n\u001b[0m\u001b[1;32m   1674\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1675\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1140\u001b[0m         \u001b[0mhead_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_head_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_hidden_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1142\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m   1143\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1144\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    693\u001b[0m                 )\n\u001b[1;32m    694\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 695\u001b[0;31m                 layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m    696\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    697\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    583\u001b[0m         \u001b[0;31m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0mself_attn_past_key_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpast_key_value\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpast_key_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 585\u001b[0;31m         self_attention_outputs = self.attention(\n\u001b[0m\u001b[1;32m    586\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    522\u001b[0m             \u001b[0moutput_attentions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m         )\n\u001b[0;32m--> 524\u001b[0;31m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    525\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# add attentions if we output them\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    526\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 466\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    467\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    468\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayerNorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/peft/tuners/lora/layer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, *args, **kwargs)\u001b[0m\n\u001b[1;32m    607\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    608\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 609\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    610\u001b[0m             \u001b[0mtorch_result_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mactive_adapter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactive_adapters\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 2.12 MiB is free. Process 22430 has 14.74 GiB memory in use. Of the allocated memory 14.58 GiB is allocated by PyTorch, and 19.84 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "import gc\n",
        "\n",
        "class ModelWithTemperature(nn.Module):\n",
        "    def __init__(self, model):\n",
        "        super(ModelWithTemperature, self).__init__()\n",
        "        self.model = model\n",
        "        self.temperature = nn.Parameter(torch.ones(1) * 1.5)\n",
        "\n",
        "        # Enable gradient checkpointing\n",
        "        if hasattr(self.model, 'gradient_checkpointing_enable'):\n",
        "            self.model.gradient_checkpointing_enable()\n",
        "            print(\"Gradient checkpointing enabled\")\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        logits = self.model(input_ids=input_ids, attention_mask=attention_mask).logits\n",
        "        return logits / self.temperature\n",
        "\n",
        "def calibrate_temperature(model, valid_loader, device='cuda', chunk_size=100):\n",
        "    \"\"\"\n",
        "    Temperature calibration with gradient checkpointing and CPU offloading\n",
        "    \"\"\"\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    temperature_model = ModelWithTemperature(model)\n",
        "    temperature_model = temperature_model.to(device)\n",
        "    temperature_model.eval()\n",
        "\n",
        "    nll_criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.LBFGS([temperature_model.temperature], lr=0.01, max_iter=50)\n",
        "\n",
        "    # Get total dataset size\n",
        "    total_samples = len(valid_loader.dataset)\n",
        "\n",
        "    # Create chunks of dataset indices\n",
        "    chunk_indices = list(range(0, total_samples, chunk_size))\n",
        "\n",
        "    def eval_step():\n",
        "        optimizer.zero_grad()\n",
        "        total_loss = torch.tensor(0., device=device)\n",
        "        processed_samples = 0\n",
        "\n",
        "        # Process dataset in chunks\n",
        "        for start_idx in chunk_indices:\n",
        "            end_idx = min(start_idx + chunk_size, total_samples)\n",
        "            chunk_dataset = Subset(valid_loader.dataset, range(start_idx, end_idx))\n",
        "            chunk_loader = DataLoader(\n",
        "                chunk_dataset,\n",
        "                batch_size=1,  # Process one sample at a time\n",
        "                collate_fn=valid_loader.collate_fn\n",
        "            )\n",
        "\n",
        "            # Process each chunk\n",
        "            for batch in chunk_loader:\n",
        "                input_ids = batch['input_ids'].to(device)\n",
        "                attention_mask = batch['attention_mask'].to(device)\n",
        "                labels = batch['labels'].to(device)\n",
        "\n",
        "                with torch.set_grad_enabled(True):\n",
        "                    scaled_logits = temperature_model(input_ids, attention_mask)\n",
        "                    loss = nll_criterion(scaled_logits, labels)\n",
        "                    total_loss += loss * input_ids.size(0)\n",
        "                    processed_samples += input_ids.size(0)\n",
        "\n",
        "                # Clear memory\n",
        "                del input_ids, attention_mask, labels, scaled_logits\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "            print(f\"Processed {processed_samples}/{total_samples} samples\")\n",
        "\n",
        "        avg_loss = total_loss / processed_samples\n",
        "        avg_loss.backward()\n",
        "        return avg_loss\n",
        "\n",
        "    optimizer.step(eval_step)\n",
        "    return temperature_model\n",
        "\n",
        "# Create validation dataloader with smaller chunk size\n",
        "print(\"Starting temperature calibration with gradient checkpointing...\")\n",
        "try:\n",
        "    calibrated_model = calibrate_temperature(\n",
        "        model_5,\n",
        "        valid_dataloader,\n",
        "        chunk_size=50  # Process 50 samples at a time\n",
        "    )\n",
        "    optimal_temperature = calibrated_model.temperature.item()\n",
        "    print(f\"Optimal temperature: {optimal_temperature:.3f}\")\n",
        "\n",
        "    def get_predictions(model, dataloader, temperature=None, device='cuda', chunk_size=50):\n",
        "        model.eval()\n",
        "        all_probs = []\n",
        "        all_labels = []\n",
        "        total_samples = len(dataloader.dataset)\n",
        "\n",
        "        for start_idx in range(0, total_samples, chunk_size):\n",
        "            end_idx = min(start_idx + chunk_size, total_samples)\n",
        "            chunk_dataset = Subset(dataloader.dataset, range(start_idx, end_idx))\n",
        "            chunk_loader = DataLoader(\n",
        "                chunk_dataset,\n",
        "                batch_size=1,\n",
        "                collate_fn=dataloader.collate_fn\n",
        "            )\n",
        "\n",
        "            with torch.no_grad():\n",
        "                for batch in chunk_loader:\n",
        "                    input_ids = batch['input_ids'].to(device)\n",
        "                    attention_mask = batch['attention_mask'].to(device)\n",
        "                    labels = batch['labels']\n",
        "\n",
        "                    outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "                    logits = outputs.logits\n",
        "\n",
        "                    if temperature is not None:\n",
        "                        logits = logits / temperature\n",
        "\n",
        "                    probs = torch.softmax(logits, dim=-1)\n",
        "\n",
        "                    all_probs.extend(probs.cpu().numpy())\n",
        "                    all_labels.extend(labels.numpy())\n",
        "\n",
        "                    del input_ids, attention_mask, logits, probs\n",
        "                    torch.cuda.empty_cache()\n",
        "\n",
        "            print(f\"Processed predictions for {end_idx}/{total_samples} samples\")\n",
        "\n",
        "        return np.array(all_probs), np.array(all_labels)\n",
        "\n",
        "    print(\"Computing predictions...\")\n",
        "    uncalibrated_probs, true_labels = get_predictions(model_5, valid_dataloader)\n",
        "    calibrated_probs, _ = get_predictions(model_5, valid_dataloader, optimal_temperature)\n",
        "\n",
        "    def compute_ece(probs, labels, n_bins=15):\n",
        "        confidences = np.max(probs, axis=1)\n",
        "        predictions = np.argmax(probs, axis=1)\n",
        "        accuracies = predictions == labels\n",
        "\n",
        "        ece = 0\n",
        "        total_samples = len(labels)\n",
        "\n",
        "        for bin_lower in np.linspace(0, 1, n_bins+1)[:-1]:\n",
        "            bin_upper = bin_lower + 1/n_bins\n",
        "            bin_mask = (confidences >= bin_lower) & (confidences < bin_upper)\n",
        "            if np.any(bin_mask):\n",
        "                bin_accuracy = np.mean(accuracies[bin_mask])\n",
        "                bin_confidence = np.mean(confidences[bin_mask])\n",
        "                bin_samples = np.sum(bin_mask)\n",
        "                ece += (bin_samples / total_samples) * np.abs(bin_accuracy - bin_confidence)\n",
        "\n",
        "        return ece\n",
        "\n",
        "    ece_before = compute_ece(uncalibrated_probs, true_labels)\n",
        "    ece_after = compute_ece(calibrated_probs, true_labels)\n",
        "\n",
        "    print(f\"ECE Before Calibration: {ece_before:.3f}\")\n",
        "    print(f\"ECE After Calibration: {ece_after:.3f}\")\n",
        "\n",
        "except RuntimeError as e:\n",
        "    if \"out of memory\" in str(e):\n",
        "        print(\"Still experiencing memory issues. Try these additional steps:\")\n",
        "        print(\"1. Reduce chunk_size further (currently 50)\")\n",
        "        print(\"2. Enable mixed precision (fp16)\")\n",
        "        print(\"3. Reduce model precision to float16\")\n",
        "    raise e"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        },
        "id": "iztIL5Fkvtck",
        "outputId": "57c26624-b6d2-4de4-f34d-6552a79df022"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting temperature calibration with gradient checkpointing...\n",
            "Gradient checkpointing enabled\n",
            "Still experiencing memory issues. Try these additional steps:\n",
            "1. Reduce chunk_size further (currently 50)\n",
            "2. Enable mixed precision (fp16)\n",
            "3. Reduce model precision to float16\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OutOfMemoryError",
          "evalue": "CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 2.12 MiB is free. Process 22430 has 14.74 GiB memory in use. Of the allocated memory 14.59 GiB is allocated by PyTorch, and 11.80 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-36b7262faecc>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"2. Enable mixed precision (fp16)\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"3. Reduce model precision to float16\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-32-36b7262faecc>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Starting temperature calibration with gradient checkpointing...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m     calibrated_model = calibrate_temperature(\n\u001b[0m\u001b[1;32m     87\u001b[0m         \u001b[0mmodel_5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0mvalid_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-32-36b7262faecc>\u001b[0m in \u001b[0;36mcalibrate_temperature\u001b[0;34m(model, valid_loader, device, chunk_size)\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mavg_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0meval_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtemperature_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                             )\n\u001b[1;32m    486\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 487\u001b[0;31m                 \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    488\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optimizer_step_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/optim/lbfgs.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    328\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    329\u001b[0m         \u001b[0;31m# evaluate initial f(x) and df/dx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 330\u001b[0;31m         \u001b[0morig_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    331\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    332\u001b[0m         \u001b[0mcurrent_evals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-32-36b7262faecc>\u001b[0m in \u001b[0;36meval_step\u001b[0;34m()\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m                     \u001b[0mscaled_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtemperature_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m                     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnll_criterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscaled_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m                     \u001b[0mtotal_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-32-36b7262faecc>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m         \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlogits\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtemperature\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/utils/operations.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m     \u001b[0;31m# To act like a decorator so that it can be popped when doing `extract_model_from_parallel`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/utils/operations.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    805\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mconvert_to_fp32\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/amp/autocast_mode.py\u001b[0m in \u001b[0;36mdecorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_autocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mautocast_instance\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mdecorate_autocast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__script_unsupported\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"@autocast() decorator is not supported in script mode\"\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/peft/peft_model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mpeft_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpeft_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mPeftType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPOLY\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"task_ids\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtask_ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1521\u001b[0;31m                 return self.base_model(\n\u001b[0m\u001b[1;32m   1522\u001b[0m                     \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1523\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/peft/tuners/tuners_utils.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_pre_injection_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mPeftConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madapter_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1671\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1672\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1673\u001b[0;31m         outputs = self.bert(\n\u001b[0m\u001b[1;32m   1674\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1675\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1140\u001b[0m         \u001b[0mhead_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_head_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_hidden_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1142\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m   1143\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1144\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    693\u001b[0m                 )\n\u001b[1;32m    694\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 695\u001b[0;31m                 layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m    696\u001b[0m                     \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    697\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    625\u001b[0m             \u001b[0mpresent_key_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpresent_key_value\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mcross_attn_present_key_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 627\u001b[0;31m         layer_output = apply_chunking_to_forward(\n\u001b[0m\u001b[1;32m    628\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_forward_chunk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk_size_feed_forward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq_len_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/pytorch_utils.py\u001b[0m in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m    259\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_chunks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mchunk_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mforward_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mfeed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    638\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfeed_forward_chunk\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    639\u001b[0m         \u001b[0mintermediate_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 640\u001b[0;31m         \u001b[0mlayer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintermediate_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    641\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlayer_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[1;32m    552\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 554\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayerNorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    555\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 14.74 GiB of which 2.12 MiB is free. Process 22430 has 14.74 GiB memory in use. Of the allocated memory 14.59 GiB is allocated by PyTorch, and 11.80 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "import gc\n",
        "\n",
        "class ModelWithTemperature(nn.Module):\n",
        "    def __init__(self, model):\n",
        "        super(ModelWithTemperature, self).__init__()\n",
        "        # Convert model to half precision\n",
        "        self.model = model.half()\n",
        "        self.temperature = nn.Parameter(torch.ones(1, dtype=torch.float16) * 1.5)\n",
        "\n",
        "        # Enable gradient checkpointing\n",
        "        if hasattr(self.model, 'gradient_checkpointing_enable'):\n",
        "            self.model.gradient_checkpointing_enable()\n",
        "            print(\"Gradient checkpointing enabled\")\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        with autocast():\n",
        "            logits = self.model(input_ids=input_ids, attention_mask=attention_mask).logits\n",
        "            return logits / self.temperature\n",
        "\n",
        "def calibrate_temperature(model, valid_loader, device='cuda', chunk_size=10):\n",
        "    \"\"\"\n",
        "    Ultra memory-efficient temperature calibration with all optimizations\n",
        "    \"\"\"\n",
        "    # Clear memory\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    # Initialize model with optimizations\n",
        "    temperature_model = ModelWithTemperature(model)\n",
        "    temperature_model = temperature_model.to(device)\n",
        "    temperature_model.eval()\n",
        "\n",
        "    # Initialize mixed precision training\n",
        "    scaler = GradScaler()\n",
        "\n",
        "    nll_criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.LBFGS([temperature_model.temperature], lr=0.01, max_iter=50)\n",
        "\n",
        "    # Get total dataset size\n",
        "    total_samples = len(valid_loader.dataset)\n",
        "\n",
        "    # Create smaller chunks\n",
        "    chunk_indices = list(range(0, total_samples, chunk_size))\n",
        "\n",
        "    def eval_step():\n",
        "        optimizer.zero_grad()\n",
        "        total_loss = torch.tensor(0., device=device, dtype=torch.float16)\n",
        "        processed_samples = 0\n",
        "\n",
        "        for start_idx in chunk_indices:\n",
        "            end_idx = min(start_idx + chunk_size, total_samples)\n",
        "            chunk_dataset = Subset(valid_loader.dataset, range(start_idx, end_idx))\n",
        "            chunk_loader = DataLoader(\n",
        "                chunk_dataset,\n",
        "                batch_size=1,\n",
        "                collate_fn=valid_loader.collate_fn\n",
        "            )\n",
        "\n",
        "            for batch in chunk_loader:\n",
        "                # Convert inputs to half precision\n",
        "                input_ids = batch['input_ids'].to(device).half()\n",
        "                attention_mask = batch['attention_mask'].to(device).half()\n",
        "                labels = batch['labels'].to(device)\n",
        "\n",
        "                with torch.set_grad_enabled(True), autocast():\n",
        "                    scaled_logits = temperature_model(input_ids, attention_mask)\n",
        "                    loss = nll_criterion(scaled_logits, labels)\n",
        "                    loss = loss * input_ids.size(0)\n",
        "\n",
        "                # Scale loss and accumulate\n",
        "                scaler.scale(loss).backward()\n",
        "                total_loss += loss.item()\n",
        "                processed_samples += input_ids.size(0)\n",
        "\n",
        "                # Clear memory\n",
        "                del input_ids, attention_mask, labels, scaled_logits, loss\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "            print(f\"Processed {processed_samples}/{total_samples} samples\")\n",
        "\n",
        "        avg_loss = total_loss / processed_samples\n",
        "        return avg_loss\n",
        "\n",
        "    try:\n",
        "        optimizer.step(eval_step)\n",
        "    except RuntimeError as e:\n",
        "        print(f\"Error during optimization: {str(e)}\")\n",
        "        print(\"Current temperature value:\", temperature_model.temperature.item())\n",
        "\n",
        "    return temperature_model\n",
        "\n",
        "# Try to free up as much memory as possible\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "print(\"Starting ultra memory-efficient temperature calibration...\")\n",
        "\n",
        "try:\n",
        "    # Convert model to half precision before calibration\n",
        "    model_5 = model_5.half()\n",
        "\n",
        "    calibrated_model = calibrate_temperature(\n",
        "        model_5,\n",
        "        valid_dataloader,\n",
        "        chunk_size=5  # Very small chunk size\n",
        "    )\n",
        "    optimal_temperature = calibrated_model.temperature.item()\n",
        "    print(f\"Optimal temperature: {optimal_temperature:.3f}\")\n",
        "\n",
        "    def get_predictions(model, dataloader, temperature=None, device='cuda', chunk_size=5):\n",
        "        model.eval()\n",
        "        all_probs = []\n",
        "        all_labels = []\n",
        "        total_samples = len(dataloader.dataset)\n",
        "\n",
        "        for start_idx in range(0, total_samples, chunk_size):\n",
        "            end_idx = min(start_idx + chunk_size, total_samples)\n",
        "            chunk_dataset = Subset(dataloader.dataset, range(start_idx, end_idx))\n",
        "            chunk_loader = DataLoader(\n",
        "                chunk_dataset,\n",
        "                batch_size=1,\n",
        "                collate_fn=dataloader.collate_fn\n",
        "            )\n",
        "\n",
        "            with torch.no_grad(), autocast():\n",
        "                for batch in chunk_loader:\n",
        "                    input_ids = batch['input_ids'].to(device).half()\n",
        "                    attention_mask = batch['attention_mask'].to(device).half()\n",
        "                    labels = batch['labels']\n",
        "\n",
        "                    outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "                    logits = outputs.logits\n",
        "\n",
        "                    if temperature is not None:\n",
        "                        logits = logits / temperature\n",
        "\n",
        "                    probs = torch.softmax(logits, dim=-1)\n",
        "\n",
        "                    all_probs.extend(probs.cpu().numpy())\n",
        "                    all_labels.extend(labels.numpy())\n",
        "\n",
        "                    del input_ids, attention_mask, logits, probs\n",
        "                    torch.cuda.empty_cache()\n",
        "\n",
        "            print(f\"Processed predictions for {end_idx}/{total_samples} samples\")\n",
        "\n",
        "        return np.array(all_probs), np.array(all_labels)\n",
        "\n",
        "    print(\"Computing predictions...\")\n",
        "    uncalibrated_probs, true_labels = get_predictions(model_5, valid_dataloader)\n",
        "    calibrated_probs, _ = get_predictions(model_5, valid_dataloader, optimal_temperature)\n",
        "\n",
        "    def compute_ece(probs, labels, n_bins=15):\n",
        "        confidences = np.max(probs, axis=1)\n",
        "        predictions = np.argmax(probs, axis=1)\n",
        "        accuracies = predictions == labels\n",
        "\n",
        "        ece = 0\n",
        "        total_samples = len(labels)\n",
        "\n",
        "        for bin_lower in np.linspace(0, 1, n_bins+1)[:-1]:\n",
        "            bin_upper = bin_lower + 1/n_bins\n",
        "            bin_mask = (confidences >= bin_lower) & (confidences < bin_upper)\n",
        "            if np.any(bin_mask):\n",
        "                bin_accuracy = np.mean(accuracies[bin_mask])\n",
        "                bin_confidence = np.mean(confidences[bin_mask])\n",
        "                bin_samples = np.sum(bin_mask)\n",
        "                ece += (bin_samples / total_samples) * np.abs(bin_accuracy - bin_confidence)\n",
        "\n",
        "        return ece\n",
        "\n",
        "    ece_before = compute_ece(uncalibrated_probs, true_labels)\n",
        "    ece_after = compute_ece(calibrated_probs, true_labels)\n",
        "\n",
        "    print(f\"ECE Before Calibration: {ece_before:.3f}\")\n",
        "    print(f\"ECE After Calibration: {ece_after:.3f}\")\n",
        "\n",
        "except RuntimeError as e:\n",
        "    if \"out of memory\" in str(e):\n",
        "        print(\"\\nStill experiencing memory issues. Additional suggestions:\")\n",
        "        print(\"1. Try running this on CPU only (much slower but should work)\")\n",
        "        print(\"2. Reduce the validation set size temporarily\")\n",
        "        print(\"3. Try a simpler calibration method like Platt scaling\")\n",
        "    raise e"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 619
        },
        "id": "AFj2hxExwR3J",
        "outputId": "7cd12458-6b73-47d9-a687-a91b48e13f7a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting ultra memory-efficient temperature calibration...\n",
            "Gradient checkpointing enabled\n",
            "Error during optimization: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.cuda.HalfTensor instead (while checking arguments for embedding)\n",
            "Current temperature value: 1.5\n",
            "Optimal temperature: 1.500\n",
            "Computing predictions...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-33-eb717d6f435b>:40: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()\n",
            "<ipython-input-33-eb717d6f435b>:71: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.set_grad_enabled(True), autocast():\n",
            "<ipython-input-33-eb717d6f435b>:22: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "<ipython-input-33-eb717d6f435b>:131: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.no_grad(), autocast():\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.cuda.HalfTensor instead (while checking arguments for embedding)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-eb717d6f435b>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    188\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"2. Reduce the validation set size temporarily\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"3. Try a simpler calibration method like Platt scaling\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-33-eb717d6f435b>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Computing predictions...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m     \u001b[0muncalibrated_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_predictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_dataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m     \u001b[0mcalibrated_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_predictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimal_temperature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-33-eb717d6f435b>\u001b[0m in \u001b[0;36mget_predictions\u001b[0;34m(model, dataloader, temperature, device, chunk_size)\u001b[0m\n\u001b[1;32m    135\u001b[0m                     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'labels'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m                     \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/utils/operations.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    817\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmodel_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m     \u001b[0;31m# To act like a decorator so that it can be popped when doing `extract_model_from_parallel`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/accelerate/utils/operations.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    805\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mconvert_to_fp32\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getstate__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/amp/autocast_mode.py\u001b[0m in \u001b[0;36mdecorate_autocast\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_autocast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mautocast_instance\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mdecorate_autocast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__script_unsupported\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"@autocast() decorator is not supported in script mode\"\u001b[0m  \u001b[0;31m# type: ignore[attr-defined]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/peft/peft_model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[0m\n\u001b[1;32m   1519\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mpeft_config\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpeft_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mPeftType\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPOLY\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1520\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"task_ids\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtask_ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1521\u001b[0;31m                 return self.base_model(\n\u001b[0m\u001b[1;32m   1522\u001b[0m                     \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1523\u001b[0m                     \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/peft/tuners/tuners_utils.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_pre_injection_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mPeftConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madapter_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1671\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1672\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1673\u001b[0;31m         outputs = self.bert(\n\u001b[0m\u001b[1;32m   1674\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1675\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1076\u001b[0m                 \u001b[0mtoken_type_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1077\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1078\u001b[0;31m         embedding_output = self.embeddings(\n\u001b[0m\u001b[1;32m   1079\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1080\u001b[0m             \u001b[0mposition_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)\u001b[0m\n\u001b[1;32m    209\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0minputs_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m         \u001b[0mtoken_type_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_type_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1735\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1736\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m         return F.embedding(\n\u001b[0m\u001b[1;32m    191\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2549\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2550\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2551\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Expected tensor for argument #1 'indices' to have one of the following scalar types: Long, Int; but got torch.cuda.HalfTensor instead (while checking arguments for embedding)"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "from torch.amp import autocast, GradScaler\n",
        "import gc\n",
        "\n",
        "class ModelWithTemperature(nn.Module):\n",
        "    def __init__(self, model):\n",
        "        super(ModelWithTemperature, self).__init__()\n",
        "        self.model = model.half()  # Convert model to half precision\n",
        "        self.temperature = nn.Parameter(torch.ones(1, dtype=torch.float16) * 1.5)\n",
        "\n",
        "        if hasattr(self.model, 'gradient_checkpointing_enable'):\n",
        "            self.model.gradient_checkpointing_enable()\n",
        "            print(\"Gradient checkpointing enabled\")\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        with autocast('cuda'):\n",
        "            # input_ids remains as Long type, attention_mask as float16\n",
        "            logits = self.model(input_ids=input_ids, attention_mask=attention_mask).logits\n",
        "            return logits / self.temperature\n",
        "\n",
        "def calibrate_temperature(model, valid_loader, device='cuda', chunk_size=10):\n",
        "    \"\"\"\n",
        "    Memory-efficient temperature calibration with correct type handling\n",
        "    \"\"\"\n",
        "    torch.cuda.empty_cache()\n",
        "    gc.collect()\n",
        "\n",
        "    temperature_model = ModelWithTemperature(model)\n",
        "    temperature_model = temperature_model.to(device)\n",
        "    temperature_model.eval()\n",
        "\n",
        "    scaler = GradScaler('cuda')\n",
        "    nll_criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.LBFGS([temperature_model.temperature], lr=0.01, max_iter=50)\n",
        "\n",
        "    total_samples = len(valid_loader.dataset)\n",
        "    chunk_indices = list(range(0, total_samples, chunk_size))\n",
        "\n",
        "    def eval_step():\n",
        "        optimizer.zero_grad()\n",
        "        total_loss = torch.tensor(0., device=device, dtype=torch.float32)\n",
        "        processed_samples = 0\n",
        "\n",
        "        for start_idx in chunk_indices:\n",
        "            end_idx = min(start_idx + chunk_size, total_samples)\n",
        "            chunk_dataset = Subset(valid_loader.dataset, range(start_idx, end_idx))\n",
        "            chunk_loader = DataLoader(\n",
        "                chunk_dataset,\n",
        "                batch_size=1,\n",
        "                collate_fn=valid_loader.collate_fn\n",
        "            )\n",
        "\n",
        "            for batch in chunk_loader:\n",
        "                # Keep input_ids as Long, convert attention_mask to half\n",
        "                input_ids = batch['input_ids'].to(device)  # Keep as Long\n",
        "                attention_mask = batch['attention_mask'].to(device).half()\n",
        "                labels = batch['labels'].to(device)\n",
        "\n",
        "                with torch.set_grad_enabled(True), autocast('cuda'):\n",
        "                    scaled_logits = temperature_model(input_ids, attention_mask)\n",
        "                    loss = nll_criterion(scaled_logits, labels)\n",
        "                    loss = loss * input_ids.size(0)\n",
        "\n",
        "                scaler.scale(loss).backward()\n",
        "                total_loss += loss.item()\n",
        "                processed_samples += input_ids.size(0)\n",
        "\n",
        "                del input_ids, attention_mask, labels, scaled_logits, loss\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "            print(f\"Processed {processed_samples}/{total_samples} samples\")\n",
        "\n",
        "        avg_loss = total_loss / processed_samples\n",
        "        return avg_loss\n",
        "\n",
        "    try:\n",
        "        optimizer.step(eval_step)\n",
        "    except RuntimeError as e:\n",
        "        print(f\"Error during optimization: {str(e)}\")\n",
        "        print(\"Current temperature value:\", temperature_model.temperature.item())\n",
        "\n",
        "    return temperature_model\n",
        "\n",
        "# Clear memory before starting\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "print(\"Starting type-aware temperature calibration...\")\n",
        "\n",
        "try:\n",
        "    # Convert model to half precision\n",
        "    model_5 = model_5.half()\n",
        "\n",
        "    calibrated_model = calibrate_temperature(\n",
        "        model_5,\n",
        "        valid_dataloader,\n",
        "        chunk_size=5\n",
        "    )\n",
        "    optimal_temperature = calibrated_model.temperature.item()\n",
        "    print(f\"Optimal temperature: {optimal_temperature:.3f}\")\n",
        "\n",
        "    def get_predictions(model, dataloader, temperature=None, device='cuda', chunk_size=5):\n",
        "        model.eval()\n",
        "        all_probs = []\n",
        "        all_labels = []\n",
        "        total_samples = len(dataloader.dataset)\n",
        "\n",
        "        for start_idx in range(0, total_samples, chunk_size):\n",
        "            end_idx = min(start_idx + chunk_size, total_samples)\n",
        "            chunk_dataset = Subset(dataloader.dataset, range(start_idx, end_idx))\n",
        "            chunk_loader = DataLoader(\n",
        "                chunk_dataset,\n",
        "                batch_size=1,\n",
        "                collate_fn=dataloader.collate_fn\n",
        "            )\n",
        "\n",
        "            with torch.no_grad(), autocast('cuda'):\n",
        "                for batch in chunk_loader:\n",
        "                    input_ids = batch['input_ids'].to(device)  # Keep as Long\n",
        "                    attention_mask = batch['attention_mask'].to(device).half()\n",
        "                    labels = batch['labels']\n",
        "\n",
        "                    outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "                    logits = outputs.logits\n",
        "\n",
        "                    if temperature is not None:\n",
        "                        logits = logits / temperature\n",
        "\n",
        "                    probs = torch.softmax(logits, dim=-1)\n",
        "\n",
        "                    all_probs.extend(probs.cpu().numpy())\n",
        "                    all_labels.extend(labels.numpy())\n",
        "\n",
        "                    del input_ids, attention_mask, logits, probs\n",
        "                    torch.cuda.empty_cache()\n",
        "\n",
        "            print(f\"Processed predictions for {end_idx}/{total_samples} samples\")\n",
        "\n",
        "        return np.array(all_probs), np.array(all_labels)\n",
        "\n",
        "    print(\"Computing predictions...\")\n",
        "    uncalibrated_probs, true_labels = get_predictions(model_5, valid_dataloader)\n",
        "    calibrated_probs, _ = get_predictions(model_5, valid_dataloader, optimal_temperature)\n",
        "\n",
        "    def compute_ece(probs, labels, n_bins=15):\n",
        "        confidences = np.max(probs, axis=1)\n",
        "        predictions = np.argmax(probs, axis=1)\n",
        "        accuracies = predictions == labels\n",
        "\n",
        "        ece = 0\n",
        "        total_samples = len(labels)\n",
        "\n",
        "        for bin_lower in np.linspace(0, 1, n_bins+1)[:-1]:\n",
        "            bin_upper = bin_lower + 1/n_bins\n",
        "            bin_mask = (confidences >= bin_lower) & (confidences < bin_upper)\n",
        "            if np.any(bin_mask):\n",
        "                bin_accuracy = np.mean(accuracies[bin_mask])\n",
        "                bin_confidence = np.mean(confidences[bin_mask])\n",
        "                bin_samples = np.sum(bin_mask)\n",
        "                ece += (bin_samples / total_samples) * np.abs(bin_accuracy - bin_confidence)\n",
        "\n",
        "        return ece\n",
        "\n",
        "    ece_before = compute_ece(uncalibrated_probs, true_labels)\n",
        "    ece_after = compute_ece(calibrated_probs, true_labels)\n",
        "\n",
        "    print(f\"ECE Before Calibration: {ece_before:.3f}\")\n",
        "    print(f\"ECE After Calibration: {ece_after:.3f}\")\n",
        "\n",
        "except RuntimeError as e:\n",
        "    if \"out of memory\" in str(e):\n",
        "        print(\"\\nStill experiencing memory issues. Last resort options:\")\n",
        "        print(\"1. Run on CPU only\")\n",
        "        print(\"2. Use a smaller subset of validation data\")\n",
        "        print(\"3. Try Platt scaling instead\")\n",
        "    raise e"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eQiVrFFMwmQi",
        "outputId": "1c52b3fe-c645-4fd7-907e-0f9a91eeba66"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting type-aware temperature calibration...\n",
            "Gradient checkpointing enabled\n",
            "Processed 5/468 samples\n",
            "Processed 10/468 samples\n",
            "Processed 15/468 samples\n",
            "Processed 20/468 samples\n",
            "Processed 25/468 samples\n",
            "Processed 30/468 samples\n",
            "Processed 35/468 samples\n",
            "Processed 40/468 samples\n",
            "Processed 45/468 samples\n",
            "Processed 50/468 samples\n",
            "Processed 55/468 samples\n",
            "Processed 60/468 samples\n",
            "Processed 65/468 samples\n",
            "Processed 70/468 samples\n",
            "Processed 75/468 samples\n",
            "Processed 80/468 samples\n",
            "Processed 85/468 samples\n",
            "Processed 90/468 samples\n",
            "Processed 95/468 samples\n",
            "Processed 100/468 samples\n",
            "Processed 105/468 samples\n",
            "Processed 110/468 samples\n",
            "Processed 115/468 samples\n",
            "Processed 120/468 samples\n",
            "Processed 125/468 samples\n",
            "Processed 130/468 samples\n",
            "Processed 135/468 samples\n",
            "Processed 140/468 samples\n",
            "Processed 145/468 samples\n",
            "Processed 150/468 samples\n",
            "Processed 155/468 samples\n",
            "Processed 160/468 samples\n",
            "Processed 165/468 samples\n",
            "Processed 170/468 samples\n",
            "Processed 175/468 samples\n",
            "Processed 180/468 samples\n",
            "Processed 185/468 samples\n",
            "Processed 190/468 samples\n",
            "Processed 195/468 samples\n",
            "Processed 200/468 samples\n",
            "Processed 205/468 samples\n",
            "Processed 210/468 samples\n",
            "Processed 215/468 samples\n",
            "Processed 220/468 samples\n",
            "Processed 225/468 samples\n",
            "Processed 230/468 samples\n",
            "Processed 235/468 samples\n",
            "Processed 240/468 samples\n",
            "Processed 245/468 samples\n",
            "Processed 250/468 samples\n",
            "Processed 255/468 samples\n",
            "Processed 260/468 samples\n",
            "Processed 265/468 samples\n",
            "Processed 270/468 samples\n",
            "Processed 275/468 samples\n",
            "Processed 280/468 samples\n",
            "Processed 285/468 samples\n",
            "Processed 290/468 samples\n",
            "Processed 295/468 samples\n",
            "Processed 300/468 samples\n",
            "Processed 305/468 samples\n",
            "Processed 310/468 samples\n",
            "Processed 315/468 samples\n",
            "Processed 320/468 samples\n",
            "Processed 325/468 samples\n",
            "Processed 330/468 samples\n",
            "Processed 335/468 samples\n",
            "Processed 340/468 samples\n",
            "Processed 345/468 samples\n",
            "Processed 350/468 samples\n",
            "Processed 355/468 samples\n",
            "Processed 360/468 samples\n",
            "Processed 365/468 samples\n",
            "Processed 370/468 samples\n",
            "Processed 375/468 samples\n",
            "Processed 380/468 samples\n",
            "Processed 385/468 samples\n",
            "Processed 390/468 samples\n",
            "Processed 395/468 samples\n",
            "Processed 400/468 samples\n",
            "Processed 405/468 samples\n",
            "Processed 410/468 samples\n",
            "Processed 415/468 samples\n",
            "Processed 420/468 samples\n",
            "Processed 425/468 samples\n",
            "Processed 430/468 samples\n",
            "Processed 435/468 samples\n",
            "Processed 440/468 samples\n",
            "Processed 445/468 samples\n",
            "Processed 450/468 samples\n",
            "Processed 455/468 samples\n",
            "Processed 460/468 samples\n",
            "Processed 465/468 samples\n",
            "Processed 468/468 samples\n",
            "Processed 5/468 samples\n",
            "Processed 10/468 samples\n",
            "Processed 15/468 samples\n",
            "Processed 20/468 samples\n",
            "Processed 25/468 samples\n",
            "Processed 30/468 samples\n",
            "Processed 35/468 samples\n",
            "Processed 40/468 samples\n",
            "Processed 45/468 samples\n",
            "Processed 50/468 samples\n",
            "Processed 55/468 samples\n",
            "Processed 60/468 samples\n",
            "Processed 65/468 samples\n",
            "Processed 70/468 samples\n",
            "Processed 75/468 samples\n",
            "Processed 80/468 samples\n",
            "Processed 85/468 samples\n",
            "Processed 90/468 samples\n",
            "Processed 95/468 samples\n",
            "Processed 100/468 samples\n",
            "Processed 105/468 samples\n",
            "Processed 110/468 samples\n",
            "Processed 115/468 samples\n",
            "Processed 120/468 samples\n",
            "Processed 125/468 samples\n",
            "Processed 130/468 samples\n",
            "Processed 135/468 samples\n",
            "Processed 140/468 samples\n",
            "Processed 145/468 samples\n",
            "Processed 150/468 samples\n",
            "Processed 155/468 samples\n",
            "Processed 160/468 samples\n",
            "Processed 165/468 samples\n",
            "Processed 170/468 samples\n",
            "Processed 175/468 samples\n",
            "Processed 180/468 samples\n",
            "Processed 185/468 samples\n",
            "Processed 190/468 samples\n",
            "Processed 195/468 samples\n",
            "Processed 200/468 samples\n",
            "Processed 205/468 samples\n",
            "Processed 210/468 samples\n",
            "Processed 215/468 samples\n",
            "Processed 220/468 samples\n",
            "Processed 225/468 samples\n",
            "Processed 230/468 samples\n",
            "Processed 235/468 samples\n",
            "Processed 240/468 samples\n",
            "Processed 245/468 samples\n",
            "Processed 250/468 samples\n",
            "Processed 255/468 samples\n",
            "Processed 260/468 samples\n",
            "Processed 265/468 samples\n",
            "Processed 270/468 samples\n",
            "Processed 275/468 samples\n",
            "Processed 280/468 samples\n",
            "Processed 285/468 samples\n",
            "Processed 290/468 samples\n",
            "Processed 295/468 samples\n",
            "Processed 300/468 samples\n",
            "Processed 305/468 samples\n",
            "Processed 310/468 samples\n",
            "Processed 315/468 samples\n",
            "Processed 320/468 samples\n",
            "Processed 325/468 samples\n",
            "Processed 330/468 samples\n",
            "Processed 335/468 samples\n",
            "Processed 340/468 samples\n",
            "Processed 345/468 samples\n",
            "Processed 350/468 samples\n",
            "Processed 355/468 samples\n",
            "Processed 360/468 samples\n",
            "Processed 365/468 samples\n",
            "Processed 370/468 samples\n",
            "Processed 375/468 samples\n",
            "Processed 380/468 samples\n",
            "Processed 385/468 samples\n",
            "Processed 390/468 samples\n",
            "Processed 395/468 samples\n",
            "Processed 400/468 samples\n",
            "Processed 405/468 samples\n",
            "Processed 410/468 samples\n",
            "Processed 415/468 samples\n",
            "Processed 420/468 samples\n",
            "Processed 425/468 samples\n",
            "Processed 430/468 samples\n",
            "Processed 435/468 samples\n",
            "Processed 440/468 samples\n",
            "Processed 445/468 samples\n",
            "Processed 450/468 samples\n",
            "Processed 455/468 samples\n",
            "Processed 460/468 samples\n",
            "Processed 465/468 samples\n",
            "Processed 468/468 samples\n",
            "Processed 5/468 samples\n",
            "Processed 10/468 samples\n",
            "Processed 15/468 samples\n",
            "Processed 20/468 samples\n",
            "Processed 25/468 samples\n",
            "Processed 30/468 samples\n",
            "Processed 35/468 samples\n",
            "Processed 40/468 samples\n",
            "Processed 45/468 samples\n",
            "Processed 50/468 samples\n",
            "Processed 55/468 samples\n",
            "Processed 60/468 samples\n",
            "Processed 65/468 samples\n",
            "Processed 70/468 samples\n",
            "Processed 75/468 samples\n",
            "Processed 80/468 samples\n",
            "Processed 85/468 samples\n",
            "Processed 90/468 samples\n",
            "Processed 95/468 samples\n",
            "Processed 100/468 samples\n",
            "Processed 105/468 samples\n",
            "Processed 110/468 samples\n",
            "Processed 115/468 samples\n",
            "Processed 120/468 samples\n",
            "Processed 125/468 samples\n",
            "Processed 130/468 samples\n",
            "Processed 135/468 samples\n",
            "Processed 140/468 samples\n",
            "Processed 145/468 samples\n",
            "Processed 150/468 samples\n",
            "Processed 155/468 samples\n",
            "Processed 160/468 samples\n",
            "Processed 165/468 samples\n",
            "Processed 170/468 samples\n",
            "Processed 175/468 samples\n",
            "Processed 180/468 samples\n",
            "Processed 185/468 samples\n",
            "Processed 190/468 samples\n",
            "Processed 195/468 samples\n",
            "Processed 200/468 samples\n",
            "Processed 205/468 samples\n",
            "Processed 210/468 samples\n",
            "Processed 215/468 samples\n",
            "Processed 220/468 samples\n",
            "Processed 225/468 samples\n",
            "Processed 230/468 samples\n",
            "Processed 235/468 samples\n",
            "Processed 240/468 samples\n",
            "Processed 245/468 samples\n",
            "Processed 250/468 samples\n",
            "Processed 255/468 samples\n",
            "Processed 260/468 samples\n",
            "Processed 265/468 samples\n",
            "Processed 270/468 samples\n",
            "Processed 275/468 samples\n",
            "Processed 280/468 samples\n",
            "Processed 285/468 samples\n",
            "Processed 290/468 samples\n",
            "Processed 295/468 samples\n",
            "Processed 300/468 samples\n",
            "Processed 305/468 samples\n",
            "Processed 310/468 samples\n",
            "Processed 315/468 samples\n",
            "Processed 320/468 samples\n",
            "Processed 325/468 samples\n",
            "Processed 330/468 samples\n",
            "Processed 335/468 samples\n",
            "Processed 340/468 samples\n",
            "Processed 345/468 samples\n",
            "Processed 350/468 samples\n",
            "Processed 355/468 samples\n",
            "Processed 360/468 samples\n",
            "Processed 365/468 samples\n",
            "Processed 370/468 samples\n",
            "Processed 375/468 samples\n",
            "Processed 380/468 samples\n",
            "Processed 385/468 samples\n",
            "Processed 390/468 samples\n",
            "Processed 395/468 samples\n",
            "Processed 400/468 samples\n",
            "Processed 405/468 samples\n",
            "Processed 410/468 samples\n",
            "Processed 415/468 samples\n",
            "Processed 420/468 samples\n",
            "Processed 425/468 samples\n",
            "Processed 430/468 samples\n",
            "Processed 435/468 samples\n",
            "Processed 440/468 samples\n",
            "Processed 445/468 samples\n",
            "Processed 450/468 samples\n",
            "Processed 455/468 samples\n",
            "Processed 460/468 samples\n",
            "Processed 465/468 samples\n",
            "Processed 468/468 samples\n",
            "Processed 5/468 samples\n",
            "Processed 10/468 samples\n",
            "Processed 15/468 samples\n",
            "Processed 20/468 samples\n",
            "Processed 25/468 samples\n",
            "Processed 30/468 samples\n",
            "Processed 35/468 samples\n",
            "Processed 40/468 samples\n",
            "Processed 45/468 samples\n",
            "Processed 50/468 samples\n",
            "Processed 55/468 samples\n",
            "Processed 60/468 samples\n",
            "Processed 65/468 samples\n",
            "Processed 70/468 samples\n",
            "Processed 75/468 samples\n",
            "Processed 80/468 samples\n",
            "Processed 85/468 samples\n",
            "Processed 90/468 samples\n",
            "Processed 95/468 samples\n",
            "Processed 100/468 samples\n",
            "Processed 105/468 samples\n",
            "Processed 110/468 samples\n",
            "Processed 115/468 samples\n",
            "Processed 120/468 samples\n",
            "Processed 125/468 samples\n",
            "Processed 130/468 samples\n",
            "Processed 135/468 samples\n",
            "Processed 140/468 samples\n",
            "Processed 145/468 samples\n",
            "Processed 150/468 samples\n",
            "Processed 155/468 samples\n",
            "Processed 160/468 samples\n",
            "Processed 165/468 samples\n",
            "Processed 170/468 samples\n",
            "Processed 175/468 samples\n",
            "Processed 180/468 samples\n",
            "Processed 185/468 samples\n",
            "Processed 190/468 samples\n",
            "Processed 195/468 samples\n",
            "Processed 200/468 samples\n",
            "Processed 205/468 samples\n",
            "Processed 210/468 samples\n",
            "Processed 215/468 samples\n",
            "Processed 220/468 samples\n",
            "Processed 225/468 samples\n",
            "Processed 230/468 samples\n",
            "Processed 235/468 samples\n",
            "Processed 240/468 samples\n",
            "Processed 245/468 samples\n",
            "Processed 250/468 samples\n",
            "Processed 255/468 samples\n",
            "Processed 260/468 samples\n",
            "Processed 265/468 samples\n",
            "Processed 270/468 samples\n",
            "Processed 275/468 samples\n",
            "Processed 280/468 samples\n",
            "Processed 285/468 samples\n",
            "Processed 290/468 samples\n",
            "Processed 295/468 samples\n",
            "Processed 300/468 samples\n",
            "Processed 305/468 samples\n",
            "Processed 310/468 samples\n",
            "Processed 315/468 samples\n",
            "Processed 320/468 samples\n",
            "Processed 325/468 samples\n",
            "Processed 330/468 samples\n",
            "Processed 335/468 samples\n",
            "Processed 340/468 samples\n",
            "Processed 345/468 samples\n",
            "Processed 350/468 samples\n",
            "Processed 355/468 samples\n",
            "Processed 360/468 samples\n",
            "Processed 365/468 samples\n",
            "Processed 370/468 samples\n",
            "Processed 375/468 samples\n",
            "Processed 380/468 samples\n",
            "Processed 385/468 samples\n",
            "Processed 390/468 samples\n",
            "Processed 395/468 samples\n",
            "Processed 400/468 samples\n",
            "Processed 405/468 samples\n",
            "Processed 410/468 samples\n",
            "Processed 415/468 samples\n",
            "Processed 420/468 samples\n",
            "Processed 425/468 samples\n",
            "Processed 430/468 samples\n",
            "Processed 435/468 samples\n",
            "Processed 440/468 samples\n",
            "Processed 445/468 samples\n",
            "Processed 450/468 samples\n",
            "Processed 455/468 samples\n",
            "Processed 460/468 samples\n",
            "Processed 465/468 samples\n",
            "Processed 468/468 samples\n",
            "Processed 5/468 samples\n",
            "Processed 10/468 samples\n",
            "Processed 15/468 samples\n",
            "Processed 20/468 samples\n",
            "Processed 25/468 samples\n",
            "Processed 30/468 samples\n",
            "Processed 35/468 samples\n",
            "Processed 40/468 samples\n",
            "Processed 45/468 samples\n",
            "Processed 50/468 samples\n",
            "Processed 55/468 samples\n",
            "Processed 60/468 samples\n",
            "Processed 65/468 samples\n",
            "Processed 70/468 samples\n",
            "Processed 75/468 samples\n",
            "Processed 80/468 samples\n",
            "Processed 85/468 samples\n",
            "Processed 90/468 samples\n",
            "Processed 95/468 samples\n",
            "Processed 100/468 samples\n",
            "Processed 105/468 samples\n",
            "Processed 110/468 samples\n",
            "Processed 115/468 samples\n",
            "Processed 120/468 samples\n",
            "Processed 125/468 samples\n",
            "Processed 130/468 samples\n",
            "Processed 135/468 samples\n",
            "Processed 140/468 samples\n",
            "Processed 145/468 samples\n",
            "Processed 150/468 samples\n",
            "Processed 155/468 samples\n",
            "Processed 160/468 samples\n",
            "Processed 165/468 samples\n",
            "Processed 170/468 samples\n",
            "Processed 175/468 samples\n",
            "Processed 180/468 samples\n",
            "Processed 185/468 samples\n",
            "Processed 190/468 samples\n",
            "Processed 195/468 samples\n",
            "Processed 200/468 samples\n",
            "Processed 205/468 samples\n",
            "Processed 210/468 samples\n",
            "Processed 215/468 samples\n",
            "Processed 220/468 samples\n",
            "Processed 225/468 samples\n",
            "Processed 230/468 samples\n",
            "Processed 235/468 samples\n",
            "Processed 240/468 samples\n",
            "Processed 245/468 samples\n",
            "Processed 250/468 samples\n",
            "Processed 255/468 samples\n",
            "Processed 260/468 samples\n",
            "Processed 265/468 samples\n",
            "Processed 270/468 samples\n",
            "Processed 275/468 samples\n",
            "Processed 280/468 samples\n",
            "Processed 285/468 samples\n",
            "Processed 290/468 samples\n",
            "Processed 295/468 samples\n",
            "Processed 300/468 samples\n",
            "Processed 305/468 samples\n",
            "Processed 310/468 samples\n",
            "Processed 315/468 samples\n",
            "Processed 320/468 samples\n",
            "Processed 325/468 samples\n",
            "Processed 330/468 samples\n",
            "Processed 335/468 samples\n",
            "Processed 340/468 samples\n",
            "Processed 345/468 samples\n",
            "Processed 350/468 samples\n",
            "Processed 355/468 samples\n",
            "Processed 360/468 samples\n",
            "Processed 365/468 samples\n",
            "Processed 370/468 samples\n",
            "Processed 375/468 samples\n",
            "Processed 380/468 samples\n",
            "Processed 385/468 samples\n",
            "Processed 390/468 samples\n",
            "Processed 395/468 samples\n",
            "Processed 400/468 samples\n",
            "Processed 405/468 samples\n",
            "Processed 410/468 samples\n",
            "Processed 415/468 samples\n",
            "Processed 420/468 samples\n",
            "Processed 425/468 samples\n",
            "Processed 430/468 samples\n",
            "Processed 435/468 samples\n",
            "Processed 440/468 samples\n",
            "Processed 445/468 samples\n",
            "Processed 450/468 samples\n",
            "Processed 455/468 samples\n",
            "Processed 460/468 samples\n",
            "Processed 465/468 samples\n",
            "Processed 468/468 samples\n",
            "Processed 5/468 samples\n",
            "Processed 10/468 samples\n",
            "Processed 15/468 samples\n",
            "Processed 20/468 samples\n",
            "Processed 25/468 samples\n",
            "Processed 30/468 samples\n",
            "Processed 35/468 samples\n",
            "Processed 40/468 samples\n",
            "Processed 45/468 samples\n",
            "Processed 50/468 samples\n",
            "Processed 55/468 samples\n",
            "Processed 60/468 samples\n",
            "Processed 65/468 samples\n",
            "Processed 70/468 samples\n",
            "Processed 75/468 samples\n",
            "Processed 80/468 samples\n",
            "Processed 85/468 samples\n",
            "Processed 90/468 samples\n",
            "Processed 95/468 samples\n",
            "Processed 100/468 samples\n",
            "Processed 105/468 samples\n",
            "Processed 110/468 samples\n",
            "Processed 115/468 samples\n",
            "Processed 120/468 samples\n",
            "Processed 125/468 samples\n",
            "Processed 130/468 samples\n",
            "Processed 135/468 samples\n",
            "Processed 140/468 samples\n",
            "Processed 145/468 samples\n",
            "Processed 150/468 samples\n",
            "Processed 155/468 samples\n",
            "Processed 160/468 samples\n",
            "Processed 165/468 samples\n",
            "Processed 170/468 samples\n",
            "Processed 175/468 samples\n",
            "Processed 180/468 samples\n",
            "Processed 185/468 samples\n",
            "Processed 190/468 samples\n",
            "Processed 195/468 samples\n",
            "Processed 200/468 samples\n",
            "Processed 205/468 samples\n",
            "Processed 210/468 samples\n",
            "Processed 215/468 samples\n",
            "Processed 220/468 samples\n",
            "Processed 225/468 samples\n",
            "Processed 230/468 samples\n",
            "Processed 235/468 samples\n",
            "Processed 240/468 samples\n",
            "Processed 245/468 samples\n",
            "Processed 250/468 samples\n",
            "Processed 255/468 samples\n",
            "Processed 260/468 samples\n",
            "Processed 265/468 samples\n",
            "Processed 270/468 samples\n",
            "Processed 275/468 samples\n",
            "Processed 280/468 samples\n",
            "Processed 285/468 samples\n",
            "Processed 290/468 samples\n",
            "Processed 295/468 samples\n",
            "Processed 300/468 samples\n",
            "Processed 305/468 samples\n",
            "Processed 310/468 samples\n",
            "Processed 315/468 samples\n",
            "Processed 320/468 samples\n",
            "Processed 325/468 samples\n",
            "Processed 330/468 samples\n",
            "Processed 335/468 samples\n",
            "Processed 340/468 samples\n",
            "Processed 345/468 samples\n",
            "Processed 350/468 samples\n",
            "Processed 355/468 samples\n",
            "Processed 360/468 samples\n",
            "Processed 365/468 samples\n",
            "Processed 370/468 samples\n",
            "Processed 375/468 samples\n",
            "Processed 380/468 samples\n",
            "Processed 385/468 samples\n",
            "Processed 390/468 samples\n",
            "Processed 395/468 samples\n",
            "Processed 400/468 samples\n",
            "Processed 405/468 samples\n",
            "Processed 410/468 samples\n",
            "Processed 415/468 samples\n",
            "Processed 420/468 samples\n",
            "Processed 425/468 samples\n",
            "Processed 430/468 samples\n",
            "Processed 435/468 samples\n",
            "Processed 440/468 samples\n",
            "Processed 445/468 samples\n",
            "Processed 450/468 samples\n",
            "Processed 455/468 samples\n",
            "Processed 460/468 samples\n",
            "Processed 465/468 samples\n",
            "Processed 468/468 samples\n",
            "Processed 5/468 samples\n",
            "Processed 10/468 samples\n",
            "Processed 15/468 samples\n",
            "Processed 20/468 samples\n",
            "Processed 25/468 samples\n",
            "Processed 30/468 samples\n",
            "Processed 35/468 samples\n",
            "Processed 40/468 samples\n",
            "Processed 45/468 samples\n",
            "Processed 50/468 samples\n",
            "Processed 55/468 samples\n",
            "Processed 60/468 samples\n",
            "Processed 65/468 samples\n",
            "Processed 70/468 samples\n",
            "Processed 75/468 samples\n",
            "Processed 80/468 samples\n",
            "Processed 85/468 samples\n",
            "Processed 90/468 samples\n",
            "Processed 95/468 samples\n",
            "Processed 100/468 samples\n",
            "Processed 105/468 samples\n",
            "Processed 110/468 samples\n",
            "Processed 115/468 samples\n",
            "Processed 120/468 samples\n",
            "Processed 125/468 samples\n",
            "Processed 130/468 samples\n",
            "Processed 135/468 samples\n",
            "Processed 140/468 samples\n",
            "Processed 145/468 samples\n",
            "Processed 150/468 samples\n",
            "Processed 155/468 samples\n",
            "Processed 160/468 samples\n",
            "Processed 165/468 samples\n",
            "Processed 170/468 samples\n",
            "Processed 175/468 samples\n",
            "Processed 180/468 samples\n",
            "Processed 185/468 samples\n",
            "Processed 190/468 samples\n",
            "Processed 195/468 samples\n",
            "Processed 200/468 samples\n",
            "Processed 205/468 samples\n",
            "Processed 210/468 samples\n",
            "Processed 215/468 samples\n",
            "Processed 220/468 samples\n",
            "Processed 225/468 samples\n",
            "Processed 230/468 samples\n",
            "Processed 235/468 samples\n",
            "Processed 240/468 samples\n",
            "Processed 245/468 samples\n",
            "Processed 250/468 samples\n",
            "Processed 255/468 samples\n",
            "Processed 260/468 samples\n",
            "Processed 265/468 samples\n",
            "Processed 270/468 samples\n",
            "Processed 275/468 samples\n",
            "Processed 280/468 samples\n",
            "Processed 285/468 samples\n",
            "Processed 290/468 samples\n",
            "Processed 295/468 samples\n",
            "Processed 300/468 samples\n",
            "Processed 305/468 samples\n",
            "Processed 310/468 samples\n",
            "Processed 315/468 samples\n",
            "Processed 320/468 samples\n",
            "Processed 325/468 samples\n",
            "Processed 330/468 samples\n",
            "Processed 335/468 samples\n",
            "Processed 340/468 samples\n",
            "Processed 345/468 samples\n",
            "Processed 350/468 samples\n",
            "Processed 355/468 samples\n",
            "Processed 360/468 samples\n",
            "Processed 365/468 samples\n",
            "Processed 370/468 samples\n",
            "Processed 375/468 samples\n",
            "Processed 380/468 samples\n",
            "Processed 385/468 samples\n",
            "Processed 390/468 samples\n",
            "Processed 395/468 samples\n",
            "Processed 400/468 samples\n",
            "Processed 405/468 samples\n",
            "Processed 410/468 samples\n",
            "Processed 415/468 samples\n",
            "Processed 420/468 samples\n",
            "Processed 425/468 samples\n",
            "Processed 430/468 samples\n",
            "Processed 435/468 samples\n",
            "Processed 440/468 samples\n",
            "Processed 445/468 samples\n",
            "Processed 450/468 samples\n",
            "Processed 455/468 samples\n",
            "Processed 460/468 samples\n",
            "Processed 465/468 samples\n",
            "Processed 468/468 samples\n",
            "Processed 5/468 samples\n",
            "Processed 10/468 samples\n",
            "Processed 15/468 samples\n",
            "Processed 20/468 samples\n",
            "Processed 25/468 samples\n",
            "Processed 30/468 samples\n",
            "Processed 35/468 samples\n",
            "Processed 40/468 samples\n",
            "Processed 45/468 samples\n",
            "Processed 50/468 samples\n",
            "Processed 55/468 samples\n",
            "Processed 60/468 samples\n",
            "Processed 65/468 samples\n",
            "Processed 70/468 samples\n",
            "Processed 75/468 samples\n",
            "Processed 80/468 samples\n",
            "Processed 85/468 samples\n",
            "Processed 90/468 samples\n",
            "Processed 95/468 samples\n",
            "Processed 100/468 samples\n",
            "Processed 105/468 samples\n",
            "Processed 110/468 samples\n",
            "Processed 115/468 samples\n",
            "Processed 120/468 samples\n",
            "Processed 125/468 samples\n",
            "Processed 130/468 samples\n",
            "Processed 135/468 samples\n",
            "Processed 140/468 samples\n",
            "Processed 145/468 samples\n",
            "Processed 150/468 samples\n",
            "Processed 155/468 samples\n",
            "Processed 160/468 samples\n",
            "Processed 165/468 samples\n",
            "Processed 170/468 samples\n",
            "Processed 175/468 samples\n",
            "Processed 180/468 samples\n",
            "Processed 185/468 samples\n",
            "Processed 190/468 samples\n",
            "Processed 195/468 samples\n",
            "Processed 200/468 samples\n",
            "Processed 205/468 samples\n",
            "Processed 210/468 samples\n",
            "Processed 215/468 samples\n",
            "Processed 220/468 samples\n",
            "Processed 225/468 samples\n",
            "Processed 230/468 samples\n",
            "Processed 235/468 samples\n",
            "Processed 240/468 samples\n",
            "Processed 245/468 samples\n",
            "Processed 250/468 samples\n",
            "Processed 255/468 samples\n",
            "Processed 260/468 samples\n",
            "Processed 265/468 samples\n",
            "Processed 270/468 samples\n",
            "Processed 275/468 samples\n",
            "Processed 280/468 samples\n",
            "Processed 285/468 samples\n",
            "Processed 290/468 samples\n",
            "Processed 295/468 samples\n",
            "Processed 300/468 samples\n",
            "Processed 305/468 samples\n",
            "Processed 310/468 samples\n",
            "Processed 315/468 samples\n",
            "Processed 320/468 samples\n",
            "Processed 325/468 samples\n",
            "Processed 330/468 samples\n",
            "Processed 335/468 samples\n",
            "Processed 340/468 samples\n",
            "Processed 345/468 samples\n",
            "Processed 350/468 samples\n",
            "Processed 355/468 samples\n",
            "Processed 360/468 samples\n",
            "Processed 365/468 samples\n",
            "Processed 370/468 samples\n",
            "Processed 375/468 samples\n",
            "Processed 380/468 samples\n",
            "Processed 385/468 samples\n",
            "Processed 390/468 samples\n",
            "Processed 395/468 samples\n",
            "Processed 400/468 samples\n",
            "Processed 405/468 samples\n",
            "Processed 410/468 samples\n",
            "Processed 415/468 samples\n",
            "Processed 420/468 samples\n",
            "Processed 425/468 samples\n",
            "Processed 430/468 samples\n",
            "Processed 435/468 samples\n",
            "Processed 440/468 samples\n",
            "Processed 445/468 samples\n",
            "Processed 450/468 samples\n",
            "Processed 455/468 samples\n",
            "Processed 460/468 samples\n",
            "Processed 465/468 samples\n",
            "Processed 468/468 samples\n",
            "Processed 5/468 samples\n",
            "Processed 10/468 samples\n",
            "Processed 15/468 samples\n",
            "Processed 20/468 samples\n",
            "Processed 25/468 samples\n",
            "Processed 30/468 samples\n",
            "Processed 35/468 samples\n",
            "Processed 40/468 samples\n",
            "Processed 45/468 samples\n",
            "Processed 50/468 samples\n",
            "Processed 55/468 samples\n",
            "Processed 60/468 samples\n",
            "Processed 65/468 samples\n",
            "Processed 70/468 samples\n",
            "Processed 75/468 samples\n",
            "Processed 80/468 samples\n",
            "Processed 85/468 samples\n",
            "Processed 90/468 samples\n",
            "Processed 95/468 samples\n",
            "Processed 100/468 samples\n",
            "Processed 105/468 samples\n",
            "Processed 110/468 samples\n",
            "Processed 115/468 samples\n",
            "Processed 120/468 samples\n",
            "Processed 125/468 samples\n",
            "Processed 130/468 samples\n",
            "Processed 135/468 samples\n",
            "Processed 140/468 samples\n",
            "Processed 145/468 samples\n",
            "Processed 150/468 samples\n",
            "Processed 155/468 samples\n",
            "Processed 160/468 samples\n",
            "Processed 165/468 samples\n",
            "Processed 170/468 samples\n",
            "Processed 175/468 samples\n",
            "Processed 180/468 samples\n",
            "Processed 185/468 samples\n",
            "Processed 190/468 samples\n",
            "Processed 195/468 samples\n",
            "Processed 200/468 samples\n",
            "Processed 205/468 samples\n",
            "Processed 210/468 samples\n",
            "Processed 215/468 samples\n",
            "Processed 220/468 samples\n",
            "Processed 225/468 samples\n",
            "Processed 230/468 samples\n",
            "Processed 235/468 samples\n",
            "Processed 240/468 samples\n",
            "Processed 245/468 samples\n",
            "Processed 250/468 samples\n",
            "Processed 255/468 samples\n",
            "Processed 260/468 samples\n",
            "Processed 265/468 samples\n",
            "Processed 270/468 samples\n",
            "Processed 275/468 samples\n",
            "Processed 280/468 samples\n",
            "Processed 285/468 samples\n",
            "Processed 290/468 samples\n",
            "Processed 295/468 samples\n",
            "Processed 300/468 samples\n",
            "Processed 305/468 samples\n",
            "Processed 310/468 samples\n",
            "Processed 315/468 samples\n",
            "Processed 320/468 samples\n",
            "Processed 325/468 samples\n",
            "Processed 330/468 samples\n",
            "Processed 335/468 samples\n",
            "Processed 340/468 samples\n",
            "Processed 345/468 samples\n",
            "Processed 350/468 samples\n",
            "Processed 355/468 samples\n",
            "Processed 360/468 samples\n",
            "Processed 365/468 samples\n",
            "Processed 370/468 samples\n",
            "Processed 375/468 samples\n",
            "Processed 380/468 samples\n",
            "Processed 385/468 samples\n",
            "Processed 390/468 samples\n",
            "Processed 395/468 samples\n",
            "Processed 400/468 samples\n",
            "Processed 405/468 samples\n",
            "Processed 410/468 samples\n",
            "Processed 415/468 samples\n",
            "Processed 420/468 samples\n",
            "Processed 425/468 samples\n",
            "Processed 430/468 samples\n",
            "Processed 435/468 samples\n",
            "Processed 440/468 samples\n",
            "Processed 445/468 samples\n",
            "Processed 450/468 samples\n",
            "Processed 455/468 samples\n",
            "Processed 460/468 samples\n",
            "Processed 465/468 samples\n",
            "Processed 468/468 samples\n",
            "Processed 5/468 samples\n",
            "Processed 10/468 samples\n",
            "Processed 15/468 samples\n",
            "Processed 20/468 samples\n",
            "Processed 25/468 samples\n",
            "Processed 30/468 samples\n",
            "Processed 35/468 samples\n",
            "Processed 40/468 samples\n",
            "Processed 45/468 samples\n",
            "Processed 50/468 samples\n",
            "Processed 55/468 samples\n",
            "Processed 60/468 samples\n",
            "Processed 65/468 samples\n",
            "Processed 70/468 samples\n",
            "Processed 75/468 samples\n",
            "Processed 80/468 samples\n",
            "Processed 85/468 samples\n",
            "Processed 90/468 samples\n",
            "Processed 95/468 samples\n",
            "Processed 100/468 samples\n",
            "Processed 105/468 samples\n",
            "Processed 110/468 samples\n",
            "Processed 115/468 samples\n",
            "Processed 120/468 samples\n",
            "Processed 125/468 samples\n",
            "Processed 130/468 samples\n",
            "Processed 135/468 samples\n",
            "Processed 140/468 samples\n",
            "Processed 145/468 samples\n",
            "Processed 150/468 samples\n",
            "Processed 155/468 samples\n",
            "Processed 160/468 samples\n",
            "Processed 165/468 samples\n",
            "Processed 170/468 samples\n",
            "Processed 175/468 samples\n",
            "Processed 180/468 samples\n",
            "Processed 185/468 samples\n",
            "Processed 190/468 samples\n",
            "Processed 195/468 samples\n",
            "Processed 200/468 samples\n",
            "Processed 205/468 samples\n",
            "Processed 210/468 samples\n",
            "Processed 215/468 samples\n",
            "Processed 220/468 samples\n",
            "Processed 225/468 samples\n",
            "Processed 230/468 samples\n",
            "Processed 235/468 samples\n",
            "Processed 240/468 samples\n",
            "Processed 245/468 samples\n",
            "Processed 250/468 samples\n",
            "Processed 255/468 samples\n",
            "Processed 260/468 samples\n",
            "Processed 265/468 samples\n",
            "Processed 270/468 samples\n",
            "Processed 275/468 samples\n",
            "Processed 280/468 samples\n",
            "Processed 285/468 samples\n",
            "Processed 290/468 samples\n",
            "Processed 295/468 samples\n",
            "Processed 300/468 samples\n",
            "Processed 305/468 samples\n",
            "Processed 310/468 samples\n",
            "Processed 315/468 samples\n",
            "Processed 320/468 samples\n",
            "Processed 325/468 samples\n",
            "Processed 330/468 samples\n",
            "Processed 335/468 samples\n",
            "Processed 340/468 samples\n",
            "Processed 345/468 samples\n",
            "Processed 350/468 samples\n",
            "Processed 355/468 samples\n",
            "Processed 360/468 samples\n",
            "Processed 365/468 samples\n",
            "Processed 370/468 samples\n",
            "Processed 375/468 samples\n",
            "Processed 380/468 samples\n",
            "Processed 385/468 samples\n",
            "Processed 390/468 samples\n",
            "Processed 395/468 samples\n",
            "Processed 400/468 samples\n",
            "Processed 405/468 samples\n",
            "Processed 410/468 samples\n",
            "Processed 415/468 samples\n",
            "Processed 420/468 samples\n",
            "Processed 425/468 samples\n",
            "Processed 430/468 samples\n",
            "Processed 435/468 samples\n",
            "Processed 440/468 samples\n",
            "Processed 445/468 samples\n",
            "Processed 450/468 samples\n",
            "Processed 455/468 samples\n",
            "Processed 460/468 samples\n",
            "Processed 465/468 samples\n",
            "Processed 468/468 samples\n",
            "Processed 5/468 samples\n",
            "Processed 10/468 samples\n",
            "Processed 15/468 samples\n",
            "Processed 20/468 samples\n",
            "Processed 25/468 samples\n",
            "Processed 30/468 samples\n",
            "Processed 35/468 samples\n",
            "Processed 40/468 samples\n",
            "Processed 45/468 samples\n",
            "Processed 50/468 samples\n",
            "Processed 55/468 samples\n",
            "Processed 60/468 samples\n",
            "Processed 65/468 samples\n",
            "Processed 70/468 samples\n",
            "Processed 75/468 samples\n",
            "Processed 80/468 samples\n",
            "Processed 85/468 samples\n",
            "Processed 90/468 samples\n",
            "Processed 95/468 samples\n",
            "Processed 100/468 samples\n",
            "Processed 105/468 samples\n",
            "Processed 110/468 samples\n",
            "Processed 115/468 samples\n",
            "Processed 120/468 samples\n",
            "Processed 125/468 samples\n",
            "Processed 130/468 samples\n",
            "Processed 135/468 samples\n",
            "Processed 140/468 samples\n",
            "Processed 145/468 samples\n",
            "Processed 150/468 samples\n",
            "Processed 155/468 samples\n",
            "Processed 160/468 samples\n",
            "Processed 165/468 samples\n",
            "Processed 170/468 samples\n",
            "Processed 175/468 samples\n",
            "Processed 180/468 samples\n",
            "Processed 185/468 samples\n",
            "Processed 190/468 samples\n",
            "Processed 195/468 samples\n",
            "Processed 200/468 samples\n",
            "Processed 205/468 samples\n",
            "Processed 210/468 samples\n",
            "Processed 215/468 samples\n",
            "Processed 220/468 samples\n",
            "Processed 225/468 samples\n",
            "Processed 230/468 samples\n",
            "Processed 235/468 samples\n",
            "Processed 240/468 samples\n",
            "Processed 245/468 samples\n",
            "Processed 250/468 samples\n",
            "Processed 255/468 samples\n",
            "Processed 260/468 samples\n",
            "Processed 265/468 samples\n",
            "Processed 270/468 samples\n",
            "Processed 275/468 samples\n",
            "Processed 280/468 samples\n",
            "Processed 285/468 samples\n",
            "Processed 290/468 samples\n",
            "Processed 295/468 samples\n",
            "Processed 300/468 samples\n",
            "Processed 305/468 samples\n",
            "Processed 310/468 samples\n",
            "Processed 315/468 samples\n",
            "Processed 320/468 samples\n",
            "Processed 325/468 samples\n",
            "Processed 330/468 samples\n",
            "Processed 335/468 samples\n",
            "Processed 340/468 samples\n",
            "Processed 345/468 samples\n",
            "Processed 350/468 samples\n",
            "Processed 355/468 samples\n",
            "Processed 360/468 samples\n",
            "Processed 365/468 samples\n",
            "Processed 370/468 samples\n",
            "Processed 375/468 samples\n",
            "Processed 380/468 samples\n",
            "Processed 385/468 samples\n",
            "Processed 390/468 samples\n",
            "Processed 395/468 samples\n",
            "Processed 400/468 samples\n",
            "Processed 405/468 samples\n",
            "Processed 410/468 samples\n",
            "Processed 415/468 samples\n",
            "Processed 420/468 samples\n",
            "Processed 425/468 samples\n",
            "Processed 430/468 samples\n",
            "Processed 435/468 samples\n",
            "Processed 440/468 samples\n",
            "Processed 445/468 samples\n",
            "Processed 450/468 samples\n",
            "Processed 455/468 samples\n",
            "Processed 460/468 samples\n",
            "Processed 465/468 samples\n",
            "Processed 468/468 samples\n",
            "Processed 5/468 samples\n",
            "Processed 10/468 samples\n",
            "Processed 15/468 samples\n",
            "Processed 20/468 samples\n",
            "Processed 25/468 samples\n",
            "Processed 30/468 samples\n",
            "Processed 35/468 samples\n",
            "Processed 40/468 samples\n",
            "Processed 45/468 samples\n",
            "Processed 50/468 samples\n",
            "Processed 55/468 samples\n",
            "Processed 60/468 samples\n",
            "Processed 65/468 samples\n",
            "Processed 70/468 samples\n",
            "Processed 75/468 samples\n",
            "Processed 80/468 samples\n",
            "Processed 85/468 samples\n",
            "Processed 90/468 samples\n",
            "Processed 95/468 samples\n",
            "Processed 100/468 samples\n",
            "Processed 105/468 samples\n",
            "Processed 110/468 samples\n",
            "Processed 115/468 samples\n",
            "Processed 120/468 samples\n",
            "Processed 125/468 samples\n",
            "Processed 130/468 samples\n",
            "Processed 135/468 samples\n",
            "Processed 140/468 samples\n",
            "Processed 145/468 samples\n",
            "Processed 150/468 samples\n",
            "Processed 155/468 samples\n",
            "Processed 160/468 samples\n",
            "Processed 165/468 samples\n",
            "Processed 170/468 samples\n",
            "Processed 175/468 samples\n",
            "Processed 180/468 samples\n",
            "Processed 185/468 samples\n",
            "Processed 190/468 samples\n",
            "Processed 195/468 samples\n",
            "Processed 200/468 samples\n",
            "Processed 205/468 samples\n",
            "Processed 210/468 samples\n",
            "Processed 215/468 samples\n",
            "Processed 220/468 samples\n",
            "Processed 225/468 samples\n",
            "Processed 230/468 samples\n",
            "Processed 235/468 samples\n",
            "Processed 240/468 samples\n",
            "Processed 245/468 samples\n",
            "Processed 250/468 samples\n",
            "Processed 255/468 samples\n",
            "Processed 260/468 samples\n",
            "Processed 265/468 samples\n",
            "Processed 270/468 samples\n",
            "Processed 275/468 samples\n",
            "Processed 280/468 samples\n",
            "Processed 285/468 samples\n",
            "Processed 290/468 samples\n",
            "Processed 295/468 samples\n",
            "Processed 300/468 samples\n",
            "Processed 305/468 samples\n",
            "Processed 310/468 samples\n",
            "Processed 315/468 samples\n",
            "Processed 320/468 samples\n",
            "Processed 325/468 samples\n",
            "Processed 330/468 samples\n",
            "Processed 335/468 samples\n",
            "Processed 340/468 samples\n",
            "Processed 345/468 samples\n",
            "Processed 350/468 samples\n",
            "Processed 355/468 samples\n",
            "Processed 360/468 samples\n",
            "Processed 365/468 samples\n",
            "Processed 370/468 samples\n",
            "Processed 375/468 samples\n",
            "Processed 380/468 samples\n",
            "Processed 385/468 samples\n",
            "Processed 390/468 samples\n",
            "Processed 395/468 samples\n",
            "Processed 400/468 samples\n",
            "Processed 405/468 samples\n",
            "Processed 410/468 samples\n",
            "Processed 415/468 samples\n",
            "Processed 420/468 samples\n",
            "Processed 425/468 samples\n",
            "Processed 430/468 samples\n",
            "Processed 435/468 samples\n",
            "Processed 440/468 samples\n",
            "Processed 445/468 samples\n",
            "Processed 450/468 samples\n",
            "Processed 455/468 samples\n",
            "Processed 460/468 samples\n",
            "Processed 465/468 samples\n",
            "Processed 468/468 samples\n",
            "Processed 5/468 samples\n",
            "Processed 10/468 samples\n",
            "Processed 15/468 samples\n",
            "Processed 20/468 samples\n",
            "Processed 25/468 samples\n",
            "Processed 30/468 samples\n",
            "Processed 35/468 samples\n",
            "Processed 40/468 samples\n",
            "Processed 45/468 samples\n",
            "Processed 50/468 samples\n",
            "Processed 55/468 samples\n",
            "Processed 60/468 samples\n",
            "Processed 65/468 samples\n",
            "Processed 70/468 samples\n",
            "Processed 75/468 samples\n",
            "Processed 80/468 samples\n",
            "Processed 85/468 samples\n",
            "Processed 90/468 samples\n",
            "Processed 95/468 samples\n",
            "Processed 100/468 samples\n",
            "Processed 105/468 samples\n",
            "Processed 110/468 samples\n",
            "Processed 115/468 samples\n",
            "Processed 120/468 samples\n",
            "Processed 125/468 samples\n",
            "Processed 130/468 samples\n",
            "Processed 135/468 samples\n",
            "Processed 140/468 samples\n",
            "Processed 145/468 samples\n",
            "Processed 150/468 samples\n",
            "Processed 155/468 samples\n",
            "Processed 160/468 samples\n",
            "Processed 165/468 samples\n",
            "Processed 170/468 samples\n",
            "Processed 175/468 samples\n",
            "Processed 180/468 samples\n",
            "Processed 185/468 samples\n",
            "Processed 190/468 samples\n",
            "Processed 195/468 samples\n",
            "Processed 200/468 samples\n",
            "Processed 205/468 samples\n",
            "Processed 210/468 samples\n",
            "Processed 215/468 samples\n",
            "Processed 220/468 samples\n",
            "Processed 225/468 samples\n",
            "Processed 230/468 samples\n",
            "Processed 235/468 samples\n",
            "Processed 240/468 samples\n",
            "Processed 245/468 samples\n",
            "Processed 250/468 samples\n",
            "Processed 255/468 samples\n",
            "Processed 260/468 samples\n",
            "Processed 265/468 samples\n",
            "Processed 270/468 samples\n",
            "Processed 275/468 samples\n",
            "Processed 280/468 samples\n",
            "Processed 285/468 samples\n",
            "Processed 290/468 samples\n",
            "Processed 295/468 samples\n",
            "Processed 300/468 samples\n",
            "Processed 305/468 samples\n",
            "Processed 310/468 samples\n",
            "Processed 315/468 samples\n",
            "Processed 320/468 samples\n",
            "Processed 325/468 samples\n",
            "Processed 330/468 samples\n",
            "Processed 335/468 samples\n",
            "Processed 340/468 samples\n",
            "Processed 345/468 samples\n",
            "Processed 350/468 samples\n",
            "Processed 355/468 samples\n",
            "Processed 360/468 samples\n",
            "Processed 365/468 samples\n",
            "Processed 370/468 samples\n",
            "Processed 375/468 samples\n",
            "Processed 380/468 samples\n",
            "Processed 385/468 samples\n",
            "Processed 390/468 samples\n",
            "Processed 395/468 samples\n",
            "Processed 400/468 samples\n",
            "Processed 405/468 samples\n",
            "Processed 410/468 samples\n",
            "Processed 415/468 samples\n",
            "Processed 420/468 samples\n",
            "Processed 425/468 samples\n",
            "Processed 430/468 samples\n",
            "Processed 435/468 samples\n",
            "Processed 440/468 samples\n",
            "Processed 445/468 samples\n",
            "Processed 450/468 samples\n",
            "Processed 455/468 samples\n",
            "Processed 460/468 samples\n",
            "Processed 465/468 samples\n",
            "Processed 468/468 samples\n",
            "Processed 5/468 samples\n",
            "Processed 10/468 samples\n",
            "Processed 15/468 samples\n",
            "Processed 20/468 samples\n",
            "Processed 25/468 samples\n",
            "Processed 30/468 samples\n",
            "Processed 35/468 samples\n",
            "Processed 40/468 samples\n",
            "Processed 45/468 samples\n",
            "Processed 50/468 samples\n",
            "Processed 55/468 samples\n",
            "Processed 60/468 samples\n",
            "Processed 65/468 samples\n",
            "Processed 70/468 samples\n",
            "Processed 75/468 samples\n",
            "Processed 80/468 samples\n",
            "Processed 85/468 samples\n",
            "Processed 90/468 samples\n",
            "Processed 95/468 samples\n",
            "Processed 100/468 samples\n",
            "Processed 105/468 samples\n",
            "Processed 110/468 samples\n",
            "Processed 115/468 samples\n",
            "Processed 120/468 samples\n",
            "Processed 125/468 samples\n",
            "Processed 130/468 samples\n",
            "Processed 135/468 samples\n",
            "Processed 140/468 samples\n",
            "Processed 145/468 samples\n",
            "Processed 150/468 samples\n",
            "Processed 155/468 samples\n",
            "Processed 160/468 samples\n",
            "Processed 165/468 samples\n",
            "Processed 170/468 samples\n",
            "Processed 175/468 samples\n",
            "Processed 180/468 samples\n",
            "Processed 185/468 samples\n",
            "Processed 190/468 samples\n",
            "Processed 195/468 samples\n",
            "Processed 200/468 samples\n",
            "Processed 205/468 samples\n",
            "Processed 210/468 samples\n",
            "Processed 215/468 samples\n",
            "Processed 220/468 samples\n",
            "Processed 225/468 samples\n",
            "Processed 230/468 samples\n",
            "Processed 235/468 samples\n",
            "Processed 240/468 samples\n",
            "Processed 245/468 samples\n",
            "Processed 250/468 samples\n",
            "Processed 255/468 samples\n",
            "Processed 260/468 samples\n",
            "Processed 265/468 samples\n",
            "Processed 270/468 samples\n",
            "Processed 275/468 samples\n",
            "Processed 280/468 samples\n",
            "Processed 285/468 samples\n",
            "Processed 290/468 samples\n",
            "Processed 295/468 samples\n",
            "Processed 300/468 samples\n",
            "Processed 305/468 samples\n",
            "Processed 310/468 samples\n",
            "Processed 315/468 samples\n",
            "Processed 320/468 samples\n",
            "Processed 325/468 samples\n",
            "Processed 330/468 samples\n",
            "Processed 335/468 samples\n",
            "Processed 340/468 samples\n",
            "Processed 345/468 samples\n",
            "Processed 350/468 samples\n",
            "Processed 355/468 samples\n",
            "Processed 360/468 samples\n",
            "Processed 365/468 samples\n",
            "Processed 370/468 samples\n",
            "Processed 375/468 samples\n",
            "Processed 380/468 samples\n",
            "Processed 385/468 samples\n",
            "Processed 390/468 samples\n",
            "Processed 395/468 samples\n",
            "Processed 400/468 samples\n",
            "Processed 405/468 samples\n",
            "Processed 410/468 samples\n",
            "Processed 415/468 samples\n",
            "Processed 420/468 samples\n",
            "Processed 425/468 samples\n",
            "Processed 430/468 samples\n",
            "Processed 435/468 samples\n",
            "Processed 440/468 samples\n",
            "Processed 445/468 samples\n",
            "Processed 450/468 samples\n",
            "Processed 455/468 samples\n",
            "Processed 460/468 samples\n",
            "Processed 465/468 samples\n",
            "Processed 468/468 samples\n",
            "Processed 5/468 samples\n",
            "Processed 10/468 samples\n",
            "Processed 15/468 samples\n",
            "Processed 20/468 samples\n",
            "Processed 25/468 samples\n",
            "Processed 30/468 samples\n",
            "Processed 35/468 samples\n",
            "Processed 40/468 samples\n",
            "Processed 45/468 samples\n",
            "Processed 50/468 samples\n",
            "Processed 55/468 samples\n",
            "Processed 60/468 samples\n",
            "Processed 65/468 samples\n",
            "Processed 70/468 samples\n",
            "Processed 75/468 samples\n",
            "Processed 80/468 samples\n",
            "Processed 85/468 samples\n",
            "Processed 90/468 samples\n",
            "Processed 95/468 samples\n",
            "Processed 100/468 samples\n",
            "Processed 105/468 samples\n",
            "Processed 110/468 samples\n",
            "Processed 115/468 samples\n",
            "Processed 120/468 samples\n",
            "Processed 125/468 samples\n",
            "Processed 130/468 samples\n",
            "Processed 135/468 samples\n",
            "Processed 140/468 samples\n",
            "Processed 145/468 samples\n",
            "Processed 150/468 samples\n",
            "Processed 155/468 samples\n",
            "Processed 160/468 samples\n",
            "Processed 165/468 samples\n",
            "Processed 170/468 samples\n",
            "Processed 175/468 samples\n",
            "Processed 180/468 samples\n",
            "Processed 185/468 samples\n",
            "Processed 190/468 samples\n",
            "Processed 195/468 samples\n",
            "Processed 200/468 samples\n",
            "Processed 205/468 samples\n",
            "Processed 210/468 samples\n",
            "Processed 215/468 samples\n",
            "Processed 220/468 samples\n",
            "Processed 225/468 samples\n",
            "Processed 230/468 samples\n",
            "Processed 235/468 samples\n",
            "Processed 240/468 samples\n",
            "Processed 245/468 samples\n",
            "Processed 250/468 samples\n",
            "Processed 255/468 samples\n",
            "Processed 260/468 samples\n",
            "Processed 265/468 samples\n",
            "Processed 270/468 samples\n",
            "Processed 275/468 samples\n",
            "Processed 280/468 samples\n",
            "Processed 285/468 samples\n",
            "Processed 290/468 samples\n",
            "Processed 295/468 samples\n",
            "Processed 300/468 samples\n",
            "Processed 305/468 samples\n",
            "Processed 310/468 samples\n",
            "Processed 315/468 samples\n",
            "Processed 320/468 samples\n",
            "Processed 325/468 samples\n",
            "Processed 330/468 samples\n",
            "Processed 335/468 samples\n",
            "Processed 340/468 samples\n",
            "Processed 345/468 samples\n",
            "Processed 350/468 samples\n",
            "Processed 355/468 samples\n",
            "Processed 360/468 samples\n",
            "Processed 365/468 samples\n",
            "Processed 370/468 samples\n",
            "Processed 375/468 samples\n",
            "Processed 380/468 samples\n",
            "Processed 385/468 samples\n",
            "Processed 390/468 samples\n",
            "Processed 395/468 samples\n",
            "Processed 400/468 samples\n",
            "Processed 405/468 samples\n",
            "Processed 410/468 samples\n",
            "Processed 415/468 samples\n",
            "Processed 420/468 samples\n",
            "Processed 425/468 samples\n",
            "Processed 430/468 samples\n",
            "Processed 435/468 samples\n",
            "Processed 440/468 samples\n",
            "Processed 445/468 samples\n",
            "Processed 450/468 samples\n",
            "Processed 455/468 samples\n",
            "Processed 460/468 samples\n",
            "Processed 465/468 samples\n",
            "Processed 468/468 samples\n",
            "Processed 5/468 samples\n",
            "Processed 10/468 samples\n",
            "Processed 15/468 samples\n",
            "Processed 20/468 samples\n",
            "Processed 25/468 samples\n",
            "Processed 30/468 samples\n",
            "Processed 35/468 samples\n",
            "Processed 40/468 samples\n",
            "Processed 45/468 samples\n",
            "Processed 50/468 samples\n",
            "Processed 55/468 samples\n",
            "Processed 60/468 samples\n",
            "Processed 65/468 samples\n",
            "Processed 70/468 samples\n",
            "Processed 75/468 samples\n",
            "Processed 80/468 samples\n",
            "Processed 85/468 samples\n",
            "Processed 90/468 samples\n",
            "Processed 95/468 samples\n",
            "Processed 100/468 samples\n",
            "Processed 105/468 samples\n",
            "Processed 110/468 samples\n",
            "Processed 115/468 samples\n",
            "Processed 120/468 samples\n",
            "Processed 125/468 samples\n",
            "Processed 130/468 samples\n",
            "Processed 135/468 samples\n",
            "Processed 140/468 samples\n",
            "Processed 145/468 samples\n",
            "Processed 150/468 samples\n",
            "Processed 155/468 samples\n",
            "Processed 160/468 samples\n",
            "Processed 165/468 samples\n",
            "Processed 170/468 samples\n",
            "Processed 175/468 samples\n",
            "Processed 180/468 samples\n",
            "Processed 185/468 samples\n",
            "Processed 190/468 samples\n",
            "Processed 195/468 samples\n",
            "Processed 200/468 samples\n",
            "Processed 205/468 samples\n",
            "Processed 210/468 samples\n",
            "Processed 215/468 samples\n",
            "Processed 220/468 samples\n",
            "Processed 225/468 samples\n",
            "Processed 230/468 samples\n",
            "Processed 235/468 samples\n",
            "Processed 240/468 samples\n",
            "Processed 245/468 samples\n",
            "Processed 250/468 samples\n",
            "Processed 255/468 samples\n",
            "Processed 260/468 samples\n",
            "Processed 265/468 samples\n",
            "Processed 270/468 samples\n",
            "Processed 275/468 samples\n",
            "Processed 280/468 samples\n",
            "Processed 285/468 samples\n",
            "Processed 290/468 samples\n",
            "Processed 295/468 samples\n",
            "Processed 300/468 samples\n",
            "Processed 305/468 samples\n",
            "Processed 310/468 samples\n",
            "Processed 315/468 samples\n",
            "Processed 320/468 samples\n",
            "Processed 325/468 samples\n",
            "Processed 330/468 samples\n",
            "Processed 335/468 samples\n",
            "Processed 340/468 samples\n",
            "Processed 345/468 samples\n",
            "Processed 350/468 samples\n",
            "Processed 355/468 samples\n",
            "Processed 360/468 samples\n",
            "Processed 365/468 samples\n",
            "Processed 370/468 samples\n",
            "Processed 375/468 samples\n",
            "Processed 380/468 samples\n",
            "Processed 385/468 samples\n",
            "Processed 390/468 samples\n",
            "Processed 395/468 samples\n",
            "Processed 400/468 samples\n",
            "Processed 405/468 samples\n",
            "Processed 410/468 samples\n",
            "Processed 415/468 samples\n",
            "Processed 420/468 samples\n",
            "Processed 425/468 samples\n",
            "Processed 430/468 samples\n",
            "Processed 435/468 samples\n",
            "Processed 440/468 samples\n",
            "Processed 445/468 samples\n",
            "Processed 450/468 samples\n",
            "Processed 455/468 samples\n",
            "Processed 460/468 samples\n",
            "Processed 465/468 samples\n",
            "Processed 468/468 samples\n",
            "Processed 5/468 samples\n",
            "Processed 10/468 samples\n",
            "Processed 15/468 samples\n",
            "Processed 20/468 samples\n",
            "Processed 25/468 samples\n",
            "Processed 30/468 samples\n",
            "Processed 35/468 samples\n",
            "Processed 40/468 samples\n",
            "Processed 45/468 samples\n",
            "Processed 50/468 samples\n",
            "Processed 55/468 samples\n",
            "Processed 60/468 samples\n",
            "Processed 65/468 samples\n",
            "Processed 70/468 samples\n",
            "Processed 75/468 samples\n",
            "Processed 80/468 samples\n",
            "Processed 85/468 samples\n",
            "Processed 90/468 samples\n",
            "Processed 95/468 samples\n",
            "Processed 100/468 samples\n",
            "Processed 105/468 samples\n",
            "Processed 110/468 samples\n",
            "Processed 115/468 samples\n",
            "Processed 120/468 samples\n",
            "Processed 125/468 samples\n",
            "Processed 130/468 samples\n",
            "Processed 135/468 samples\n",
            "Processed 140/468 samples\n",
            "Processed 145/468 samples\n",
            "Processed 150/468 samples\n",
            "Processed 155/468 samples\n",
            "Processed 160/468 samples\n",
            "Processed 165/468 samples\n",
            "Processed 170/468 samples\n",
            "Processed 175/468 samples\n",
            "Processed 180/468 samples\n",
            "Processed 185/468 samples\n",
            "Processed 190/468 samples\n",
            "Processed 195/468 samples\n",
            "Processed 200/468 samples\n",
            "Processed 205/468 samples\n",
            "Processed 210/468 samples\n",
            "Processed 215/468 samples\n",
            "Processed 220/468 samples\n",
            "Processed 225/468 samples\n",
            "Processed 230/468 samples\n",
            "Processed 235/468 samples\n",
            "Processed 240/468 samples\n",
            "Processed 245/468 samples\n",
            "Processed 250/468 samples\n",
            "Processed 255/468 samples\n",
            "Processed 260/468 samples\n",
            "Processed 265/468 samples\n",
            "Processed 270/468 samples\n",
            "Processed 275/468 samples\n",
            "Processed 280/468 samples\n",
            "Processed 285/468 samples\n",
            "Processed 290/468 samples\n",
            "Processed 295/468 samples\n",
            "Processed 300/468 samples\n",
            "Processed 305/468 samples\n",
            "Processed 310/468 samples\n",
            "Processed 315/468 samples\n",
            "Processed 320/468 samples\n",
            "Processed 325/468 samples\n",
            "Processed 330/468 samples\n",
            "Processed 335/468 samples\n",
            "Processed 340/468 samples\n",
            "Processed 345/468 samples\n",
            "Processed 350/468 samples\n",
            "Processed 355/468 samples\n",
            "Processed 360/468 samples\n",
            "Processed 365/468 samples\n",
            "Processed 370/468 samples\n",
            "Processed 375/468 samples\n",
            "Processed 380/468 samples\n",
            "Processed 385/468 samples\n",
            "Processed 390/468 samples\n",
            "Processed 395/468 samples\n",
            "Processed 400/468 samples\n",
            "Processed 405/468 samples\n",
            "Processed 410/468 samples\n",
            "Processed 415/468 samples\n",
            "Processed 420/468 samples\n",
            "Processed 425/468 samples\n",
            "Processed 430/468 samples\n",
            "Processed 435/468 samples\n",
            "Processed 440/468 samples\n",
            "Processed 445/468 samples\n",
            "Processed 450/468 samples\n",
            "Processed 455/468 samples\n",
            "Processed 460/468 samples\n",
            "Processed 465/468 samples\n",
            "Processed 468/468 samples\n",
            "Processed 5/468 samples\n",
            "Processed 10/468 samples\n",
            "Processed 15/468 samples\n",
            "Processed 20/468 samples\n",
            "Processed 25/468 samples\n",
            "Processed 30/468 samples\n",
            "Processed 35/468 samples\n",
            "Processed 40/468 samples\n",
            "Processed 45/468 samples\n",
            "Processed 50/468 samples\n",
            "Processed 55/468 samples\n",
            "Processed 60/468 samples\n",
            "Processed 65/468 samples\n",
            "Processed 70/468 samples\n",
            "Processed 75/468 samples\n",
            "Processed 80/468 samples\n",
            "Processed 85/468 samples\n",
            "Processed 90/468 samples\n",
            "Processed 95/468 samples\n",
            "Processed 100/468 samples\n",
            "Processed 105/468 samples\n",
            "Processed 110/468 samples\n",
            "Processed 115/468 samples\n",
            "Processed 120/468 samples\n",
            "Processed 125/468 samples\n",
            "Processed 130/468 samples\n",
            "Processed 135/468 samples\n",
            "Processed 140/468 samples\n",
            "Processed 145/468 samples\n",
            "Processed 150/468 samples\n",
            "Processed 155/468 samples\n",
            "Processed 160/468 samples\n",
            "Processed 165/468 samples\n",
            "Processed 170/468 samples\n",
            "Processed 175/468 samples\n",
            "Processed 180/468 samples\n",
            "Processed 185/468 samples\n",
            "Processed 190/468 samples\n",
            "Processed 195/468 samples\n",
            "Processed 200/468 samples\n",
            "Processed 205/468 samples\n",
            "Processed 210/468 samples\n",
            "Processed 215/468 samples\n",
            "Processed 220/468 samples\n",
            "Processed 225/468 samples\n",
            "Processed 230/468 samples\n",
            "Processed 235/468 samples\n",
            "Processed 240/468 samples\n",
            "Processed 245/468 samples\n",
            "Processed 250/468 samples\n",
            "Processed 255/468 samples\n",
            "Processed 260/468 samples\n",
            "Processed 265/468 samples\n",
            "Processed 270/468 samples\n",
            "Processed 275/468 samples\n",
            "Processed 280/468 samples\n",
            "Processed 285/468 samples\n",
            "Processed 290/468 samples\n",
            "Processed 295/468 samples\n",
            "Processed 300/468 samples\n",
            "Processed 305/468 samples\n",
            "Processed 310/468 samples\n",
            "Processed 315/468 samples\n",
            "Processed 320/468 samples\n",
            "Processed 325/468 samples\n",
            "Processed 330/468 samples\n",
            "Processed 335/468 samples\n",
            "Processed 340/468 samples\n",
            "Processed 345/468 samples\n",
            "Processed 350/468 samples\n",
            "Processed 355/468 samples\n",
            "Processed 360/468 samples\n",
            "Processed 365/468 samples\n",
            "Processed 370/468 samples\n",
            "Processed 375/468 samples\n",
            "Processed 380/468 samples\n",
            "Processed 385/468 samples\n",
            "Processed 390/468 samples\n",
            "Processed 395/468 samples\n",
            "Processed 400/468 samples\n",
            "Processed 405/468 samples\n",
            "Processed 410/468 samples\n",
            "Processed 415/468 samples\n",
            "Processed 420/468 samples\n",
            "Processed 425/468 samples\n",
            "Processed 430/468 samples\n",
            "Processed 435/468 samples\n",
            "Processed 440/468 samples\n",
            "Processed 445/468 samples\n",
            "Processed 450/468 samples\n",
            "Processed 455/468 samples\n",
            "Processed 460/468 samples\n",
            "Processed 465/468 samples\n",
            "Processed 468/468 samples\n",
            "Processed 5/468 samples\n",
            "Processed 10/468 samples\n",
            "Processed 15/468 samples\n",
            "Processed 20/468 samples\n",
            "Processed 25/468 samples\n",
            "Processed 30/468 samples\n",
            "Processed 35/468 samples\n",
            "Processed 40/468 samples\n",
            "Processed 45/468 samples\n",
            "Processed 50/468 samples\n",
            "Processed 55/468 samples\n",
            "Processed 60/468 samples\n",
            "Processed 65/468 samples\n",
            "Processed 70/468 samples\n",
            "Processed 75/468 samples\n",
            "Processed 80/468 samples\n",
            "Processed 85/468 samples\n",
            "Processed 90/468 samples\n",
            "Processed 95/468 samples\n",
            "Processed 100/468 samples\n",
            "Processed 105/468 samples\n",
            "Processed 110/468 samples\n",
            "Processed 115/468 samples\n",
            "Processed 120/468 samples\n",
            "Processed 125/468 samples\n",
            "Processed 130/468 samples\n",
            "Processed 135/468 samples\n",
            "Processed 140/468 samples\n",
            "Processed 145/468 samples\n",
            "Processed 150/468 samples\n",
            "Processed 155/468 samples\n",
            "Processed 160/468 samples\n",
            "Processed 165/468 samples\n",
            "Processed 170/468 samples\n",
            "Processed 175/468 samples\n",
            "Processed 180/468 samples\n",
            "Processed 185/468 samples\n",
            "Processed 190/468 samples\n",
            "Processed 195/468 samples\n",
            "Processed 200/468 samples\n",
            "Processed 205/468 samples\n",
            "Processed 210/468 samples\n",
            "Processed 215/468 samples\n",
            "Processed 220/468 samples\n",
            "Processed 225/468 samples\n",
            "Processed 230/468 samples\n",
            "Processed 235/468 samples\n",
            "Processed 240/468 samples\n",
            "Processed 245/468 samples\n",
            "Processed 250/468 samples\n",
            "Processed 255/468 samples\n",
            "Processed 260/468 samples\n",
            "Processed 265/468 samples\n",
            "Processed 270/468 samples\n",
            "Processed 275/468 samples\n",
            "Processed 280/468 samples\n",
            "Processed 285/468 samples\n",
            "Processed 290/468 samples\n",
            "Processed 295/468 samples\n",
            "Processed 300/468 samples\n",
            "Processed 305/468 samples\n",
            "Processed 310/468 samples\n",
            "Processed 315/468 samples\n",
            "Processed 320/468 samples\n",
            "Processed 325/468 samples\n",
            "Processed 330/468 samples\n",
            "Processed 335/468 samples\n",
            "Processed 340/468 samples\n",
            "Processed 345/468 samples\n",
            "Processed 350/468 samples\n",
            "Processed 355/468 samples\n",
            "Processed 360/468 samples\n",
            "Processed 365/468 samples\n",
            "Processed 370/468 samples\n",
            "Processed 375/468 samples\n",
            "Processed 380/468 samples\n",
            "Processed 385/468 samples\n",
            "Processed 390/468 samples\n",
            "Processed 395/468 samples\n",
            "Processed 400/468 samples\n",
            "Processed 405/468 samples\n",
            "Processed 410/468 samples\n",
            "Processed 415/468 samples\n",
            "Processed 420/468 samples\n",
            "Processed 425/468 samples\n",
            "Processed 430/468 samples\n",
            "Processed 435/468 samples\n",
            "Processed 440/468 samples\n",
            "Processed 445/468 samples\n",
            "Processed 450/468 samples\n",
            "Processed 455/468 samples\n",
            "Processed 460/468 samples\n",
            "Processed 465/468 samples\n",
            "Processed 468/468 samples\n",
            "Processed 5/468 samples\n",
            "Processed 10/468 samples\n",
            "Processed 15/468 samples\n",
            "Processed 20/468 samples\n",
            "Processed 25/468 samples\n",
            "Processed 30/468 samples\n",
            "Processed 35/468 samples\n",
            "Processed 40/468 samples\n",
            "Processed 45/468 samples\n",
            "Processed 50/468 samples\n",
            "Processed 55/468 samples\n",
            "Processed 60/468 samples\n",
            "Processed 65/468 samples\n",
            "Processed 70/468 samples\n",
            "Processed 75/468 samples\n",
            "Processed 80/468 samples\n",
            "Processed 85/468 samples\n",
            "Processed 90/468 samples\n",
            "Processed 95/468 samples\n",
            "Processed 100/468 samples\n",
            "Processed 105/468 samples\n",
            "Processed 110/468 samples\n",
            "Processed 115/468 samples\n",
            "Processed 120/468 samples\n",
            "Processed 125/468 samples\n",
            "Processed 130/468 samples\n",
            "Processed 135/468 samples\n",
            "Processed 140/468 samples\n",
            "Processed 145/468 samples\n",
            "Processed 150/468 samples\n",
            "Processed 155/468 samples\n",
            "Processed 160/468 samples\n",
            "Processed 165/468 samples\n",
            "Processed 170/468 samples\n",
            "Processed 175/468 samples\n",
            "Processed 180/468 samples\n",
            "Processed 185/468 samples\n",
            "Processed 190/468 samples\n",
            "Processed 195/468 samples\n",
            "Processed 200/468 samples\n",
            "Processed 205/468 samples\n",
            "Processed 210/468 samples\n",
            "Processed 215/468 samples\n",
            "Processed 220/468 samples\n",
            "Processed 225/468 samples\n",
            "Processed 230/468 samples\n",
            "Processed 235/468 samples\n",
            "Processed 240/468 samples\n",
            "Processed 245/468 samples\n",
            "Processed 250/468 samples\n",
            "Processed 255/468 samples\n",
            "Processed 260/468 samples\n",
            "Processed 265/468 samples\n",
            "Processed 270/468 samples\n",
            "Processed 275/468 samples\n",
            "Processed 280/468 samples\n",
            "Processed 285/468 samples\n",
            "Processed 290/468 samples\n",
            "Processed 295/468 samples\n",
            "Processed 300/468 samples\n",
            "Processed 305/468 samples\n",
            "Processed 310/468 samples\n",
            "Processed 315/468 samples\n",
            "Processed 320/468 samples\n",
            "Processed 325/468 samples\n",
            "Processed 330/468 samples\n",
            "Processed 335/468 samples\n",
            "Processed 340/468 samples\n",
            "Processed 345/468 samples\n",
            "Processed 350/468 samples\n",
            "Processed 355/468 samples\n",
            "Processed 360/468 samples\n",
            "Processed 365/468 samples\n",
            "Processed 370/468 samples\n",
            "Processed 375/468 samples\n",
            "Processed 380/468 samples\n",
            "Processed 385/468 samples\n",
            "Processed 390/468 samples\n",
            "Processed 395/468 samples\n",
            "Processed 400/468 samples\n",
            "Processed 405/468 samples\n",
            "Processed 410/468 samples\n",
            "Processed 415/468 samples\n",
            "Processed 420/468 samples\n",
            "Processed 425/468 samples\n",
            "Processed 430/468 samples\n",
            "Processed 435/468 samples\n",
            "Processed 440/468 samples\n",
            "Processed 445/468 samples\n",
            "Processed 450/468 samples\n",
            "Processed 455/468 samples\n",
            "Processed 460/468 samples\n",
            "Processed 465/468 samples\n",
            "Processed 468/468 samples\n",
            "Processed 5/468 samples\n",
            "Processed 10/468 samples\n",
            "Processed 15/468 samples\n",
            "Processed 20/468 samples\n",
            "Processed 25/468 samples\n",
            "Processed 30/468 samples\n",
            "Processed 35/468 samples\n",
            "Processed 40/468 samples\n",
            "Processed 45/468 samples\n",
            "Processed 50/468 samples\n",
            "Processed 55/468 samples\n",
            "Processed 60/468 samples\n",
            "Processed 65/468 samples\n",
            "Processed 70/468 samples\n",
            "Processed 75/468 samples\n",
            "Processed 80/468 samples\n",
            "Processed 85/468 samples\n",
            "Processed 90/468 samples\n",
            "Processed 95/468 samples\n",
            "Processed 100/468 samples\n",
            "Processed 105/468 samples\n",
            "Processed 110/468 samples\n",
            "Processed 115/468 samples\n",
            "Processed 120/468 samples\n",
            "Processed 125/468 samples\n",
            "Processed 130/468 samples\n",
            "Processed 135/468 samples\n",
            "Processed 140/468 samples\n",
            "Processed 145/468 samples\n",
            "Processed 150/468 samples\n",
            "Processed 155/468 samples\n",
            "Processed 160/468 samples\n",
            "Processed 165/468 samples\n",
            "Processed 170/468 samples\n",
            "Processed 175/468 samples\n",
            "Processed 180/468 samples\n",
            "Processed 185/468 samples\n",
            "Processed 190/468 samples\n",
            "Processed 195/468 samples\n",
            "Processed 200/468 samples\n",
            "Processed 205/468 samples\n",
            "Processed 210/468 samples\n",
            "Processed 215/468 samples\n",
            "Processed 220/468 samples\n",
            "Processed 225/468 samples\n",
            "Processed 230/468 samples\n",
            "Processed 235/468 samples\n",
            "Processed 240/468 samples\n",
            "Processed 245/468 samples\n",
            "Processed 250/468 samples\n",
            "Processed 255/468 samples\n",
            "Processed 260/468 samples\n",
            "Processed 265/468 samples\n",
            "Processed 270/468 samples\n",
            "Processed 275/468 samples\n",
            "Processed 280/468 samples\n",
            "Processed 285/468 samples\n",
            "Processed 290/468 samples\n",
            "Processed 295/468 samples\n",
            "Processed 300/468 samples\n",
            "Processed 305/468 samples\n",
            "Processed 310/468 samples\n",
            "Processed 315/468 samples\n",
            "Processed 320/468 samples\n",
            "Processed 325/468 samples\n",
            "Processed 330/468 samples\n",
            "Processed 335/468 samples\n",
            "Processed 340/468 samples\n",
            "Processed 345/468 samples\n",
            "Processed 350/468 samples\n",
            "Processed 355/468 samples\n",
            "Processed 360/468 samples\n",
            "Processed 365/468 samples\n",
            "Processed 370/468 samples\n",
            "Processed 375/468 samples\n",
            "Processed 380/468 samples\n",
            "Processed 385/468 samples\n",
            "Processed 390/468 samples\n",
            "Processed 395/468 samples\n",
            "Processed 400/468 samples\n",
            "Processed 405/468 samples\n",
            "Processed 410/468 samples\n",
            "Processed 415/468 samples\n",
            "Processed 420/468 samples\n",
            "Processed 425/468 samples\n",
            "Processed 430/468 samples\n",
            "Processed 435/468 samples\n",
            "Processed 440/468 samples\n",
            "Processed 445/468 samples\n",
            "Processed 450/468 samples\n",
            "Processed 455/468 samples\n",
            "Processed 460/468 samples\n",
            "Processed 465/468 samples\n",
            "Processed 468/468 samples\n",
            "Processed 5/468 samples\n",
            "Processed 10/468 samples\n",
            "Processed 15/468 samples\n",
            "Processed 20/468 samples\n",
            "Processed 25/468 samples\n",
            "Processed 30/468 samples\n",
            "Processed 35/468 samples\n",
            "Processed 40/468 samples\n",
            "Processed 45/468 samples\n",
            "Processed 50/468 samples\n",
            "Processed 55/468 samples\n",
            "Processed 60/468 samples\n",
            "Processed 65/468 samples\n",
            "Processed 70/468 samples\n",
            "Processed 75/468 samples\n",
            "Processed 80/468 samples\n",
            "Processed 85/468 samples\n",
            "Processed 90/468 samples\n",
            "Processed 95/468 samples\n",
            "Processed 100/468 samples\n",
            "Processed 105/468 samples\n",
            "Processed 110/468 samples\n",
            "Processed 115/468 samples\n",
            "Processed 120/468 samples\n",
            "Processed 125/468 samples\n",
            "Processed 130/468 samples\n",
            "Processed 135/468 samples\n",
            "Processed 140/468 samples\n",
            "Processed 145/468 samples\n",
            "Processed 150/468 samples\n",
            "Processed 155/468 samples\n",
            "Processed 160/468 samples\n",
            "Processed 165/468 samples\n",
            "Processed 170/468 samples\n",
            "Processed 175/468 samples\n",
            "Processed 180/468 samples\n",
            "Processed 185/468 samples\n",
            "Processed 190/468 samples\n",
            "Processed 195/468 samples\n",
            "Processed 200/468 samples\n",
            "Processed 205/468 samples\n",
            "Processed 210/468 samples\n",
            "Processed 215/468 samples\n",
            "Processed 220/468 samples\n",
            "Processed 225/468 samples\n",
            "Processed 230/468 samples\n",
            "Processed 235/468 samples\n",
            "Processed 240/468 samples\n",
            "Processed 245/468 samples\n",
            "Processed 250/468 samples\n",
            "Processed 255/468 samples\n",
            "Processed 260/468 samples\n",
            "Processed 265/468 samples\n",
            "Processed 270/468 samples\n",
            "Processed 275/468 samples\n",
            "Processed 280/468 samples\n",
            "Processed 285/468 samples\n",
            "Processed 290/468 samples\n",
            "Processed 295/468 samples\n",
            "Processed 300/468 samples\n",
            "Processed 305/468 samples\n",
            "Processed 310/468 samples\n",
            "Processed 315/468 samples\n",
            "Processed 320/468 samples\n",
            "Processed 325/468 samples\n",
            "Processed 330/468 samples\n",
            "Processed 335/468 samples\n",
            "Processed 340/468 samples\n",
            "Processed 345/468 samples\n",
            "Processed 350/468 samples\n",
            "Processed 355/468 samples\n",
            "Processed 360/468 samples\n",
            "Processed 365/468 samples\n",
            "Processed 370/468 samples\n",
            "Processed 375/468 samples\n",
            "Processed 380/468 samples\n",
            "Processed 385/468 samples\n",
            "Processed 390/468 samples\n",
            "Processed 395/468 samples\n",
            "Processed 400/468 samples\n",
            "Processed 405/468 samples\n",
            "Processed 410/468 samples\n",
            "Processed 415/468 samples\n",
            "Processed 420/468 samples\n",
            "Processed 425/468 samples\n",
            "Processed 430/468 samples\n",
            "Processed 435/468 samples\n",
            "Processed 440/468 samples\n",
            "Processed 445/468 samples\n",
            "Processed 450/468 samples\n",
            "Processed 455/468 samples\n",
            "Processed 460/468 samples\n",
            "Processed 465/468 samples\n",
            "Processed 468/468 samples\n",
            "Processed 5/468 samples\n",
            "Processed 10/468 samples\n",
            "Processed 15/468 samples\n",
            "Processed 20/468 samples\n",
            "Processed 25/468 samples\n",
            "Processed 30/468 samples\n",
            "Processed 35/468 samples\n",
            "Processed 40/468 samples\n",
            "Processed 45/468 samples\n",
            "Processed 50/468 samples\n",
            "Processed 55/468 samples\n",
            "Processed 60/468 samples\n",
            "Processed 65/468 samples\n",
            "Processed 70/468 samples\n",
            "Processed 75/468 samples\n",
            "Processed 80/468 samples\n",
            "Processed 85/468 samples\n",
            "Processed 90/468 samples\n",
            "Processed 95/468 samples\n",
            "Processed 100/468 samples\n",
            "Processed 105/468 samples\n",
            "Processed 110/468 samples\n",
            "Processed 115/468 samples\n",
            "Processed 120/468 samples\n",
            "Processed 125/468 samples\n",
            "Processed 130/468 samples\n",
            "Processed 135/468 samples\n",
            "Processed 140/468 samples\n",
            "Processed 145/468 samples\n",
            "Processed 150/468 samples\n",
            "Processed 155/468 samples\n",
            "Processed 160/468 samples\n",
            "Processed 165/468 samples\n",
            "Processed 170/468 samples\n",
            "Processed 175/468 samples\n",
            "Processed 180/468 samples\n",
            "Processed 185/468 samples\n",
            "Processed 190/468 samples\n",
            "Processed 195/468 samples\n",
            "Processed 200/468 samples\n",
            "Processed 205/468 samples\n",
            "Processed 210/468 samples\n",
            "Processed 215/468 samples\n",
            "Processed 220/468 samples\n",
            "Processed 225/468 samples\n",
            "Processed 230/468 samples\n",
            "Processed 235/468 samples\n",
            "Processed 240/468 samples\n",
            "Processed 245/468 samples\n",
            "Processed 250/468 samples\n",
            "Processed 255/468 samples\n",
            "Processed 260/468 samples\n",
            "Processed 265/468 samples\n",
            "Processed 270/468 samples\n",
            "Processed 275/468 samples\n",
            "Processed 280/468 samples\n",
            "Processed 285/468 samples\n",
            "Processed 290/468 samples\n",
            "Processed 295/468 samples\n",
            "Processed 300/468 samples\n",
            "Processed 305/468 samples\n",
            "Processed 310/468 samples\n",
            "Processed 315/468 samples\n",
            "Processed 320/468 samples\n",
            "Processed 325/468 samples\n",
            "Processed 330/468 samples\n",
            "Processed 335/468 samples\n",
            "Processed 340/468 samples\n",
            "Processed 345/468 samples\n",
            "Processed 350/468 samples\n",
            "Processed 355/468 samples\n",
            "Processed 360/468 samples\n",
            "Processed 365/468 samples\n",
            "Processed 370/468 samples\n",
            "Processed 375/468 samples\n",
            "Processed 380/468 samples\n",
            "Processed 385/468 samples\n",
            "Processed 390/468 samples\n",
            "Processed 395/468 samples\n",
            "Processed 400/468 samples\n",
            "Processed 405/468 samples\n",
            "Processed 410/468 samples\n",
            "Processed 415/468 samples\n",
            "Processed 420/468 samples\n",
            "Processed 425/468 samples\n",
            "Processed 430/468 samples\n",
            "Processed 435/468 samples\n",
            "Processed 440/468 samples\n",
            "Processed 445/468 samples\n",
            "Processed 450/468 samples\n",
            "Processed 455/468 samples\n",
            "Processed 460/468 samples\n",
            "Processed 465/468 samples\n",
            "Processed 468/468 samples\n",
            "Processed 5/468 samples\n",
            "Processed 10/468 samples\n",
            "Processed 15/468 samples\n",
            "Processed 20/468 samples\n",
            "Processed 25/468 samples\n",
            "Processed 30/468 samples\n",
            "Processed 35/468 samples\n",
            "Processed 40/468 samples\n",
            "Processed 45/468 samples\n",
            "Processed 50/468 samples\n",
            "Processed 55/468 samples\n",
            "Processed 60/468 samples\n",
            "Processed 65/468 samples\n",
            "Processed 70/468 samples\n",
            "Processed 75/468 samples\n",
            "Processed 80/468 samples\n",
            "Processed 85/468 samples\n",
            "Processed 90/468 samples\n",
            "Processed 95/468 samples\n",
            "Processed 100/468 samples\n",
            "Processed 105/468 samples\n",
            "Processed 110/468 samples\n",
            "Processed 115/468 samples\n",
            "Processed 120/468 samples\n",
            "Processed 125/468 samples\n",
            "Processed 130/468 samples\n",
            "Processed 135/468 samples\n",
            "Processed 140/468 samples\n",
            "Processed 145/468 samples\n",
            "Processed 150/468 samples\n",
            "Processed 155/468 samples\n",
            "Processed 160/468 samples\n",
            "Processed 165/468 samples\n",
            "Processed 170/468 samples\n",
            "Processed 175/468 samples\n",
            "Processed 180/468 samples\n",
            "Processed 185/468 samples\n",
            "Processed 190/468 samples\n",
            "Processed 195/468 samples\n",
            "Processed 200/468 samples\n",
            "Processed 205/468 samples\n",
            "Processed 210/468 samples\n",
            "Processed 215/468 samples\n",
            "Processed 220/468 samples\n",
            "Processed 225/468 samples\n",
            "Processed 230/468 samples\n",
            "Processed 235/468 samples\n",
            "Processed 240/468 samples\n",
            "Processed 245/468 samples\n",
            "Processed 250/468 samples\n",
            "Processed 255/468 samples\n",
            "Processed 260/468 samples\n",
            "Processed 265/468 samples\n",
            "Processed 270/468 samples\n",
            "Processed 275/468 samples\n",
            "Processed 280/468 samples\n",
            "Processed 285/468 samples\n",
            "Processed 290/468 samples\n",
            "Processed 295/468 samples\n",
            "Processed 300/468 samples\n",
            "Processed 305/468 samples\n",
            "Processed 310/468 samples\n",
            "Processed 315/468 samples\n",
            "Processed 320/468 samples\n",
            "Processed 325/468 samples\n",
            "Processed 330/468 samples\n",
            "Processed 335/468 samples\n",
            "Processed 340/468 samples\n",
            "Processed 345/468 samples\n",
            "Processed 350/468 samples\n",
            "Processed 355/468 samples\n",
            "Processed 360/468 samples\n",
            "Processed 365/468 samples\n",
            "Processed 370/468 samples\n",
            "Processed 375/468 samples\n",
            "Processed 380/468 samples\n",
            "Processed 385/468 samples\n",
            "Processed 390/468 samples\n",
            "Processed 395/468 samples\n",
            "Processed 400/468 samples\n",
            "Processed 405/468 samples\n",
            "Processed 410/468 samples\n",
            "Processed 415/468 samples\n",
            "Processed 420/468 samples\n",
            "Processed 425/468 samples\n",
            "Processed 430/468 samples\n",
            "Processed 435/468 samples\n",
            "Processed 440/468 samples\n",
            "Processed 445/468 samples\n",
            "Processed 450/468 samples\n",
            "Processed 455/468 samples\n",
            "Processed 460/468 samples\n",
            "Processed 465/468 samples\n",
            "Processed 468/468 samples\n",
            "Processed 5/468 samples\n",
            "Processed 10/468 samples\n",
            "Processed 15/468 samples\n",
            "Processed 20/468 samples\n",
            "Processed 25/468 samples\n",
            "Processed 30/468 samples\n",
            "Processed 35/468 samples\n",
            "Processed 40/468 samples\n",
            "Processed 45/468 samples\n",
            "Processed 50/468 samples\n",
            "Processed 55/468 samples\n",
            "Processed 60/468 samples\n",
            "Processed 65/468 samples\n",
            "Processed 70/468 samples\n",
            "Processed 75/468 samples\n",
            "Processed 80/468 samples\n",
            "Processed 85/468 samples\n",
            "Processed 90/468 samples\n",
            "Processed 95/468 samples\n",
            "Processed 100/468 samples\n",
            "Processed 105/468 samples\n",
            "Processed 110/468 samples\n",
            "Processed 115/468 samples\n",
            "Processed 120/468 samples\n",
            "Processed 125/468 samples\n",
            "Processed 130/468 samples\n",
            "Processed 135/468 samples\n",
            "Processed 140/468 samples\n",
            "Processed 145/468 samples\n",
            "Processed 150/468 samples\n",
            "Processed 155/468 samples\n",
            "Processed 160/468 samples\n",
            "Processed 165/468 samples\n",
            "Processed 170/468 samples\n",
            "Processed 175/468 samples\n",
            "Processed 180/468 samples\n",
            "Processed 185/468 samples\n",
            "Processed 190/468 samples\n",
            "Processed 195/468 samples\n",
            "Processed 200/468 samples\n",
            "Processed 205/468 samples\n",
            "Processed 210/468 samples\n",
            "Processed 215/468 samples\n",
            "Processed 220/468 samples\n",
            "Processed 225/468 samples\n",
            "Processed 230/468 samples\n",
            "Processed 235/468 samples\n",
            "Processed 240/468 samples\n",
            "Processed 245/468 samples\n",
            "Processed 250/468 samples\n",
            "Processed 255/468 samples\n",
            "Processed 260/468 samples\n",
            "Processed 265/468 samples\n",
            "Processed 270/468 samples\n",
            "Processed 275/468 samples\n",
            "Processed 280/468 samples\n",
            "Processed 285/468 samples\n",
            "Processed 290/468 samples\n",
            "Processed 295/468 samples\n",
            "Processed 300/468 samples\n",
            "Processed 305/468 samples\n",
            "Processed 310/468 samples\n",
            "Processed 315/468 samples\n",
            "Processed 320/468 samples\n",
            "Processed 325/468 samples\n",
            "Processed 330/468 samples\n",
            "Processed 335/468 samples\n",
            "Processed 340/468 samples\n",
            "Processed 345/468 samples\n",
            "Processed 350/468 samples\n",
            "Processed 355/468 samples\n",
            "Processed 360/468 samples\n",
            "Processed 365/468 samples\n",
            "Processed 370/468 samples\n",
            "Processed 375/468 samples\n",
            "Processed 380/468 samples\n",
            "Processed 385/468 samples\n",
            "Processed 390/468 samples\n",
            "Processed 395/468 samples\n",
            "Processed 400/468 samples\n",
            "Processed 405/468 samples\n",
            "Processed 410/468 samples\n",
            "Processed 415/468 samples\n",
            "Processed 420/468 samples\n",
            "Processed 425/468 samples\n",
            "Processed 430/468 samples\n",
            "Processed 435/468 samples\n",
            "Processed 440/468 samples\n",
            "Processed 445/468 samples\n",
            "Processed 450/468 samples\n",
            "Processed 455/468 samples\n",
            "Processed 460/468 samples\n",
            "Processed 465/468 samples\n",
            "Processed 468/468 samples\n",
            "Processed 5/468 samples\n",
            "Processed 10/468 samples\n",
            "Processed 15/468 samples\n",
            "Processed 20/468 samples\n",
            "Processed 25/468 samples\n",
            "Processed 30/468 samples\n",
            "Processed 35/468 samples\n",
            "Processed 40/468 samples\n",
            "Processed 45/468 samples\n",
            "Processed 50/468 samples\n",
            "Processed 55/468 samples\n",
            "Processed 60/468 samples\n",
            "Processed 65/468 samples\n",
            "Processed 70/468 samples\n",
            "Processed 75/468 samples\n",
            "Processed 80/468 samples\n",
            "Processed 85/468 samples\n",
            "Processed 90/468 samples\n",
            "Processed 95/468 samples\n",
            "Processed 100/468 samples\n",
            "Processed 105/468 samples\n",
            "Processed 110/468 samples\n",
            "Processed 115/468 samples\n",
            "Processed 120/468 samples\n",
            "Processed 125/468 samples\n",
            "Processed 130/468 samples\n",
            "Processed 135/468 samples\n",
            "Processed 140/468 samples\n",
            "Processed 145/468 samples\n",
            "Processed 150/468 samples\n",
            "Processed 155/468 samples\n",
            "Processed 160/468 samples\n",
            "Processed 165/468 samples\n",
            "Processed 170/468 samples\n",
            "Processed 175/468 samples\n",
            "Processed 180/468 samples\n",
            "Processed 185/468 samples\n",
            "Processed 190/468 samples\n",
            "Processed 195/468 samples\n",
            "Processed 200/468 samples\n",
            "Processed 205/468 samples\n",
            "Processed 210/468 samples\n",
            "Processed 215/468 samples\n",
            "Processed 220/468 samples\n",
            "Processed 225/468 samples\n",
            "Processed 230/468 samples\n",
            "Processed 235/468 samples\n",
            "Processed 240/468 samples\n",
            "Processed 245/468 samples\n",
            "Processed 250/468 samples\n",
            "Processed 255/468 samples\n",
            "Processed 260/468 samples\n",
            "Processed 265/468 samples\n",
            "Processed 270/468 samples\n",
            "Processed 275/468 samples\n",
            "Processed 280/468 samples\n",
            "Processed 285/468 samples\n",
            "Processed 290/468 samples\n",
            "Processed 295/468 samples\n",
            "Processed 300/468 samples\n",
            "Processed 305/468 samples\n",
            "Processed 310/468 samples\n",
            "Processed 315/468 samples\n",
            "Processed 320/468 samples\n",
            "Processed 325/468 samples\n",
            "Processed 330/468 samples\n",
            "Processed 335/468 samples\n",
            "Processed 340/468 samples\n",
            "Processed 345/468 samples\n",
            "Processed 350/468 samples\n",
            "Processed 355/468 samples\n",
            "Processed 360/468 samples\n",
            "Processed 365/468 samples\n",
            "Processed 370/468 samples\n",
            "Processed 375/468 samples\n",
            "Processed 380/468 samples\n",
            "Processed 385/468 samples\n",
            "Processed 390/468 samples\n",
            "Processed 395/468 samples\n",
            "Processed 400/468 samples\n",
            "Processed 405/468 samples\n",
            "Processed 410/468 samples\n",
            "Processed 415/468 samples\n",
            "Processed 420/468 samples\n",
            "Processed 425/468 samples\n",
            "Processed 430/468 samples\n",
            "Processed 435/468 samples\n",
            "Processed 440/468 samples\n",
            "Processed 445/468 samples\n",
            "Processed 450/468 samples\n",
            "Processed 455/468 samples\n",
            "Processed 460/468 samples\n",
            "Processed 465/468 samples\n",
            "Processed 468/468 samples\n",
            "Processed 5/468 samples\n",
            "Processed 10/468 samples\n",
            "Processed 15/468 samples\n",
            "Processed 20/468 samples\n",
            "Processed 25/468 samples\n",
            "Processed 30/468 samples\n",
            "Processed 35/468 samples\n",
            "Processed 40/468 samples\n",
            "Processed 45/468 samples\n",
            "Processed 50/468 samples\n",
            "Processed 55/468 samples\n",
            "Processed 60/468 samples\n",
            "Processed 65/468 samples\n",
            "Processed 70/468 samples\n",
            "Processed 75/468 samples\n",
            "Processed 80/468 samples\n",
            "Processed 85/468 samples\n",
            "Processed 90/468 samples\n",
            "Processed 95/468 samples\n",
            "Processed 100/468 samples\n",
            "Processed 105/468 samples\n",
            "Processed 110/468 samples\n",
            "Processed 115/468 samples\n",
            "Processed 120/468 samples\n",
            "Processed 125/468 samples\n",
            "Processed 130/468 samples\n",
            "Processed 135/468 samples\n",
            "Processed 140/468 samples\n",
            "Processed 145/468 samples\n",
            "Processed 150/468 samples\n",
            "Processed 155/468 samples\n",
            "Processed 160/468 samples\n",
            "Processed 165/468 samples\n",
            "Processed 170/468 samples\n",
            "Processed 175/468 samples\n",
            "Processed 180/468 samples\n",
            "Processed 185/468 samples\n",
            "Processed 190/468 samples\n",
            "Processed 195/468 samples\n",
            "Processed 200/468 samples\n",
            "Processed 205/468 samples\n",
            "Processed 210/468 samples\n",
            "Processed 215/468 samples\n",
            "Processed 220/468 samples\n",
            "Processed 225/468 samples\n",
            "Processed 230/468 samples\n",
            "Processed 235/468 samples\n",
            "Processed 240/468 samples\n",
            "Processed 245/468 samples\n",
            "Processed 250/468 samples\n",
            "Processed 255/468 samples\n",
            "Processed 260/468 samples\n",
            "Processed 265/468 samples\n",
            "Processed 270/468 samples\n",
            "Processed 275/468 samples\n",
            "Processed 280/468 samples\n",
            "Processed 285/468 samples\n",
            "Processed 290/468 samples\n",
            "Processed 295/468 samples\n",
            "Processed 300/468 samples\n",
            "Processed 305/468 samples\n",
            "Processed 310/468 samples\n",
            "Processed 315/468 samples\n",
            "Processed 320/468 samples\n",
            "Processed 325/468 samples\n",
            "Processed 330/468 samples\n",
            "Processed 335/468 samples\n",
            "Processed 340/468 samples\n",
            "Processed 345/468 samples\n",
            "Processed 350/468 samples\n",
            "Processed 355/468 samples\n",
            "Processed 360/468 samples\n",
            "Processed 365/468 samples\n",
            "Processed 370/468 samples\n",
            "Processed 375/468 samples\n",
            "Processed 380/468 samples\n",
            "Processed 385/468 samples\n",
            "Processed 390/468 samples\n",
            "Processed 395/468 samples\n",
            "Processed 400/468 samples\n",
            "Processed 405/468 samples\n",
            "Processed 410/468 samples\n",
            "Processed 415/468 samples\n",
            "Processed 420/468 samples\n",
            "Processed 425/468 samples\n",
            "Processed 430/468 samples\n",
            "Processed 435/468 samples\n",
            "Processed 440/468 samples\n",
            "Processed 445/468 samples\n",
            "Processed 450/468 samples\n",
            "Processed 455/468 samples\n",
            "Processed 460/468 samples\n",
            "Processed 465/468 samples\n",
            "Processed 468/468 samples\n",
            "Processed 5/468 samples\n",
            "Processed 10/468 samples\n",
            "Processed 15/468 samples\n",
            "Processed 20/468 samples\n",
            "Processed 25/468 samples\n",
            "Processed 30/468 samples\n",
            "Processed 35/468 samples\n",
            "Processed 40/468 samples\n",
            "Processed 45/468 samples\n",
            "Processed 50/468 samples\n",
            "Processed 55/468 samples\n",
            "Processed 60/468 samples\n",
            "Processed 65/468 samples\n",
            "Processed 70/468 samples\n",
            "Processed 75/468 samples\n",
            "Processed 80/468 samples\n",
            "Processed 85/468 samples\n",
            "Processed 90/468 samples\n",
            "Processed 95/468 samples\n",
            "Processed 100/468 samples\n",
            "Processed 105/468 samples\n",
            "Processed 110/468 samples\n",
            "Processed 115/468 samples\n",
            "Processed 120/468 samples\n",
            "Processed 125/468 samples\n",
            "Processed 130/468 samples\n",
            "Processed 135/468 samples\n",
            "Processed 140/468 samples\n",
            "Processed 145/468 samples\n",
            "Processed 150/468 samples\n",
            "Processed 155/468 samples\n",
            "Processed 160/468 samples\n",
            "Processed 165/468 samples\n",
            "Processed 170/468 samples\n",
            "Processed 175/468 samples\n",
            "Processed 180/468 samples\n",
            "Processed 185/468 samples\n",
            "Processed 190/468 samples\n",
            "Processed 195/468 samples\n",
            "Processed 200/468 samples\n",
            "Processed 205/468 samples\n",
            "Processed 210/468 samples\n",
            "Processed 215/468 samples\n",
            "Processed 220/468 samples\n",
            "Processed 225/468 samples\n",
            "Processed 230/468 samples\n",
            "Processed 235/468 samples\n",
            "Processed 240/468 samples\n",
            "Processed 245/468 samples\n",
            "Processed 250/468 samples\n",
            "Processed 255/468 samples\n",
            "Processed 260/468 samples\n",
            "Processed 265/468 samples\n",
            "Processed 270/468 samples\n",
            "Processed 275/468 samples\n",
            "Processed 280/468 samples\n",
            "Processed 285/468 samples\n",
            "Processed 290/468 samples\n",
            "Processed 295/468 samples\n",
            "Processed 300/468 samples\n",
            "Processed 305/468 samples\n",
            "Processed 310/468 samples\n",
            "Processed 315/468 samples\n",
            "Processed 320/468 samples\n",
            "Processed 325/468 samples\n",
            "Processed 330/468 samples\n",
            "Processed 335/468 samples\n",
            "Processed 340/468 samples\n",
            "Processed 345/468 samples\n",
            "Processed 350/468 samples\n",
            "Processed 355/468 samples\n",
            "Processed 360/468 samples\n",
            "Processed 365/468 samples\n",
            "Processed 370/468 samples\n",
            "Processed 375/468 samples\n",
            "Processed 380/468 samples\n",
            "Processed 385/468 samples\n",
            "Processed 390/468 samples\n",
            "Processed 395/468 samples\n",
            "Processed 400/468 samples\n",
            "Processed 405/468 samples\n",
            "Processed 410/468 samples\n",
            "Processed 415/468 samples\n",
            "Processed 420/468 samples\n",
            "Processed 425/468 samples\n",
            "Processed 430/468 samples\n",
            "Processed 435/468 samples\n",
            "Processed 440/468 samples\n",
            "Processed 445/468 samples\n",
            "Processed 450/468 samples\n",
            "Processed 455/468 samples\n",
            "Processed 460/468 samples\n",
            "Processed 465/468 samples\n",
            "Processed 468/468 samples\n",
            "Processed 5/468 samples\n",
            "Processed 10/468 samples\n",
            "Processed 15/468 samples\n",
            "Processed 20/468 samples\n",
            "Processed 25/468 samples\n",
            "Processed 30/468 samples\n",
            "Processed 35/468 samples\n",
            "Processed 40/468 samples\n",
            "Processed 45/468 samples\n",
            "Processed 50/468 samples\n",
            "Processed 55/468 samples\n",
            "Processed 60/468 samples\n",
            "Processed 65/468 samples\n",
            "Processed 70/468 samples\n",
            "Processed 75/468 samples\n",
            "Processed 80/468 samples\n",
            "Processed 85/468 samples\n",
            "Processed 90/468 samples\n",
            "Processed 95/468 samples\n",
            "Processed 100/468 samples\n",
            "Processed 105/468 samples\n",
            "Processed 110/468 samples\n",
            "Processed 115/468 samples\n",
            "Processed 120/468 samples\n",
            "Processed 125/468 samples\n",
            "Processed 130/468 samples\n",
            "Processed 135/468 samples\n",
            "Processed 140/468 samples\n",
            "Processed 145/468 samples\n",
            "Processed 150/468 samples\n",
            "Processed 155/468 samples\n",
            "Processed 160/468 samples\n",
            "Processed 165/468 samples\n",
            "Processed 170/468 samples\n",
            "Processed 175/468 samples\n",
            "Processed 180/468 samples\n",
            "Processed 185/468 samples\n",
            "Processed 190/468 samples\n",
            "Processed 195/468 samples\n",
            "Processed 200/468 samples\n",
            "Processed 205/468 samples\n",
            "Processed 210/468 samples\n",
            "Processed 215/468 samples\n",
            "Processed 220/468 samples\n",
            "Processed 225/468 samples\n",
            "Processed 230/468 samples\n",
            "Processed 235/468 samples\n",
            "Processed 240/468 samples\n",
            "Processed 245/468 samples\n",
            "Processed 250/468 samples\n",
            "Processed 255/468 samples\n",
            "Processed 260/468 samples\n",
            "Processed 265/468 samples\n",
            "Processed 270/468 samples\n",
            "Processed 275/468 samples\n",
            "Processed 280/468 samples\n",
            "Processed 285/468 samples\n",
            "Processed 290/468 samples\n",
            "Processed 295/468 samples\n",
            "Processed 300/468 samples\n",
            "Processed 305/468 samples\n",
            "Processed 310/468 samples\n",
            "Processed 315/468 samples\n",
            "Processed 320/468 samples\n",
            "Processed 325/468 samples\n",
            "Processed 330/468 samples\n",
            "Processed 335/468 samples\n",
            "Processed 340/468 samples\n",
            "Processed 345/468 samples\n",
            "Processed 350/468 samples\n",
            "Processed 355/468 samples\n",
            "Processed 360/468 samples\n",
            "Processed 365/468 samples\n",
            "Processed 370/468 samples\n",
            "Processed 375/468 samples\n",
            "Processed 380/468 samples\n",
            "Processed 385/468 samples\n",
            "Processed 390/468 samples\n",
            "Processed 395/468 samples\n",
            "Processed 400/468 samples\n",
            "Processed 405/468 samples\n",
            "Processed 410/468 samples\n",
            "Processed 415/468 samples\n",
            "Processed 420/468 samples\n",
            "Processed 425/468 samples\n",
            "Processed 430/468 samples\n",
            "Processed 435/468 samples\n",
            "Processed 440/468 samples\n",
            "Processed 445/468 samples\n",
            "Processed 450/468 samples\n",
            "Processed 455/468 samples\n",
            "Processed 460/468 samples\n",
            "Processed 465/468 samples\n",
            "Processed 468/468 samples\n",
            "Processed 5/468 samples\n",
            "Processed 10/468 samples\n",
            "Processed 15/468 samples\n",
            "Processed 20/468 samples\n",
            "Processed 25/468 samples\n",
            "Processed 30/468 samples\n",
            "Processed 35/468 samples\n",
            "Processed 40/468 samples\n",
            "Processed 45/468 samples\n",
            "Processed 50/468 samples\n",
            "Processed 55/468 samples\n",
            "Processed 60/468 samples\n",
            "Processed 65/468 samples\n",
            "Processed 70/468 samples\n",
            "Processed 75/468 samples\n",
            "Processed 80/468 samples\n",
            "Processed 85/468 samples\n",
            "Processed 90/468 samples\n",
            "Processed 95/468 samples\n",
            "Processed 100/468 samples\n",
            "Processed 105/468 samples\n",
            "Processed 110/468 samples\n",
            "Processed 115/468 samples\n",
            "Processed 120/468 samples\n",
            "Processed 125/468 samples\n",
            "Processed 130/468 samples\n",
            "Processed 135/468 samples\n",
            "Processed 140/468 samples\n",
            "Processed 145/468 samples\n",
            "Processed 150/468 samples\n",
            "Processed 155/468 samples\n",
            "Processed 160/468 samples\n",
            "Processed 165/468 samples\n",
            "Processed 170/468 samples\n",
            "Processed 175/468 samples\n",
            "Processed 180/468 samples\n",
            "Processed 185/468 samples\n",
            "Processed 190/468 samples\n",
            "Processed 195/468 samples\n",
            "Processed 200/468 samples\n",
            "Processed 205/468 samples\n",
            "Processed 210/468 samples\n",
            "Processed 215/468 samples\n",
            "Processed 220/468 samples\n",
            "Processed 225/468 samples\n",
            "Processed 230/468 samples\n",
            "Processed 235/468 samples\n",
            "Processed 240/468 samples\n",
            "Processed 245/468 samples\n",
            "Processed 250/468 samples\n",
            "Processed 255/468 samples\n",
            "Processed 260/468 samples\n",
            "Processed 265/468 samples\n",
            "Processed 270/468 samples\n",
            "Processed 275/468 samples\n",
            "Processed 280/468 samples\n",
            "Processed 285/468 samples\n",
            "Processed 290/468 samples\n",
            "Processed 295/468 samples\n",
            "Processed 300/468 samples\n",
            "Processed 305/468 samples\n",
            "Processed 310/468 samples\n",
            "Processed 315/468 samples\n",
            "Processed 320/468 samples\n",
            "Processed 325/468 samples\n",
            "Processed 330/468 samples\n",
            "Processed 335/468 samples\n",
            "Processed 340/468 samples\n",
            "Processed 345/468 samples\n",
            "Processed 350/468 samples\n",
            "Processed 355/468 samples\n",
            "Processed 360/468 samples\n",
            "Processed 365/468 samples\n",
            "Processed 370/468 samples\n",
            "Processed 375/468 samples\n",
            "Processed 380/468 samples\n",
            "Processed 385/468 samples\n",
            "Processed 390/468 samples\n",
            "Processed 395/468 samples\n",
            "Processed 400/468 samples\n",
            "Processed 405/468 samples\n",
            "Processed 410/468 samples\n",
            "Processed 415/468 samples\n",
            "Processed 420/468 samples\n",
            "Processed 425/468 samples\n",
            "Processed 430/468 samples\n",
            "Processed 435/468 samples\n",
            "Processed 440/468 samples\n",
            "Processed 445/468 samples\n",
            "Processed 450/468 samples\n",
            "Processed 455/468 samples\n",
            "Processed 460/468 samples\n",
            "Processed 465/468 samples\n",
            "Processed 468/468 samples\n",
            "Processed 5/468 samples\n",
            "Processed 10/468 samples\n",
            "Processed 15/468 samples\n",
            "Processed 20/468 samples\n",
            "Processed 25/468 samples\n",
            "Processed 30/468 samples\n",
            "Processed 35/468 samples\n",
            "Processed 40/468 samples\n",
            "Processed 45/468 samples\n",
            "Processed 50/468 samples\n",
            "Processed 55/468 samples\n",
            "Processed 60/468 samples\n",
            "Processed 65/468 samples\n",
            "Processed 70/468 samples\n",
            "Processed 75/468 samples\n",
            "Processed 80/468 samples\n",
            "Processed 85/468 samples\n",
            "Processed 90/468 samples\n",
            "Processed 95/468 samples\n",
            "Processed 100/468 samples\n",
            "Processed 105/468 samples\n",
            "Processed 110/468 samples\n",
            "Processed 115/468 samples\n",
            "Processed 120/468 samples\n",
            "Processed 125/468 samples\n",
            "Processed 130/468 samples\n",
            "Processed 135/468 samples\n",
            "Processed 140/468 samples\n",
            "Processed 145/468 samples\n",
            "Processed 150/468 samples\n",
            "Processed 155/468 samples\n",
            "Processed 160/468 samples\n",
            "Processed 165/468 samples\n",
            "Processed 170/468 samples\n",
            "Processed 175/468 samples\n",
            "Processed 180/468 samples\n",
            "Processed 185/468 samples\n",
            "Processed 190/468 samples\n",
            "Processed 195/468 samples\n",
            "Processed 200/468 samples\n",
            "Processed 205/468 samples\n",
            "Processed 210/468 samples\n",
            "Processed 215/468 samples\n",
            "Processed 220/468 samples\n",
            "Processed 225/468 samples\n",
            "Processed 230/468 samples\n",
            "Processed 235/468 samples\n",
            "Processed 240/468 samples\n",
            "Processed 245/468 samples\n",
            "Processed 250/468 samples\n",
            "Processed 255/468 samples\n",
            "Processed 260/468 samples\n",
            "Processed 265/468 samples\n",
            "Processed 270/468 samples\n",
            "Processed 275/468 samples\n",
            "Processed 280/468 samples\n",
            "Processed 285/468 samples\n",
            "Processed 290/468 samples\n",
            "Processed 295/468 samples\n",
            "Processed 300/468 samples\n",
            "Processed 305/468 samples\n",
            "Processed 310/468 samples\n",
            "Processed 315/468 samples\n",
            "Processed 320/468 samples\n",
            "Processed 325/468 samples\n",
            "Processed 330/468 samples\n",
            "Processed 335/468 samples\n",
            "Processed 340/468 samples\n",
            "Processed 345/468 samples\n",
            "Processed 350/468 samples\n",
            "Processed 355/468 samples\n",
            "Processed 360/468 samples\n",
            "Processed 365/468 samples\n",
            "Processed 370/468 samples\n",
            "Processed 375/468 samples\n",
            "Processed 380/468 samples\n",
            "Processed 385/468 samples\n",
            "Processed 390/468 samples\n",
            "Processed 395/468 samples\n",
            "Processed 400/468 samples\n",
            "Processed 405/468 samples\n",
            "Processed 410/468 samples\n",
            "Processed 415/468 samples\n",
            "Processed 420/468 samples\n",
            "Processed 425/468 samples\n",
            "Processed 430/468 samples\n",
            "Processed 435/468 samples\n",
            "Processed 440/468 samples\n",
            "Processed 445/468 samples\n",
            "Processed 450/468 samples\n",
            "Processed 455/468 samples\n",
            "Processed 460/468 samples\n",
            "Processed 465/468 samples\n",
            "Processed 468/468 samples\n",
            "Processed 5/468 samples\n",
            "Processed 10/468 samples\n",
            "Processed 15/468 samples\n",
            "Processed 20/468 samples\n",
            "Processed 25/468 samples\n",
            "Processed 30/468 samples\n",
            "Processed 35/468 samples\n",
            "Processed 40/468 samples\n",
            "Processed 45/468 samples\n",
            "Processed 50/468 samples\n",
            "Processed 55/468 samples\n",
            "Processed 60/468 samples\n",
            "Processed 65/468 samples\n",
            "Processed 70/468 samples\n",
            "Processed 75/468 samples\n",
            "Processed 80/468 samples\n",
            "Processed 85/468 samples\n",
            "Processed 90/468 samples\n",
            "Processed 95/468 samples\n",
            "Processed 100/468 samples\n",
            "Processed 105/468 samples\n",
            "Processed 110/468 samples\n",
            "Processed 115/468 samples\n",
            "Processed 120/468 samples\n",
            "Processed 125/468 samples\n",
            "Processed 130/468 samples\n",
            "Processed 135/468 samples\n",
            "Processed 140/468 samples\n",
            "Processed 145/468 samples\n",
            "Processed 150/468 samples\n",
            "Processed 155/468 samples\n",
            "Processed 160/468 samples\n",
            "Processed 165/468 samples\n",
            "Processed 170/468 samples\n",
            "Processed 175/468 samples\n",
            "Processed 180/468 samples\n",
            "Processed 185/468 samples\n",
            "Processed 190/468 samples\n",
            "Processed 195/468 samples\n",
            "Processed 200/468 samples\n",
            "Processed 205/468 samples\n",
            "Processed 210/468 samples\n",
            "Processed 215/468 samples\n",
            "Processed 220/468 samples\n",
            "Processed 225/468 samples\n",
            "Processed 230/468 samples\n",
            "Processed 235/468 samples\n",
            "Processed 240/468 samples\n",
            "Processed 245/468 samples\n",
            "Processed 250/468 samples\n",
            "Processed 255/468 samples\n",
            "Processed 260/468 samples\n",
            "Processed 265/468 samples\n",
            "Processed 270/468 samples\n",
            "Processed 275/468 samples\n",
            "Processed 280/468 samples\n",
            "Processed 285/468 samples\n",
            "Processed 290/468 samples\n",
            "Processed 295/468 samples\n",
            "Processed 300/468 samples\n",
            "Processed 305/468 samples\n",
            "Processed 310/468 samples\n",
            "Processed 315/468 samples\n",
            "Processed 320/468 samples\n",
            "Processed 325/468 samples\n",
            "Processed 330/468 samples\n",
            "Processed 335/468 samples\n",
            "Processed 340/468 samples\n",
            "Processed 345/468 samples\n",
            "Processed 350/468 samples\n",
            "Processed 355/468 samples\n",
            "Processed 360/468 samples\n",
            "Processed 365/468 samples\n",
            "Processed 370/468 samples\n",
            "Processed 375/468 samples\n",
            "Processed 380/468 samples\n",
            "Processed 385/468 samples\n",
            "Processed 390/468 samples\n",
            "Processed 395/468 samples\n",
            "Processed 400/468 samples\n",
            "Processed 405/468 samples\n",
            "Processed 410/468 samples\n",
            "Processed 415/468 samples\n",
            "Processed 420/468 samples\n",
            "Processed 425/468 samples\n",
            "Processed 430/468 samples\n",
            "Processed 435/468 samples\n",
            "Processed 440/468 samples\n",
            "Processed 445/468 samples\n",
            "Processed 450/468 samples\n",
            "Processed 455/468 samples\n",
            "Processed 460/468 samples\n",
            "Processed 465/468 samples\n",
            "Processed 468/468 samples\n",
            "Processed 5/468 samples\n",
            "Processed 10/468 samples\n",
            "Processed 15/468 samples\n",
            "Processed 20/468 samples\n",
            "Processed 25/468 samples\n",
            "Processed 30/468 samples\n",
            "Processed 35/468 samples\n",
            "Processed 40/468 samples\n",
            "Processed 45/468 samples\n",
            "Processed 50/468 samples\n",
            "Processed 55/468 samples\n",
            "Processed 60/468 samples\n",
            "Processed 65/468 samples\n",
            "Processed 70/468 samples\n",
            "Processed 75/468 samples\n",
            "Processed 80/468 samples\n",
            "Processed 85/468 samples\n",
            "Processed 90/468 samples\n",
            "Processed 95/468 samples\n",
            "Processed 100/468 samples\n",
            "Processed 105/468 samples\n",
            "Processed 110/468 samples\n",
            "Processed 115/468 samples\n",
            "Processed 120/468 samples\n",
            "Processed 125/468 samples\n",
            "Processed 130/468 samples\n",
            "Processed 135/468 samples\n",
            "Processed 140/468 samples\n",
            "Processed 145/468 samples\n",
            "Processed 150/468 samples\n",
            "Processed 155/468 samples\n",
            "Processed 160/468 samples\n",
            "Processed 165/468 samples\n",
            "Processed 170/468 samples\n",
            "Processed 175/468 samples\n",
            "Processed 180/468 samples\n",
            "Processed 185/468 samples\n",
            "Processed 190/468 samples\n",
            "Processed 195/468 samples\n",
            "Processed 200/468 samples\n",
            "Processed 205/468 samples\n",
            "Processed 210/468 samples\n",
            "Processed 215/468 samples\n",
            "Processed 220/468 samples\n",
            "Processed 225/468 samples\n",
            "Processed 230/468 samples\n",
            "Processed 235/468 samples\n",
            "Processed 240/468 samples\n",
            "Processed 245/468 samples\n",
            "Processed 250/468 samples\n",
            "Processed 255/468 samples\n",
            "Processed 260/468 samples\n",
            "Processed 265/468 samples\n",
            "Processed 270/468 samples\n",
            "Processed 275/468 samples\n",
            "Processed 280/468 samples\n",
            "Processed 285/468 samples\n",
            "Processed 290/468 samples\n",
            "Processed 295/468 samples\n",
            "Processed 300/468 samples\n",
            "Processed 305/468 samples\n",
            "Processed 310/468 samples\n",
            "Processed 315/468 samples\n",
            "Processed 320/468 samples\n",
            "Processed 325/468 samples\n",
            "Processed 330/468 samples\n",
            "Processed 335/468 samples\n",
            "Processed 340/468 samples\n",
            "Processed 345/468 samples\n",
            "Processed 350/468 samples\n",
            "Processed 355/468 samples\n",
            "Processed 360/468 samples\n",
            "Processed 365/468 samples\n",
            "Processed 370/468 samples\n",
            "Processed 375/468 samples\n",
            "Processed 380/468 samples\n",
            "Processed 385/468 samples\n",
            "Processed 390/468 samples\n",
            "Processed 395/468 samples\n",
            "Processed 400/468 samples\n",
            "Processed 405/468 samples\n",
            "Processed 410/468 samples\n",
            "Processed 415/468 samples\n",
            "Processed 420/468 samples\n",
            "Processed 425/468 samples\n",
            "Processed 430/468 samples\n",
            "Processed 435/468 samples\n",
            "Processed 440/468 samples\n",
            "Processed 445/468 samples\n",
            "Processed 450/468 samples\n",
            "Processed 455/468 samples\n",
            "Processed 460/468 samples\n",
            "Processed 465/468 samples\n",
            "Processed 468/468 samples\n",
            "Processed 5/468 samples\n",
            "Processed 10/468 samples\n",
            "Processed 15/468 samples\n",
            "Processed 20/468 samples\n",
            "Processed 25/468 samples\n",
            "Processed 30/468 samples\n",
            "Processed 35/468 samples\n",
            "Processed 40/468 samples\n",
            "Processed 45/468 samples\n",
            "Processed 50/468 samples\n",
            "Processed 55/468 samples\n",
            "Processed 60/468 samples\n",
            "Processed 65/468 samples\n",
            "Processed 70/468 samples\n",
            "Processed 75/468 samples\n",
            "Processed 80/468 samples\n",
            "Processed 85/468 samples\n",
            "Processed 90/468 samples\n",
            "Processed 95/468 samples\n",
            "Processed 100/468 samples\n",
            "Processed 105/468 samples\n",
            "Processed 110/468 samples\n",
            "Processed 115/468 samples\n",
            "Processed 120/468 samples\n",
            "Processed 125/468 samples\n",
            "Processed 130/468 samples\n",
            "Processed 135/468 samples\n",
            "Processed 140/468 samples\n",
            "Processed 145/468 samples\n",
            "Processed 150/468 samples\n",
            "Processed 155/468 samples\n",
            "Processed 160/468 samples\n",
            "Processed 165/468 samples\n",
            "Processed 170/468 samples\n",
            "Processed 175/468 samples\n",
            "Processed 180/468 samples\n",
            "Processed 185/468 samples\n",
            "Processed 190/468 samples\n",
            "Processed 195/468 samples\n",
            "Processed 200/468 samples\n",
            "Processed 205/468 samples\n",
            "Processed 210/468 samples\n",
            "Processed 215/468 samples\n",
            "Processed 220/468 samples\n",
            "Processed 225/468 samples\n",
            "Processed 230/468 samples\n",
            "Processed 235/468 samples\n",
            "Processed 240/468 samples\n",
            "Processed 245/468 samples\n",
            "Processed 250/468 samples\n",
            "Processed 255/468 samples\n",
            "Processed 260/468 samples\n",
            "Processed 265/468 samples\n",
            "Processed 270/468 samples\n",
            "Processed 275/468 samples\n",
            "Processed 280/468 samples\n",
            "Processed 285/468 samples\n",
            "Processed 290/468 samples\n",
            "Processed 295/468 samples\n",
            "Processed 300/468 samples\n",
            "Processed 305/468 samples\n",
            "Processed 310/468 samples\n",
            "Processed 315/468 samples\n",
            "Processed 320/468 samples\n",
            "Processed 325/468 samples\n",
            "Processed 330/468 samples\n",
            "Processed 335/468 samples\n",
            "Processed 340/468 samples\n",
            "Processed 345/468 samples\n",
            "Processed 350/468 samples\n",
            "Processed 355/468 samples\n",
            "Processed 360/468 samples\n",
            "Processed 365/468 samples\n",
            "Processed 370/468 samples\n",
            "Processed 375/468 samples\n",
            "Processed 380/468 samples\n",
            "Processed 385/468 samples\n",
            "Processed 390/468 samples\n",
            "Processed 395/468 samples\n",
            "Processed 400/468 samples\n",
            "Processed 405/468 samples\n",
            "Processed 410/468 samples\n",
            "Processed 415/468 samples\n",
            "Processed 420/468 samples\n",
            "Processed 425/468 samples\n",
            "Processed 430/468 samples\n",
            "Processed 435/468 samples\n",
            "Processed 440/468 samples\n",
            "Processed 445/468 samples\n",
            "Processed 450/468 samples\n",
            "Processed 455/468 samples\n",
            "Processed 460/468 samples\n",
            "Processed 465/468 samples\n",
            "Processed 468/468 samples\n",
            "Processed 5/468 samples\n",
            "Processed 10/468 samples\n",
            "Processed 15/468 samples\n",
            "Processed 20/468 samples\n",
            "Processed 25/468 samples\n",
            "Processed 30/468 samples\n",
            "Processed 35/468 samples\n",
            "Processed 40/468 samples\n",
            "Processed 45/468 samples\n",
            "Processed 50/468 samples\n",
            "Processed 55/468 samples\n",
            "Processed 60/468 samples\n",
            "Processed 65/468 samples\n",
            "Processed 70/468 samples\n",
            "Processed 75/468 samples\n",
            "Processed 80/468 samples\n",
            "Processed 85/468 samples\n",
            "Processed 90/468 samples\n",
            "Processed 95/468 samples\n",
            "Processed 100/468 samples\n",
            "Processed 105/468 samples\n",
            "Processed 110/468 samples\n",
            "Processed 115/468 samples\n",
            "Processed 120/468 samples\n",
            "Processed 125/468 samples\n",
            "Processed 130/468 samples\n",
            "Processed 135/468 samples\n",
            "Processed 140/468 samples\n",
            "Processed 145/468 samples\n",
            "Processed 150/468 samples\n",
            "Processed 155/468 samples\n",
            "Processed 160/468 samples\n",
            "Processed 165/468 samples\n",
            "Processed 170/468 samples\n",
            "Processed 175/468 samples\n",
            "Processed 180/468 samples\n",
            "Processed 185/468 samples\n",
            "Processed 190/468 samples\n",
            "Processed 195/468 samples\n",
            "Processed 200/468 samples\n",
            "Processed 205/468 samples\n",
            "Processed 210/468 samples\n",
            "Processed 215/468 samples\n",
            "Processed 220/468 samples\n",
            "Processed 225/468 samples\n",
            "Processed 230/468 samples\n",
            "Processed 235/468 samples\n",
            "Processed 240/468 samples\n",
            "Processed 245/468 samples\n",
            "Processed 250/468 samples\n",
            "Processed 255/468 samples\n",
            "Processed 260/468 samples\n",
            "Processed 265/468 samples\n",
            "Processed 270/468 samples\n",
            "Processed 275/468 samples\n",
            "Processed 280/468 samples\n",
            "Processed 285/468 samples\n",
            "Processed 290/468 samples\n",
            "Processed 295/468 samples\n",
            "Processed 300/468 samples\n",
            "Processed 305/468 samples\n",
            "Processed 310/468 samples\n",
            "Processed 315/468 samples\n",
            "Processed 320/468 samples\n",
            "Processed 325/468 samples\n",
            "Processed 330/468 samples\n",
            "Processed 335/468 samples\n",
            "Processed 340/468 samples\n",
            "Processed 345/468 samples\n",
            "Processed 350/468 samples\n",
            "Processed 355/468 samples\n",
            "Processed 360/468 samples\n",
            "Processed 365/468 samples\n",
            "Processed 370/468 samples\n",
            "Processed 375/468 samples\n",
            "Processed 380/468 samples\n",
            "Processed 385/468 samples\n",
            "Processed 390/468 samples\n",
            "Processed 395/468 samples\n",
            "Processed 400/468 samples\n",
            "Processed 405/468 samples\n",
            "Processed 410/468 samples\n",
            "Processed 415/468 samples\n",
            "Processed 420/468 samples\n",
            "Processed 425/468 samples\n",
            "Processed 430/468 samples\n",
            "Processed 435/468 samples\n",
            "Processed 440/468 samples\n",
            "Processed 445/468 samples\n",
            "Processed 450/468 samples\n",
            "Processed 455/468 samples\n",
            "Processed 460/468 samples\n",
            "Processed 465/468 samples\n",
            "Processed 468/468 samples\n",
            "Processed 5/468 samples\n",
            "Processed 10/468 samples\n",
            "Processed 15/468 samples\n",
            "Processed 20/468 samples\n",
            "Processed 25/468 samples\n",
            "Processed 30/468 samples\n",
            "Processed 35/468 samples\n",
            "Processed 40/468 samples\n",
            "Processed 45/468 samples\n",
            "Processed 50/468 samples\n",
            "Processed 55/468 samples\n",
            "Processed 60/468 samples\n",
            "Processed 65/468 samples\n",
            "Processed 70/468 samples\n",
            "Processed 75/468 samples\n",
            "Processed 80/468 samples\n",
            "Processed 85/468 samples\n",
            "Processed 90/468 samples\n",
            "Processed 95/468 samples\n",
            "Processed 100/468 samples\n",
            "Processed 105/468 samples\n",
            "Processed 110/468 samples\n",
            "Processed 115/468 samples\n",
            "Processed 120/468 samples\n",
            "Processed 125/468 samples\n",
            "Processed 130/468 samples\n",
            "Processed 135/468 samples\n",
            "Processed 140/468 samples\n",
            "Processed 145/468 samples\n",
            "Processed 150/468 samples\n",
            "Processed 155/468 samples\n",
            "Processed 160/468 samples\n",
            "Processed 165/468 samples\n",
            "Processed 170/468 samples\n",
            "Processed 175/468 samples\n",
            "Processed 180/468 samples\n",
            "Processed 185/468 samples\n",
            "Processed 190/468 samples\n",
            "Processed 195/468 samples\n",
            "Processed 200/468 samples\n",
            "Processed 205/468 samples\n",
            "Processed 210/468 samples\n",
            "Processed 215/468 samples\n",
            "Processed 220/468 samples\n",
            "Processed 225/468 samples\n",
            "Processed 230/468 samples\n",
            "Processed 235/468 samples\n",
            "Processed 240/468 samples\n",
            "Processed 245/468 samples\n",
            "Processed 250/468 samples\n",
            "Processed 255/468 samples\n",
            "Processed 260/468 samples\n",
            "Processed 265/468 samples\n",
            "Processed 270/468 samples\n",
            "Processed 275/468 samples\n",
            "Processed 280/468 samples\n",
            "Processed 285/468 samples\n",
            "Processed 290/468 samples\n",
            "Processed 295/468 samples\n",
            "Processed 300/468 samples\n",
            "Processed 305/468 samples\n",
            "Processed 310/468 samples\n",
            "Processed 315/468 samples\n",
            "Processed 320/468 samples\n",
            "Processed 325/468 samples\n",
            "Processed 330/468 samples\n",
            "Processed 335/468 samples\n",
            "Processed 340/468 samples\n",
            "Processed 345/468 samples\n",
            "Processed 350/468 samples\n",
            "Processed 355/468 samples\n",
            "Processed 360/468 samples\n",
            "Processed 365/468 samples\n",
            "Processed 370/468 samples\n",
            "Processed 375/468 samples\n",
            "Processed 380/468 samples\n",
            "Processed 385/468 samples\n",
            "Processed 390/468 samples\n",
            "Processed 395/468 samples\n",
            "Processed 400/468 samples\n",
            "Processed 405/468 samples\n",
            "Processed 410/468 samples\n",
            "Processed 415/468 samples\n",
            "Processed 420/468 samples\n",
            "Processed 425/468 samples\n",
            "Processed 430/468 samples\n",
            "Processed 435/468 samples\n",
            "Processed 440/468 samples\n",
            "Processed 445/468 samples\n",
            "Processed 450/468 samples\n",
            "Processed 455/468 samples\n",
            "Processed 460/468 samples\n",
            "Processed 465/468 samples\n",
            "Processed 468/468 samples\n",
            "Processed 5/468 samples\n",
            "Processed 10/468 samples\n",
            "Processed 15/468 samples\n",
            "Processed 20/468 samples\n",
            "Processed 25/468 samples\n",
            "Processed 30/468 samples\n",
            "Processed 35/468 samples\n",
            "Processed 40/468 samples\n",
            "Processed 45/468 samples\n",
            "Processed 50/468 samples\n",
            "Processed 55/468 samples\n",
            "Processed 60/468 samples\n",
            "Processed 65/468 samples\n",
            "Processed 70/468 samples\n",
            "Processed 75/468 samples\n",
            "Processed 80/468 samples\n",
            "Processed 85/468 samples\n",
            "Processed 90/468 samples\n",
            "Processed 95/468 samples\n",
            "Processed 100/468 samples\n",
            "Processed 105/468 samples\n",
            "Processed 110/468 samples\n",
            "Processed 115/468 samples\n",
            "Processed 120/468 samples\n",
            "Processed 125/468 samples\n",
            "Processed 130/468 samples\n",
            "Processed 135/468 samples\n",
            "Processed 140/468 samples\n",
            "Processed 145/468 samples\n",
            "Processed 150/468 samples\n",
            "Processed 155/468 samples\n",
            "Processed 160/468 samples\n",
            "Processed 165/468 samples\n",
            "Processed 170/468 samples\n",
            "Processed 175/468 samples\n",
            "Processed 180/468 samples\n",
            "Processed 185/468 samples\n",
            "Processed 190/468 samples\n",
            "Processed 195/468 samples\n",
            "Processed 200/468 samples\n",
            "Processed 205/468 samples\n",
            "Processed 210/468 samples\n",
            "Processed 215/468 samples\n",
            "Processed 220/468 samples\n",
            "Processed 225/468 samples\n",
            "Processed 230/468 samples\n",
            "Processed 235/468 samples\n",
            "Processed 240/468 samples\n",
            "Processed 245/468 samples\n",
            "Processed 250/468 samples\n",
            "Processed 255/468 samples\n",
            "Processed 260/468 samples\n",
            "Processed 265/468 samples\n",
            "Processed 270/468 samples\n",
            "Processed 275/468 samples\n",
            "Processed 280/468 samples\n",
            "Processed 285/468 samples\n",
            "Processed 290/468 samples\n",
            "Processed 295/468 samples\n",
            "Processed 300/468 samples\n",
            "Processed 305/468 samples\n",
            "Processed 310/468 samples\n",
            "Processed 315/468 samples\n",
            "Processed 320/468 samples\n",
            "Processed 325/468 samples\n",
            "Processed 330/468 samples\n",
            "Processed 335/468 samples\n",
            "Processed 340/468 samples\n",
            "Processed 345/468 samples\n",
            "Processed 350/468 samples\n",
            "Processed 355/468 samples\n",
            "Processed 360/468 samples\n",
            "Processed 365/468 samples\n",
            "Processed 370/468 samples\n",
            "Processed 375/468 samples\n",
            "Processed 380/468 samples\n",
            "Processed 385/468 samples\n",
            "Processed 390/468 samples\n",
            "Processed 395/468 samples\n",
            "Processed 400/468 samples\n",
            "Processed 405/468 samples\n",
            "Processed 410/468 samples\n",
            "Processed 415/468 samples\n",
            "Processed 420/468 samples\n",
            "Processed 425/468 samples\n",
            "Processed 430/468 samples\n",
            "Processed 435/468 samples\n",
            "Processed 440/468 samples\n",
            "Processed 445/468 samples\n",
            "Processed 450/468 samples\n",
            "Processed 455/468 samples\n",
            "Processed 460/468 samples\n",
            "Processed 465/468 samples\n",
            "Processed 468/468 samples\n",
            "Processed 5/468 samples\n",
            "Processed 10/468 samples\n",
            "Processed 15/468 samples\n",
            "Processed 20/468 samples\n",
            "Processed 25/468 samples\n",
            "Processed 30/468 samples\n",
            "Processed 35/468 samples\n",
            "Processed 40/468 samples\n",
            "Processed 45/468 samples\n",
            "Processed 50/468 samples\n",
            "Processed 55/468 samples\n",
            "Processed 60/468 samples\n",
            "Processed 65/468 samples\n",
            "Processed 70/468 samples\n",
            "Processed 75/468 samples\n",
            "Processed 80/468 samples\n",
            "Processed 85/468 samples\n",
            "Processed 90/468 samples\n",
            "Processed 95/468 samples\n",
            "Processed 100/468 samples\n",
            "Processed 105/468 samples\n",
            "Processed 110/468 samples\n",
            "Processed 115/468 samples\n",
            "Processed 120/468 samples\n",
            "Processed 125/468 samples\n",
            "Processed 130/468 samples\n",
            "Processed 135/468 samples\n",
            "Processed 140/468 samples\n",
            "Processed 145/468 samples\n",
            "Processed 150/468 samples\n",
            "Processed 155/468 samples\n",
            "Processed 160/468 samples\n",
            "Processed 165/468 samples\n",
            "Processed 170/468 samples\n",
            "Processed 175/468 samples\n",
            "Processed 180/468 samples\n",
            "Processed 185/468 samples\n",
            "Processed 190/468 samples\n",
            "Processed 195/468 samples\n",
            "Processed 200/468 samples\n",
            "Processed 205/468 samples\n",
            "Processed 210/468 samples\n",
            "Processed 215/468 samples\n",
            "Processed 220/468 samples\n",
            "Processed 225/468 samples\n",
            "Processed 230/468 samples\n",
            "Processed 235/468 samples\n",
            "Processed 240/468 samples\n",
            "Processed 245/468 samples\n",
            "Processed 250/468 samples\n",
            "Processed 255/468 samples\n",
            "Processed 260/468 samples\n",
            "Processed 265/468 samples\n",
            "Processed 270/468 samples\n",
            "Processed 275/468 samples\n",
            "Processed 280/468 samples\n",
            "Processed 285/468 samples\n",
            "Processed 290/468 samples\n",
            "Processed 295/468 samples\n",
            "Processed 300/468 samples\n",
            "Processed 305/468 samples\n",
            "Processed 310/468 samples\n",
            "Processed 315/468 samples\n",
            "Processed 320/468 samples\n",
            "Processed 325/468 samples\n",
            "Processed 330/468 samples\n",
            "Processed 335/468 samples\n",
            "Processed 340/468 samples\n",
            "Processed 345/468 samples\n",
            "Processed 350/468 samples\n",
            "Processed 355/468 samples\n",
            "Processed 360/468 samples\n",
            "Processed 365/468 samples\n",
            "Processed 370/468 samples\n",
            "Processed 375/468 samples\n",
            "Processed 380/468 samples\n",
            "Processed 385/468 samples\n",
            "Processed 390/468 samples\n",
            "Processed 395/468 samples\n",
            "Processed 400/468 samples\n",
            "Processed 405/468 samples\n",
            "Processed 410/468 samples\n",
            "Processed 415/468 samples\n",
            "Processed 420/468 samples\n",
            "Processed 425/468 samples\n",
            "Processed 430/468 samples\n",
            "Processed 435/468 samples\n",
            "Processed 440/468 samples\n",
            "Processed 445/468 samples\n",
            "Processed 450/468 samples\n",
            "Processed 455/468 samples\n",
            "Processed 460/468 samples\n",
            "Processed 465/468 samples\n",
            "Processed 468/468 samples\n",
            "Processed 5/468 samples\n",
            "Processed 10/468 samples\n",
            "Processed 15/468 samples\n",
            "Processed 20/468 samples\n",
            "Processed 25/468 samples\n",
            "Processed 30/468 samples\n",
            "Processed 35/468 samples\n",
            "Processed 40/468 samples\n",
            "Processed 45/468 samples\n",
            "Processed 50/468 samples\n",
            "Processed 55/468 samples\n",
            "Processed 60/468 samples\n",
            "Processed 65/468 samples\n",
            "Processed 70/468 samples\n",
            "Processed 75/468 samples\n",
            "Processed 80/468 samples\n",
            "Processed 85/468 samples\n",
            "Processed 90/468 samples\n",
            "Processed 95/468 samples\n",
            "Processed 100/468 samples\n",
            "Processed 105/468 samples\n",
            "Processed 110/468 samples\n",
            "Processed 115/468 samples\n",
            "Processed 120/468 samples\n",
            "Processed 125/468 samples\n",
            "Processed 130/468 samples\n",
            "Processed 135/468 samples\n",
            "Processed 140/468 samples\n",
            "Processed 145/468 samples\n",
            "Processed 150/468 samples\n",
            "Processed 155/468 samples\n",
            "Processed 160/468 samples\n",
            "Processed 165/468 samples\n",
            "Processed 170/468 samples\n",
            "Processed 175/468 samples\n",
            "Processed 180/468 samples\n",
            "Processed 185/468 samples\n",
            "Processed 190/468 samples\n",
            "Processed 195/468 samples\n",
            "Processed 200/468 samples\n",
            "Processed 205/468 samples\n",
            "Processed 210/468 samples\n",
            "Processed 215/468 samples\n",
            "Processed 220/468 samples\n",
            "Processed 225/468 samples\n",
            "Processed 230/468 samples\n",
            "Processed 235/468 samples\n",
            "Processed 240/468 samples\n",
            "Processed 245/468 samples\n",
            "Processed 250/468 samples\n",
            "Processed 255/468 samples\n",
            "Processed 260/468 samples\n",
            "Processed 265/468 samples\n",
            "Processed 270/468 samples\n",
            "Processed 275/468 samples\n",
            "Processed 280/468 samples\n",
            "Processed 285/468 samples\n",
            "Processed 290/468 samples\n",
            "Processed 295/468 samples\n",
            "Processed 300/468 samples\n",
            "Processed 305/468 samples\n",
            "Processed 310/468 samples\n",
            "Processed 315/468 samples\n",
            "Processed 320/468 samples\n",
            "Processed 325/468 samples\n",
            "Processed 330/468 samples\n",
            "Processed 335/468 samples\n",
            "Processed 340/468 samples\n",
            "Processed 345/468 samples\n",
            "Processed 350/468 samples\n",
            "Processed 355/468 samples\n",
            "Processed 360/468 samples\n",
            "Processed 365/468 samples\n",
            "Processed 370/468 samples\n",
            "Processed 375/468 samples\n",
            "Processed 380/468 samples\n",
            "Processed 385/468 samples\n",
            "Processed 390/468 samples\n",
            "Processed 395/468 samples\n",
            "Processed 400/468 samples\n",
            "Processed 405/468 samples\n",
            "Processed 410/468 samples\n",
            "Processed 415/468 samples\n",
            "Processed 420/468 samples\n",
            "Processed 425/468 samples\n",
            "Processed 430/468 samples\n",
            "Processed 435/468 samples\n",
            "Processed 440/468 samples\n",
            "Processed 445/468 samples\n",
            "Processed 450/468 samples\n",
            "Processed 455/468 samples\n",
            "Processed 460/468 samples\n",
            "Processed 465/468 samples\n",
            "Processed 468/468 samples\n",
            "Processed 5/468 samples\n",
            "Processed 10/468 samples\n",
            "Processed 15/468 samples\n",
            "Processed 20/468 samples\n",
            "Processed 25/468 samples\n",
            "Processed 30/468 samples\n",
            "Processed 35/468 samples\n",
            "Processed 40/468 samples\n",
            "Processed 45/468 samples\n",
            "Processed 50/468 samples\n",
            "Processed 55/468 samples\n",
            "Processed 60/468 samples\n",
            "Processed 65/468 samples\n",
            "Processed 70/468 samples\n",
            "Processed 75/468 samples\n",
            "Processed 80/468 samples\n",
            "Processed 85/468 samples\n",
            "Processed 90/468 samples\n",
            "Processed 95/468 samples\n",
            "Processed 100/468 samples\n",
            "Processed 105/468 samples\n",
            "Processed 110/468 samples\n",
            "Processed 115/468 samples\n",
            "Processed 120/468 samples\n",
            "Processed 125/468 samples\n",
            "Processed 130/468 samples\n",
            "Processed 135/468 samples\n",
            "Processed 140/468 samples\n",
            "Processed 145/468 samples\n",
            "Processed 150/468 samples\n",
            "Processed 155/468 samples\n",
            "Processed 160/468 samples\n",
            "Processed 165/468 samples\n",
            "Processed 170/468 samples\n",
            "Processed 175/468 samples\n",
            "Processed 180/468 samples\n",
            "Processed 185/468 samples\n",
            "Processed 190/468 samples\n",
            "Processed 195/468 samples\n",
            "Processed 200/468 samples\n",
            "Processed 205/468 samples\n",
            "Processed 210/468 samples\n",
            "Processed 215/468 samples\n",
            "Processed 220/468 samples\n",
            "Processed 225/468 samples\n",
            "Processed 230/468 samples\n",
            "Processed 235/468 samples\n",
            "Processed 240/468 samples\n",
            "Processed 245/468 samples\n",
            "Processed 250/468 samples\n",
            "Processed 255/468 samples\n",
            "Processed 260/468 samples\n",
            "Processed 265/468 samples\n",
            "Processed 270/468 samples\n",
            "Processed 275/468 samples\n",
            "Processed 280/468 samples\n",
            "Processed 285/468 samples\n",
            "Processed 290/468 samples\n",
            "Processed 295/468 samples\n",
            "Processed 300/468 samples\n",
            "Processed 305/468 samples\n",
            "Processed 310/468 samples\n",
            "Processed 315/468 samples\n",
            "Processed 320/468 samples\n",
            "Processed 325/468 samples\n",
            "Processed 330/468 samples\n",
            "Processed 335/468 samples\n",
            "Processed 340/468 samples\n",
            "Processed 345/468 samples\n",
            "Processed 350/468 samples\n",
            "Processed 355/468 samples\n",
            "Processed 360/468 samples\n",
            "Processed 365/468 samples\n",
            "Processed 370/468 samples\n",
            "Processed 375/468 samples\n",
            "Processed 380/468 samples\n",
            "Processed 385/468 samples\n",
            "Processed 390/468 samples\n",
            "Processed 395/468 samples\n",
            "Processed 400/468 samples\n",
            "Processed 405/468 samples\n",
            "Processed 410/468 samples\n",
            "Processed 415/468 samples\n",
            "Processed 420/468 samples\n",
            "Processed 425/468 samples\n",
            "Processed 430/468 samples\n",
            "Processed 435/468 samples\n",
            "Processed 440/468 samples\n",
            "Processed 445/468 samples\n",
            "Processed 450/468 samples\n",
            "Processed 455/468 samples\n",
            "Processed 460/468 samples\n",
            "Processed 465/468 samples\n",
            "Processed 468/468 samples\n",
            "Processed 5/468 samples\n",
            "Processed 10/468 samples\n",
            "Processed 15/468 samples\n",
            "Processed 20/468 samples\n",
            "Processed 25/468 samples\n",
            "Processed 30/468 samples\n",
            "Processed 35/468 samples\n",
            "Processed 40/468 samples\n",
            "Processed 45/468 samples\n",
            "Processed 50/468 samples\n",
            "Processed 55/468 samples\n",
            "Processed 60/468 samples\n",
            "Processed 65/468 samples\n",
            "Processed 70/468 samples\n",
            "Processed 75/468 samples\n",
            "Processed 80/468 samples\n",
            "Processed 85/468 samples\n",
            "Processed 90/468 samples\n",
            "Processed 95/468 samples\n",
            "Processed 100/468 samples\n",
            "Processed 105/468 samples\n",
            "Processed 110/468 samples\n",
            "Processed 115/468 samples\n",
            "Processed 120/468 samples\n",
            "Processed 125/468 samples\n",
            "Processed 130/468 samples\n",
            "Processed 135/468 samples\n",
            "Processed 140/468 samples\n",
            "Processed 145/468 samples\n",
            "Processed 150/468 samples\n",
            "Processed 155/468 samples\n",
            "Processed 160/468 samples\n",
            "Processed 165/468 samples\n",
            "Processed 170/468 samples\n",
            "Processed 175/468 samples\n",
            "Processed 180/468 samples\n",
            "Processed 185/468 samples\n",
            "Processed 190/468 samples\n",
            "Processed 195/468 samples\n",
            "Processed 200/468 samples\n",
            "Processed 205/468 samples\n",
            "Processed 210/468 samples\n",
            "Processed 215/468 samples\n",
            "Processed 220/468 samples\n",
            "Processed 225/468 samples\n",
            "Processed 230/468 samples\n",
            "Processed 235/468 samples\n",
            "Processed 240/468 samples\n",
            "Processed 245/468 samples\n",
            "Processed 250/468 samples\n",
            "Processed 255/468 samples\n",
            "Processed 260/468 samples\n",
            "Processed 265/468 samples\n",
            "Processed 270/468 samples\n",
            "Processed 275/468 samples\n",
            "Processed 280/468 samples\n",
            "Processed 285/468 samples\n",
            "Processed 290/468 samples\n",
            "Processed 295/468 samples\n",
            "Processed 300/468 samples\n",
            "Processed 305/468 samples\n",
            "Processed 310/468 samples\n",
            "Processed 315/468 samples\n",
            "Processed 320/468 samples\n",
            "Processed 325/468 samples\n",
            "Processed 330/468 samples\n",
            "Processed 335/468 samples\n",
            "Processed 340/468 samples\n",
            "Processed 345/468 samples\n",
            "Processed 350/468 samples\n",
            "Processed 355/468 samples\n",
            "Processed 360/468 samples\n",
            "Processed 365/468 samples\n",
            "Processed 370/468 samples\n",
            "Processed 375/468 samples\n",
            "Processed 380/468 samples\n",
            "Processed 385/468 samples\n",
            "Processed 390/468 samples\n",
            "Processed 395/468 samples\n",
            "Processed 400/468 samples\n",
            "Processed 405/468 samples\n",
            "Processed 410/468 samples\n",
            "Processed 415/468 samples\n",
            "Processed 420/468 samples\n",
            "Processed 425/468 samples\n",
            "Processed 430/468 samples\n",
            "Processed 435/468 samples\n",
            "Processed 440/468 samples\n",
            "Processed 445/468 samples\n",
            "Processed 450/468 samples\n",
            "Processed 455/468 samples\n",
            "Processed 460/468 samples\n",
            "Processed 465/468 samples\n",
            "Processed 468/468 samples\n",
            "Processed 5/468 samples\n",
            "Processed 10/468 samples\n",
            "Processed 15/468 samples\n",
            "Processed 20/468 samples\n",
            "Processed 25/468 samples\n",
            "Processed 30/468 samples\n",
            "Processed 35/468 samples\n",
            "Processed 40/468 samples\n",
            "Processed 45/468 samples\n",
            "Processed 50/468 samples\n",
            "Processed 55/468 samples\n",
            "Processed 60/468 samples\n",
            "Processed 65/468 samples\n",
            "Processed 70/468 samples\n",
            "Processed 75/468 samples\n",
            "Processed 80/468 samples\n",
            "Processed 85/468 samples\n",
            "Processed 90/468 samples\n",
            "Processed 95/468 samples\n",
            "Processed 100/468 samples\n",
            "Processed 105/468 samples\n",
            "Processed 110/468 samples\n",
            "Processed 115/468 samples\n",
            "Processed 120/468 samples\n",
            "Processed 125/468 samples\n",
            "Processed 130/468 samples\n",
            "Processed 135/468 samples\n",
            "Processed 140/468 samples\n",
            "Processed 145/468 samples\n",
            "Processed 150/468 samples\n",
            "Processed 155/468 samples\n",
            "Processed 160/468 samples\n",
            "Processed 165/468 samples\n",
            "Processed 170/468 samples\n",
            "Processed 175/468 samples\n",
            "Processed 180/468 samples\n",
            "Processed 185/468 samples\n",
            "Processed 190/468 samples\n",
            "Processed 195/468 samples\n",
            "Processed 200/468 samples\n",
            "Processed 205/468 samples\n",
            "Processed 210/468 samples\n",
            "Processed 215/468 samples\n",
            "Processed 220/468 samples\n",
            "Processed 225/468 samples\n",
            "Processed 230/468 samples\n",
            "Processed 235/468 samples\n",
            "Processed 240/468 samples\n",
            "Processed 245/468 samples\n",
            "Processed 250/468 samples\n",
            "Processed 255/468 samples\n",
            "Processed 260/468 samples\n",
            "Processed 265/468 samples\n",
            "Processed 270/468 samples\n",
            "Processed 275/468 samples\n",
            "Processed 280/468 samples\n",
            "Processed 285/468 samples\n",
            "Processed 290/468 samples\n",
            "Processed 295/468 samples\n",
            "Processed 300/468 samples\n",
            "Processed 305/468 samples\n",
            "Processed 310/468 samples\n",
            "Processed 315/468 samples\n",
            "Processed 320/468 samples\n",
            "Processed 325/468 samples\n",
            "Processed 330/468 samples\n",
            "Processed 335/468 samples\n",
            "Processed 340/468 samples\n",
            "Processed 345/468 samples\n",
            "Processed 350/468 samples\n",
            "Processed 355/468 samples\n",
            "Processed 360/468 samples\n",
            "Processed 365/468 samples\n",
            "Processed 370/468 samples\n",
            "Processed 375/468 samples\n",
            "Processed 380/468 samples\n",
            "Processed 385/468 samples\n",
            "Processed 390/468 samples\n",
            "Processed 395/468 samples\n",
            "Processed 400/468 samples\n",
            "Processed 405/468 samples\n",
            "Processed 410/468 samples\n",
            "Processed 415/468 samples\n",
            "Processed 420/468 samples\n",
            "Processed 425/468 samples\n",
            "Processed 430/468 samples\n",
            "Processed 435/468 samples\n",
            "Processed 440/468 samples\n",
            "Processed 445/468 samples\n",
            "Processed 450/468 samples\n",
            "Processed 455/468 samples\n",
            "Processed 460/468 samples\n",
            "Processed 465/468 samples\n",
            "Processed 468/468 samples\n",
            "Processed 5/468 samples\n",
            "Processed 10/468 samples\n",
            "Processed 15/468 samples\n",
            "Processed 20/468 samples\n",
            "Processed 25/468 samples\n",
            "Processed 30/468 samples\n",
            "Processed 35/468 samples\n",
            "Processed 40/468 samples\n",
            "Processed 45/468 samples\n",
            "Processed 50/468 samples\n",
            "Processed 55/468 samples\n",
            "Processed 60/468 samples\n",
            "Processed 65/468 samples\n",
            "Processed 70/468 samples\n",
            "Processed 75/468 samples\n",
            "Processed 80/468 samples\n",
            "Processed 85/468 samples\n",
            "Processed 90/468 samples\n",
            "Processed 95/468 samples\n",
            "Processed 100/468 samples\n",
            "Processed 105/468 samples\n",
            "Processed 110/468 samples\n",
            "Processed 115/468 samples\n",
            "Processed 120/468 samples\n",
            "Processed 125/468 samples\n",
            "Processed 130/468 samples\n",
            "Processed 135/468 samples\n",
            "Processed 140/468 samples\n",
            "Processed 145/468 samples\n",
            "Processed 150/468 samples\n",
            "Processed 155/468 samples\n",
            "Processed 160/468 samples\n",
            "Processed 165/468 samples\n",
            "Processed 170/468 samples\n",
            "Processed 175/468 samples\n",
            "Processed 180/468 samples\n",
            "Processed 185/468 samples\n",
            "Processed 190/468 samples\n",
            "Processed 195/468 samples\n",
            "Processed 200/468 samples\n",
            "Processed 205/468 samples\n",
            "Processed 210/468 samples\n",
            "Processed 215/468 samples\n",
            "Processed 220/468 samples\n",
            "Processed 225/468 samples\n",
            "Processed 230/468 samples\n",
            "Processed 235/468 samples\n",
            "Processed 240/468 samples\n",
            "Processed 245/468 samples\n",
            "Processed 250/468 samples\n",
            "Processed 255/468 samples\n",
            "Processed 260/468 samples\n",
            "Processed 265/468 samples\n",
            "Processed 270/468 samples\n",
            "Processed 275/468 samples\n",
            "Processed 280/468 samples\n",
            "Processed 285/468 samples\n",
            "Processed 290/468 samples\n",
            "Processed 295/468 samples\n",
            "Processed 300/468 samples\n",
            "Processed 305/468 samples\n",
            "Processed 310/468 samples\n",
            "Processed 315/468 samples\n",
            "Processed 320/468 samples\n",
            "Processed 325/468 samples\n",
            "Processed 330/468 samples\n",
            "Processed 335/468 samples\n",
            "Processed 340/468 samples\n",
            "Processed 345/468 samples\n",
            "Processed 350/468 samples\n",
            "Processed 355/468 samples\n",
            "Processed 360/468 samples\n",
            "Processed 365/468 samples\n",
            "Processed 370/468 samples\n",
            "Processed 375/468 samples\n",
            "Processed 380/468 samples\n",
            "Processed 385/468 samples\n",
            "Processed 390/468 samples\n",
            "Processed 395/468 samples\n",
            "Processed 400/468 samples\n",
            "Processed 405/468 samples\n",
            "Processed 410/468 samples\n",
            "Processed 415/468 samples\n",
            "Processed 420/468 samples\n",
            "Processed 425/468 samples\n",
            "Processed 430/468 samples\n",
            "Processed 435/468 samples\n",
            "Processed 440/468 samples\n",
            "Processed 445/468 samples\n",
            "Processed 450/468 samples\n",
            "Processed 455/468 samples\n",
            "Processed 460/468 samples\n",
            "Processed 465/468 samples\n",
            "Processed 468/468 samples\n",
            "Processed 5/468 samples\n",
            "Processed 10/468 samples\n",
            "Processed 15/468 samples\n",
            "Processed 20/468 samples\n",
            "Processed 25/468 samples\n",
            "Processed 30/468 samples\n",
            "Processed 35/468 samples\n",
            "Processed 40/468 samples\n",
            "Processed 45/468 samples\n",
            "Processed 50/468 samples\n",
            "Processed 55/468 samples\n",
            "Processed 60/468 samples\n",
            "Processed 65/468 samples\n",
            "Processed 70/468 samples\n",
            "Processed 75/468 samples\n",
            "Processed 80/468 samples\n",
            "Processed 85/468 samples\n",
            "Processed 90/468 samples\n",
            "Processed 95/468 samples\n",
            "Processed 100/468 samples\n",
            "Processed 105/468 samples\n",
            "Processed 110/468 samples\n",
            "Processed 115/468 samples\n",
            "Processed 120/468 samples\n",
            "Processed 125/468 samples\n",
            "Processed 130/468 samples\n",
            "Processed 135/468 samples\n",
            "Processed 140/468 samples\n",
            "Processed 145/468 samples\n",
            "Processed 150/468 samples\n",
            "Processed 155/468 samples\n",
            "Processed 160/468 samples\n",
            "Processed 165/468 samples\n",
            "Processed 170/468 samples\n",
            "Processed 175/468 samples\n",
            "Processed 180/468 samples\n",
            "Processed 185/468 samples\n",
            "Processed 190/468 samples\n",
            "Processed 195/468 samples\n",
            "Processed 200/468 samples\n",
            "Processed 205/468 samples\n",
            "Processed 210/468 samples\n",
            "Processed 215/468 samples\n",
            "Processed 220/468 samples\n",
            "Processed 225/468 samples\n",
            "Processed 230/468 samples\n",
            "Processed 235/468 samples\n",
            "Processed 240/468 samples\n",
            "Processed 245/468 samples\n",
            "Processed 250/468 samples\n",
            "Processed 255/468 samples\n",
            "Processed 260/468 samples\n",
            "Processed 265/468 samples\n",
            "Processed 270/468 samples\n",
            "Processed 275/468 samples\n",
            "Processed 280/468 samples\n",
            "Processed 285/468 samples\n",
            "Processed 290/468 samples\n",
            "Processed 295/468 samples\n",
            "Processed 300/468 samples\n",
            "Processed 305/468 samples\n",
            "Processed 310/468 samples\n",
            "Processed 315/468 samples\n",
            "Processed 320/468 samples\n",
            "Processed 325/468 samples\n",
            "Processed 330/468 samples\n",
            "Processed 335/468 samples\n",
            "Processed 340/468 samples\n",
            "Processed 345/468 samples\n",
            "Processed 350/468 samples\n",
            "Processed 355/468 samples\n",
            "Processed 360/468 samples\n",
            "Processed 365/468 samples\n",
            "Processed 370/468 samples\n",
            "Processed 375/468 samples\n",
            "Processed 380/468 samples\n",
            "Processed 385/468 samples\n",
            "Processed 390/468 samples\n",
            "Processed 395/468 samples\n",
            "Processed 400/468 samples\n",
            "Processed 405/468 samples\n",
            "Processed 410/468 samples\n",
            "Processed 415/468 samples\n",
            "Processed 420/468 samples\n",
            "Processed 425/468 samples\n",
            "Processed 430/468 samples\n",
            "Processed 435/468 samples\n",
            "Processed 440/468 samples\n",
            "Processed 445/468 samples\n",
            "Processed 450/468 samples\n",
            "Processed 455/468 samples\n",
            "Processed 460/468 samples\n",
            "Processed 465/468 samples\n",
            "Processed 468/468 samples\n",
            "Processed 5/468 samples\n",
            "Processed 10/468 samples\n",
            "Processed 15/468 samples\n",
            "Processed 20/468 samples\n",
            "Processed 25/468 samples\n",
            "Processed 30/468 samples\n",
            "Processed 35/468 samples\n",
            "Processed 40/468 samples\n",
            "Processed 45/468 samples\n",
            "Processed 50/468 samples\n",
            "Processed 55/468 samples\n",
            "Processed 60/468 samples\n",
            "Processed 65/468 samples\n",
            "Processed 70/468 samples\n",
            "Processed 75/468 samples\n",
            "Processed 80/468 samples\n",
            "Processed 85/468 samples\n",
            "Processed 90/468 samples\n",
            "Processed 95/468 samples\n",
            "Processed 100/468 samples\n",
            "Processed 105/468 samples\n",
            "Processed 110/468 samples\n",
            "Processed 115/468 samples\n",
            "Processed 120/468 samples\n",
            "Processed 125/468 samples\n",
            "Processed 130/468 samples\n",
            "Processed 135/468 samples\n",
            "Processed 140/468 samples\n",
            "Processed 145/468 samples\n",
            "Processed 150/468 samples\n",
            "Processed 155/468 samples\n",
            "Processed 160/468 samples\n",
            "Processed 165/468 samples\n",
            "Processed 170/468 samples\n",
            "Processed 175/468 samples\n",
            "Processed 180/468 samples\n",
            "Processed 185/468 samples\n",
            "Processed 190/468 samples\n",
            "Processed 195/468 samples\n",
            "Processed 200/468 samples\n",
            "Processed 205/468 samples\n",
            "Processed 210/468 samples\n",
            "Processed 215/468 samples\n",
            "Processed 220/468 samples\n",
            "Processed 225/468 samples\n",
            "Processed 230/468 samples\n",
            "Processed 235/468 samples\n",
            "Processed 240/468 samples\n",
            "Processed 245/468 samples\n",
            "Processed 250/468 samples\n",
            "Processed 255/468 samples\n",
            "Processed 260/468 samples\n",
            "Processed 265/468 samples\n",
            "Processed 270/468 samples\n",
            "Processed 275/468 samples\n",
            "Processed 280/468 samples\n",
            "Processed 285/468 samples\n",
            "Processed 290/468 samples\n",
            "Processed 295/468 samples\n",
            "Processed 300/468 samples\n",
            "Processed 305/468 samples\n",
            "Processed 310/468 samples\n",
            "Processed 315/468 samples\n",
            "Processed 320/468 samples\n",
            "Processed 325/468 samples\n",
            "Processed 330/468 samples\n",
            "Processed 335/468 samples\n",
            "Processed 340/468 samples\n",
            "Processed 345/468 samples\n",
            "Processed 350/468 samples\n",
            "Processed 355/468 samples\n",
            "Processed 360/468 samples\n",
            "Processed 365/468 samples\n",
            "Processed 370/468 samples\n",
            "Processed 375/468 samples\n",
            "Processed 380/468 samples\n",
            "Processed 385/468 samples\n",
            "Processed 390/468 samples\n",
            "Processed 395/468 samples\n",
            "Processed 400/468 samples\n",
            "Processed 405/468 samples\n",
            "Processed 410/468 samples\n",
            "Processed 415/468 samples\n",
            "Processed 420/468 samples\n",
            "Processed 425/468 samples\n",
            "Processed 430/468 samples\n",
            "Processed 435/468 samples\n",
            "Processed 440/468 samples\n",
            "Processed 445/468 samples\n",
            "Processed 450/468 samples\n",
            "Processed 455/468 samples\n",
            "Processed 460/468 samples\n",
            "Processed 465/468 samples\n",
            "Processed 468/468 samples\n",
            "Processed 5/468 samples\n",
            "Processed 10/468 samples\n",
            "Processed 15/468 samples\n",
            "Processed 20/468 samples\n",
            "Processed 25/468 samples\n",
            "Processed 30/468 samples\n",
            "Processed 35/468 samples\n",
            "Processed 40/468 samples\n",
            "Processed 45/468 samples\n",
            "Processed 50/468 samples\n",
            "Processed 55/468 samples\n",
            "Processed 60/468 samples\n",
            "Processed 65/468 samples\n",
            "Processed 70/468 samples\n",
            "Processed 75/468 samples\n",
            "Processed 80/468 samples\n",
            "Processed 85/468 samples\n",
            "Processed 90/468 samples\n",
            "Processed 95/468 samples\n",
            "Processed 100/468 samples\n",
            "Processed 105/468 samples\n",
            "Processed 110/468 samples\n",
            "Processed 115/468 samples\n",
            "Processed 120/468 samples\n",
            "Processed 125/468 samples\n",
            "Processed 130/468 samples\n",
            "Processed 135/468 samples\n",
            "Processed 140/468 samples\n",
            "Processed 145/468 samples\n",
            "Processed 150/468 samples\n",
            "Processed 155/468 samples\n",
            "Processed 160/468 samples\n",
            "Processed 165/468 samples\n",
            "Processed 170/468 samples\n",
            "Processed 175/468 samples\n",
            "Processed 180/468 samples\n",
            "Processed 185/468 samples\n",
            "Processed 190/468 samples\n",
            "Processed 195/468 samples\n",
            "Processed 200/468 samples\n",
            "Processed 205/468 samples\n",
            "Processed 210/468 samples\n",
            "Processed 215/468 samples\n",
            "Processed 220/468 samples\n",
            "Processed 225/468 samples\n",
            "Processed 230/468 samples\n",
            "Processed 235/468 samples\n",
            "Processed 240/468 samples\n",
            "Processed 245/468 samples\n",
            "Processed 250/468 samples\n",
            "Processed 255/468 samples\n",
            "Processed 260/468 samples\n",
            "Processed 265/468 samples\n",
            "Processed 270/468 samples\n",
            "Processed 275/468 samples\n",
            "Processed 280/468 samples\n",
            "Processed 285/468 samples\n",
            "Processed 290/468 samples\n",
            "Processed 295/468 samples\n",
            "Processed 300/468 samples\n",
            "Processed 305/468 samples\n",
            "Processed 310/468 samples\n",
            "Processed 315/468 samples\n",
            "Processed 320/468 samples\n",
            "Processed 325/468 samples\n",
            "Processed 330/468 samples\n",
            "Processed 335/468 samples\n",
            "Processed 340/468 samples\n",
            "Processed 345/468 samples\n",
            "Processed 350/468 samples\n",
            "Processed 355/468 samples\n",
            "Processed 360/468 samples\n",
            "Processed 365/468 samples\n",
            "Processed 370/468 samples\n",
            "Processed 375/468 samples\n",
            "Processed 380/468 samples\n",
            "Processed 385/468 samples\n",
            "Processed 390/468 samples\n",
            "Processed 395/468 samples\n",
            "Processed 400/468 samples\n",
            "Processed 405/468 samples\n",
            "Processed 410/468 samples\n",
            "Processed 415/468 samples\n",
            "Processed 420/468 samples\n",
            "Processed 425/468 samples\n",
            "Processed 430/468 samples\n",
            "Processed 435/468 samples\n",
            "Processed 440/468 samples\n",
            "Processed 445/468 samples\n",
            "Processed 450/468 samples\n",
            "Processed 455/468 samples\n",
            "Processed 460/468 samples\n",
            "Processed 465/468 samples\n",
            "Processed 468/468 samples\n",
            "Processed 5/468 samples\n",
            "Processed 10/468 samples\n",
            "Processed 15/468 samples\n",
            "Processed 20/468 samples\n",
            "Processed 25/468 samples\n",
            "Processed 30/468 samples\n",
            "Processed 35/468 samples\n",
            "Processed 40/468 samples\n",
            "Processed 45/468 samples\n",
            "Processed 50/468 samples\n",
            "Processed 55/468 samples\n",
            "Processed 60/468 samples\n",
            "Processed 65/468 samples\n",
            "Processed 70/468 samples\n",
            "Processed 75/468 samples\n",
            "Processed 80/468 samples\n",
            "Processed 85/468 samples\n",
            "Processed 90/468 samples\n",
            "Processed 95/468 samples\n",
            "Processed 100/468 samples\n",
            "Processed 105/468 samples\n",
            "Processed 110/468 samples\n",
            "Processed 115/468 samples\n",
            "Processed 120/468 samples\n",
            "Processed 125/468 samples\n",
            "Processed 130/468 samples\n",
            "Processed 135/468 samples\n",
            "Processed 140/468 samples\n",
            "Processed 145/468 samples\n",
            "Processed 150/468 samples\n",
            "Processed 155/468 samples\n",
            "Processed 160/468 samples\n",
            "Processed 165/468 samples\n",
            "Processed 170/468 samples\n",
            "Processed 175/468 samples\n",
            "Processed 180/468 samples\n",
            "Processed 185/468 samples\n",
            "Processed 190/468 samples\n",
            "Processed 195/468 samples\n",
            "Processed 200/468 samples\n",
            "Processed 205/468 samples\n",
            "Processed 210/468 samples\n",
            "Processed 215/468 samples\n",
            "Processed 220/468 samples\n",
            "Processed 225/468 samples\n",
            "Processed 230/468 samples\n",
            "Processed 235/468 samples\n",
            "Processed 240/468 samples\n",
            "Processed 245/468 samples\n",
            "Processed 250/468 samples\n",
            "Processed 255/468 samples\n",
            "Processed 260/468 samples\n",
            "Processed 265/468 samples\n",
            "Processed 270/468 samples\n",
            "Processed 275/468 samples\n",
            "Processed 280/468 samples\n",
            "Processed 285/468 samples\n",
            "Processed 290/468 samples\n",
            "Processed 295/468 samples\n",
            "Processed 300/468 samples\n",
            "Processed 305/468 samples\n",
            "Processed 310/468 samples\n",
            "Processed 315/468 samples\n",
            "Processed 320/468 samples\n",
            "Processed 325/468 samples\n",
            "Processed 330/468 samples\n",
            "Processed 335/468 samples\n",
            "Processed 340/468 samples\n",
            "Processed 345/468 samples\n",
            "Processed 350/468 samples\n",
            "Processed 355/468 samples\n",
            "Processed 360/468 samples\n",
            "Processed 365/468 samples\n",
            "Processed 370/468 samples\n",
            "Processed 375/468 samples\n",
            "Processed 380/468 samples\n",
            "Processed 385/468 samples\n",
            "Processed 390/468 samples\n",
            "Processed 395/468 samples\n",
            "Processed 400/468 samples\n",
            "Processed 405/468 samples\n",
            "Processed 410/468 samples\n",
            "Processed 415/468 samples\n",
            "Processed 420/468 samples\n",
            "Processed 425/468 samples\n",
            "Processed 430/468 samples\n",
            "Processed 435/468 samples\n",
            "Processed 440/468 samples\n",
            "Processed 445/468 samples\n",
            "Processed 450/468 samples\n",
            "Processed 455/468 samples\n",
            "Processed 460/468 samples\n",
            "Processed 465/468 samples\n",
            "Processed 468/468 samples\n",
            "Processed 5/468 samples\n",
            "Processed 10/468 samples\n",
            "Processed 15/468 samples\n",
            "Processed 20/468 samples\n",
            "Processed 25/468 samples\n",
            "Processed 30/468 samples\n",
            "Processed 35/468 samples\n",
            "Processed 40/468 samples\n",
            "Processed 45/468 samples\n",
            "Processed 50/468 samples\n",
            "Processed 55/468 samples\n",
            "Processed 60/468 samples\n",
            "Processed 65/468 samples\n",
            "Processed 70/468 samples\n",
            "Processed 75/468 samples\n",
            "Processed 80/468 samples\n",
            "Processed 85/468 samples\n",
            "Processed 90/468 samples\n",
            "Processed 95/468 samples\n",
            "Processed 100/468 samples\n",
            "Processed 105/468 samples\n",
            "Processed 110/468 samples\n",
            "Processed 115/468 samples\n",
            "Processed 120/468 samples\n",
            "Processed 125/468 samples\n",
            "Processed 130/468 samples\n",
            "Processed 135/468 samples\n",
            "Processed 140/468 samples\n",
            "Processed 145/468 samples\n",
            "Processed 150/468 samples\n",
            "Processed 155/468 samples\n",
            "Processed 160/468 samples\n",
            "Processed 165/468 samples\n",
            "Processed 170/468 samples\n",
            "Processed 175/468 samples\n",
            "Processed 180/468 samples\n",
            "Processed 185/468 samples\n",
            "Processed 190/468 samples\n",
            "Processed 195/468 samples\n",
            "Processed 200/468 samples\n",
            "Processed 205/468 samples\n",
            "Processed 210/468 samples\n",
            "Processed 215/468 samples\n",
            "Processed 220/468 samples\n",
            "Processed 225/468 samples\n",
            "Processed 230/468 samples\n",
            "Processed 235/468 samples\n",
            "Processed 240/468 samples\n",
            "Processed 245/468 samples\n",
            "Processed 250/468 samples\n",
            "Processed 255/468 samples\n",
            "Processed 260/468 samples\n",
            "Processed 265/468 samples\n",
            "Processed 270/468 samples\n",
            "Processed 275/468 samples\n",
            "Processed 280/468 samples\n",
            "Processed 285/468 samples\n",
            "Processed 290/468 samples\n",
            "Processed 295/468 samples\n",
            "Processed 300/468 samples\n",
            "Processed 305/468 samples\n",
            "Processed 310/468 samples\n",
            "Processed 315/468 samples\n",
            "Processed 320/468 samples\n",
            "Processed 325/468 samples\n",
            "Processed 330/468 samples\n",
            "Processed 335/468 samples\n",
            "Processed 340/468 samples\n",
            "Processed 345/468 samples\n",
            "Processed 350/468 samples\n",
            "Processed 355/468 samples\n",
            "Processed 360/468 samples\n",
            "Processed 365/468 samples\n",
            "Processed 370/468 samples\n",
            "Processed 375/468 samples\n",
            "Processed 380/468 samples\n",
            "Processed 385/468 samples\n",
            "Processed 390/468 samples\n",
            "Processed 395/468 samples\n",
            "Processed 400/468 samples\n",
            "Processed 405/468 samples\n",
            "Processed 410/468 samples\n",
            "Processed 415/468 samples\n",
            "Processed 420/468 samples\n",
            "Processed 425/468 samples\n",
            "Processed 430/468 samples\n",
            "Processed 435/468 samples\n",
            "Processed 440/468 samples\n",
            "Processed 445/468 samples\n",
            "Processed 450/468 samples\n",
            "Processed 455/468 samples\n",
            "Processed 460/468 samples\n",
            "Processed 465/468 samples\n",
            "Processed 468/468 samples\n",
            "Processed 5/468 samples\n",
            "Processed 10/468 samples\n",
            "Processed 15/468 samples\n",
            "Processed 20/468 samples\n",
            "Processed 25/468 samples\n",
            "Processed 30/468 samples\n",
            "Processed 35/468 samples\n",
            "Processed 40/468 samples\n",
            "Processed 45/468 samples\n",
            "Processed 50/468 samples\n",
            "Processed 55/468 samples\n",
            "Processed 60/468 samples\n",
            "Processed 65/468 samples\n",
            "Processed 70/468 samples\n",
            "Processed 75/468 samples\n",
            "Processed 80/468 samples\n",
            "Processed 85/468 samples\n",
            "Processed 90/468 samples\n",
            "Processed 95/468 samples\n",
            "Processed 100/468 samples\n",
            "Processed 105/468 samples\n",
            "Processed 110/468 samples\n",
            "Processed 115/468 samples\n",
            "Processed 120/468 samples\n",
            "Processed 125/468 samples\n",
            "Processed 130/468 samples\n",
            "Processed 135/468 samples\n",
            "Processed 140/468 samples\n",
            "Processed 145/468 samples\n",
            "Processed 150/468 samples\n",
            "Processed 155/468 samples\n",
            "Processed 160/468 samples\n",
            "Processed 165/468 samples\n",
            "Processed 170/468 samples\n",
            "Processed 175/468 samples\n",
            "Processed 180/468 samples\n",
            "Processed 185/468 samples\n",
            "Processed 190/468 samples\n",
            "Processed 195/468 samples\n",
            "Processed 200/468 samples\n",
            "Processed 205/468 samples\n",
            "Processed 210/468 samples\n",
            "Processed 215/468 samples\n",
            "Processed 220/468 samples\n",
            "Processed 225/468 samples\n",
            "Processed 230/468 samples\n",
            "Processed 235/468 samples\n",
            "Processed 240/468 samples\n",
            "Processed 245/468 samples\n",
            "Processed 250/468 samples\n",
            "Processed 255/468 samples\n",
            "Processed 260/468 samples\n",
            "Processed 265/468 samples\n",
            "Processed 270/468 samples\n",
            "Processed 275/468 samples\n",
            "Processed 280/468 samples\n",
            "Processed 285/468 samples\n",
            "Processed 290/468 samples\n",
            "Processed 295/468 samples\n",
            "Processed 300/468 samples\n",
            "Processed 305/468 samples\n",
            "Processed 310/468 samples\n",
            "Processed 315/468 samples\n",
            "Processed 320/468 samples\n",
            "Processed 325/468 samples\n",
            "Processed 330/468 samples\n",
            "Processed 335/468 samples\n",
            "Processed 340/468 samples\n",
            "Processed 345/468 samples\n",
            "Processed 350/468 samples\n",
            "Processed 355/468 samples\n",
            "Processed 360/468 samples\n",
            "Processed 365/468 samples\n",
            "Processed 370/468 samples\n",
            "Processed 375/468 samples\n",
            "Processed 380/468 samples\n",
            "Processed 385/468 samples\n",
            "Processed 390/468 samples\n",
            "Processed 395/468 samples\n",
            "Processed 400/468 samples\n",
            "Processed 405/468 samples\n",
            "Processed 410/468 samples\n",
            "Processed 415/468 samples\n",
            "Processed 420/468 samples\n",
            "Processed 425/468 samples\n",
            "Processed 430/468 samples\n",
            "Processed 435/468 samples\n",
            "Processed 440/468 samples\n",
            "Processed 445/468 samples\n",
            "Processed 450/468 samples\n",
            "Processed 455/468 samples\n",
            "Processed 460/468 samples\n",
            "Processed 465/468 samples\n",
            "Processed 468/468 samples\n",
            "Processed 5/468 samples\n",
            "Processed 10/468 samples\n",
            "Processed 15/468 samples\n",
            "Processed 20/468 samples\n",
            "Processed 25/468 samples\n",
            "Processed 30/468 samples\n",
            "Processed 35/468 samples\n",
            "Processed 40/468 samples\n",
            "Processed 45/468 samples\n",
            "Processed 50/468 samples\n",
            "Processed 55/468 samples\n",
            "Processed 60/468 samples\n",
            "Processed 65/468 samples\n",
            "Processed 70/468 samples\n",
            "Processed 75/468 samples\n",
            "Processed 80/468 samples\n",
            "Processed 85/468 samples\n",
            "Processed 90/468 samples\n",
            "Processed 95/468 samples\n",
            "Processed 100/468 samples\n",
            "Processed 105/468 samples\n",
            "Processed 110/468 samples\n",
            "Processed 115/468 samples\n",
            "Processed 120/468 samples\n",
            "Processed 125/468 samples\n",
            "Processed 130/468 samples\n",
            "Processed 135/468 samples\n",
            "Processed 140/468 samples\n",
            "Processed 145/468 samples\n",
            "Processed 150/468 samples\n",
            "Processed 155/468 samples\n",
            "Processed 160/468 samples\n",
            "Processed 165/468 samples\n",
            "Processed 170/468 samples\n",
            "Processed 175/468 samples\n",
            "Processed 180/468 samples\n",
            "Processed 185/468 samples\n",
            "Processed 190/468 samples\n",
            "Processed 195/468 samples\n",
            "Processed 200/468 samples\n",
            "Processed 205/468 samples\n",
            "Processed 210/468 samples\n",
            "Processed 215/468 samples\n",
            "Processed 220/468 samples\n",
            "Processed 225/468 samples\n",
            "Processed 230/468 samples\n",
            "Processed 235/468 samples\n",
            "Processed 240/468 samples\n",
            "Processed 245/468 samples\n",
            "Processed 250/468 samples\n",
            "Processed 255/468 samples\n",
            "Processed 260/468 samples\n",
            "Processed 265/468 samples\n",
            "Processed 270/468 samples\n",
            "Processed 275/468 samples\n",
            "Processed 280/468 samples\n",
            "Processed 285/468 samples\n",
            "Processed 290/468 samples\n",
            "Processed 295/468 samples\n",
            "Processed 300/468 samples\n",
            "Processed 305/468 samples\n",
            "Processed 310/468 samples\n",
            "Processed 315/468 samples\n",
            "Processed 320/468 samples\n",
            "Processed 325/468 samples\n",
            "Processed 330/468 samples\n",
            "Processed 335/468 samples\n",
            "Processed 340/468 samples\n",
            "Processed 345/468 samples\n",
            "Processed 350/468 samples\n",
            "Processed 355/468 samples\n",
            "Processed 360/468 samples\n",
            "Processed 365/468 samples\n",
            "Processed 370/468 samples\n",
            "Processed 375/468 samples\n",
            "Processed 380/468 samples\n",
            "Processed 385/468 samples\n",
            "Processed 390/468 samples\n",
            "Processed 395/468 samples\n",
            "Processed 400/468 samples\n",
            "Processed 405/468 samples\n",
            "Processed 410/468 samples\n",
            "Processed 415/468 samples\n",
            "Processed 420/468 samples\n",
            "Processed 425/468 samples\n",
            "Processed 430/468 samples\n",
            "Processed 435/468 samples\n",
            "Processed 440/468 samples\n",
            "Processed 445/468 samples\n",
            "Processed 450/468 samples\n",
            "Processed 455/468 samples\n",
            "Processed 460/468 samples\n",
            "Processed 465/468 samples\n",
            "Processed 468/468 samples\n",
            "Optimal temperature: nan\n",
            "Computing predictions...\n",
            "Processed predictions for 5/468 samples\n",
            "Processed predictions for 10/468 samples\n",
            "Processed predictions for 15/468 samples\n",
            "Processed predictions for 20/468 samples\n",
            "Processed predictions for 25/468 samples\n",
            "Processed predictions for 30/468 samples\n",
            "Processed predictions for 35/468 samples\n",
            "Processed predictions for 40/468 samples\n",
            "Processed predictions for 45/468 samples\n",
            "Processed predictions for 50/468 samples\n",
            "Processed predictions for 55/468 samples\n",
            "Processed predictions for 60/468 samples\n",
            "Processed predictions for 65/468 samples\n",
            "Processed predictions for 70/468 samples\n",
            "Processed predictions for 75/468 samples\n",
            "Processed predictions for 80/468 samples\n",
            "Processed predictions for 85/468 samples\n",
            "Processed predictions for 90/468 samples\n",
            "Processed predictions for 95/468 samples\n",
            "Processed predictions for 100/468 samples\n",
            "Processed predictions for 105/468 samples\n",
            "Processed predictions for 110/468 samples\n",
            "Processed predictions for 115/468 samples\n",
            "Processed predictions for 120/468 samples\n",
            "Processed predictions for 125/468 samples\n",
            "Processed predictions for 130/468 samples\n",
            "Processed predictions for 135/468 samples\n",
            "Processed predictions for 140/468 samples\n",
            "Processed predictions for 145/468 samples\n",
            "Processed predictions for 150/468 samples\n",
            "Processed predictions for 155/468 samples\n",
            "Processed predictions for 160/468 samples\n",
            "Processed predictions for 165/468 samples\n",
            "Processed predictions for 170/468 samples\n",
            "Processed predictions for 175/468 samples\n",
            "Processed predictions for 180/468 samples\n",
            "Processed predictions for 185/468 samples\n",
            "Processed predictions for 190/468 samples\n",
            "Processed predictions for 195/468 samples\n",
            "Processed predictions for 200/468 samples\n",
            "Processed predictions for 205/468 samples\n",
            "Processed predictions for 210/468 samples\n",
            "Processed predictions for 215/468 samples\n",
            "Processed predictions for 220/468 samples\n",
            "Processed predictions for 225/468 samples\n",
            "Processed predictions for 230/468 samples\n",
            "Processed predictions for 235/468 samples\n",
            "Processed predictions for 240/468 samples\n",
            "Processed predictions for 245/468 samples\n",
            "Processed predictions for 250/468 samples\n",
            "Processed predictions for 255/468 samples\n",
            "Processed predictions for 260/468 samples\n",
            "Processed predictions for 265/468 samples\n",
            "Processed predictions for 270/468 samples\n",
            "Processed predictions for 275/468 samples\n",
            "Processed predictions for 280/468 samples\n",
            "Processed predictions for 285/468 samples\n",
            "Processed predictions for 290/468 samples\n",
            "Processed predictions for 295/468 samples\n",
            "Processed predictions for 300/468 samples\n",
            "Processed predictions for 305/468 samples\n",
            "Processed predictions for 310/468 samples\n",
            "Processed predictions for 315/468 samples\n",
            "Processed predictions for 320/468 samples\n",
            "Processed predictions for 325/468 samples\n",
            "Processed predictions for 330/468 samples\n",
            "Processed predictions for 335/468 samples\n",
            "Processed predictions for 340/468 samples\n",
            "Processed predictions for 345/468 samples\n",
            "Processed predictions for 350/468 samples\n",
            "Processed predictions for 355/468 samples\n",
            "Processed predictions for 360/468 samples\n",
            "Processed predictions for 365/468 samples\n",
            "Processed predictions for 370/468 samples\n",
            "Processed predictions for 375/468 samples\n",
            "Processed predictions for 380/468 samples\n",
            "Processed predictions for 385/468 samples\n",
            "Processed predictions for 390/468 samples\n",
            "Processed predictions for 395/468 samples\n",
            "Processed predictions for 400/468 samples\n",
            "Processed predictions for 405/468 samples\n",
            "Processed predictions for 410/468 samples\n",
            "Processed predictions for 415/468 samples\n",
            "Processed predictions for 420/468 samples\n",
            "Processed predictions for 425/468 samples\n",
            "Processed predictions for 430/468 samples\n",
            "Processed predictions for 435/468 samples\n",
            "Processed predictions for 440/468 samples\n",
            "Processed predictions for 445/468 samples\n",
            "Processed predictions for 450/468 samples\n",
            "Processed predictions for 455/468 samples\n",
            "Processed predictions for 460/468 samples\n",
            "Processed predictions for 465/468 samples\n",
            "Processed predictions for 468/468 samples\n",
            "Processed predictions for 5/468 samples\n",
            "Processed predictions for 10/468 samples\n",
            "Processed predictions for 15/468 samples\n",
            "Processed predictions for 20/468 samples\n",
            "Processed predictions for 25/468 samples\n",
            "Processed predictions for 30/468 samples\n",
            "Processed predictions for 35/468 samples\n",
            "Processed predictions for 40/468 samples\n",
            "Processed predictions for 45/468 samples\n",
            "Processed predictions for 50/468 samples\n",
            "Processed predictions for 55/468 samples\n",
            "Processed predictions for 60/468 samples\n",
            "Processed predictions for 65/468 samples\n",
            "Processed predictions for 70/468 samples\n",
            "Processed predictions for 75/468 samples\n",
            "Processed predictions for 80/468 samples\n",
            "Processed predictions for 85/468 samples\n",
            "Processed predictions for 90/468 samples\n",
            "Processed predictions for 95/468 samples\n",
            "Processed predictions for 100/468 samples\n",
            "Processed predictions for 105/468 samples\n",
            "Processed predictions for 110/468 samples\n",
            "Processed predictions for 115/468 samples\n",
            "Processed predictions for 120/468 samples\n",
            "Processed predictions for 125/468 samples\n",
            "Processed predictions for 130/468 samples\n",
            "Processed predictions for 135/468 samples\n",
            "Processed predictions for 140/468 samples\n",
            "Processed predictions for 145/468 samples\n",
            "Processed predictions for 150/468 samples\n",
            "Processed predictions for 155/468 samples\n",
            "Processed predictions for 160/468 samples\n",
            "Processed predictions for 165/468 samples\n",
            "Processed predictions for 170/468 samples\n",
            "Processed predictions for 175/468 samples\n",
            "Processed predictions for 180/468 samples\n",
            "Processed predictions for 185/468 samples\n",
            "Processed predictions for 190/468 samples\n",
            "Processed predictions for 195/468 samples\n",
            "Processed predictions for 200/468 samples\n",
            "Processed predictions for 205/468 samples\n",
            "Processed predictions for 210/468 samples\n",
            "Processed predictions for 215/468 samples\n",
            "Processed predictions for 220/468 samples\n",
            "Processed predictions for 225/468 samples\n",
            "Processed predictions for 230/468 samples\n",
            "Processed predictions for 235/468 samples\n",
            "Processed predictions for 240/468 samples\n",
            "Processed predictions for 245/468 samples\n",
            "Processed predictions for 250/468 samples\n",
            "Processed predictions for 255/468 samples\n",
            "Processed predictions for 260/468 samples\n",
            "Processed predictions for 265/468 samples\n",
            "Processed predictions for 270/468 samples\n",
            "Processed predictions for 275/468 samples\n",
            "Processed predictions for 280/468 samples\n",
            "Processed predictions for 285/468 samples\n",
            "Processed predictions for 290/468 samples\n",
            "Processed predictions for 295/468 samples\n",
            "Processed predictions for 300/468 samples\n",
            "Processed predictions for 305/468 samples\n",
            "Processed predictions for 310/468 samples\n",
            "Processed predictions for 315/468 samples\n",
            "Processed predictions for 320/468 samples\n",
            "Processed predictions for 325/468 samples\n",
            "Processed predictions for 330/468 samples\n",
            "Processed predictions for 335/468 samples\n",
            "Processed predictions for 340/468 samples\n",
            "Processed predictions for 345/468 samples\n",
            "Processed predictions for 350/468 samples\n",
            "Processed predictions for 355/468 samples\n",
            "Processed predictions for 360/468 samples\n",
            "Processed predictions for 365/468 samples\n",
            "Processed predictions for 370/468 samples\n",
            "Processed predictions for 375/468 samples\n",
            "Processed predictions for 380/468 samples\n",
            "Processed predictions for 385/468 samples\n",
            "Processed predictions for 390/468 samples\n",
            "Processed predictions for 395/468 samples\n",
            "Processed predictions for 400/468 samples\n",
            "Processed predictions for 405/468 samples\n",
            "Processed predictions for 410/468 samples\n",
            "Processed predictions for 415/468 samples\n",
            "Processed predictions for 420/468 samples\n",
            "Processed predictions for 425/468 samples\n",
            "Processed predictions for 430/468 samples\n",
            "Processed predictions for 435/468 samples\n",
            "Processed predictions for 440/468 samples\n",
            "Processed predictions for 445/468 samples\n",
            "Processed predictions for 450/468 samples\n",
            "Processed predictions for 455/468 samples\n",
            "Processed predictions for 460/468 samples\n",
            "Processed predictions for 465/468 samples\n",
            "Processed predictions for 468/468 samples\n",
            "ECE Before Calibration: 0.112\n",
            "ECE After Calibration: 0.000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rzXybuVR34nY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}