{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ct6VD809W6Dd"
      },
      "source": [
        "## Guidance on configuring LoRA and QLoRA\n",
        "- Both LoRA and QLoRA use the same Trainer API and TrainingArguments structure, but there are a few key configuration differences we need to consider:\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DFZbsnDyX8bd"
      },
      "source": [
        "### 1. Model Loading and Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ADNe9erFYASC"
      },
      "source": [
        "#### `LoRA:`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ijksnpHnYIU9"
      },
      "source": [
        "How to load:\n",
        "- Load the model in standard (usually float32 or fp16) precision using `AutoModelForTokenClassification.from_pretrained(...)`.\n",
        "\n",
        "Preparation:\n",
        "- Wrap the model with LoRA adapters using `get_peft_model()`.\n",
        "\n",
        "Key Point:\n",
        "- You’re not changing the model’s precision—only adding a small set of trainable parameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6IZZ8HO1YDHY"
      },
      "source": [
        "#### `QLoRA:`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gxRtf0MJYGY9"
      },
      "source": [
        "How to load:\n",
        "- You need to load the model with 4-bit quantization. This involves passing a quantization configuration (usually via a parameter like `quantization_config`) and specifying a device map. For example, you might use a configuration that tells the model to load in 4-bit mode.\n",
        "\n",
        "Preparation:\n",
        "- After loading, run the model through `prepare_model_for_kbit_training()` (from PEFT) before wrapping it with LoRA adapters.\n",
        "\n",
        "Key Point:\n",
        "- QLoRA reduces the memory footprint by quantizing the base model to 4-bit precision. This is the major difference—you’re effectively training a quantized version of the model with LoRA adapters on top."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D_mFjKCEYLmJ"
      },
      "source": [
        "### 2. TrainingArguments and Trainer Settings\n",
        "- While both approaches use TrainingArguments and Trainer, you typically want to tweak the hyperparameters to match the training dynamics:"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HuGZlgEOYPR0"
      },
      "source": [
        "#### LoRA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5y5KlLWmYQ_i"
      },
      "source": [
        "Learning Rate:\n",
        "- You might use a higher learning rate than in full fine-tuning because only a small set of adapter parameters is updated.\n",
        "\n",
        "Batch Size & Gradient Accumulation:\n",
        "- Because the model isn’t quantized, you might use a moderate batch size (or use gradient accumulation if your GPU memory is limited).\n",
        "\n",
        "Precision:\n",
        "- You can decide whether to use fp16 or full precision based on your hardware; fp16 is common to speed up training.\n",
        "\n",
        "- Example snippet:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f14jGkIkW26g"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForTokenClassification\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "model = AutoModelForTokenClassification.from_pretrained(model_ckpt, num_labels=num_labels)\n",
        "lora_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.1,\n",
        "    target_modules=[\"query\", \"value\"],\n",
        "    bias=\"none\",\n",
        "    task_type=\"TOKEN_CLS\",\n",
        ")\n",
        "lora_model = get_peft_model(model, lora_config)\n",
        "\n",
        "training_args_lora = TrainingArguments(\n",
        "    output_dir=\"./xlm-roberta-ner-lora\",\n",
        "    num_train_epochs=2,\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=16,\n",
        "    learning_rate=2e-4,\n",
        "    weight_decay=0.001,\n",
        "    fp16=True,\n",
        "    logging_steps=1,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    warmup_ratio=0.03,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=lora_model,\n",
        "    args=training_args_lora,\n",
        "    train_dataset=tokenized_dataset[\"train\"],\n",
        "    eval_dataset=tokenized_dataset[\"validation\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "90iXFoV8YYVe"
      },
      "source": [
        "#### QLoRA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wX-0ZV6oYbJ8"
      },
      "source": [
        "Quantization Config:\n",
        "- The model loading step must include the quantization settings. This isn’t part of TrainingArguments but is essential in your model initialization.\n",
        "\n",
        "Model Preparation:\n",
        "- After loading in quantized mode, run the model through prepare_model_for_kbit_training() to ready it for fine-tuning.\n",
        "\n",
        "Learning Rate & Batch Size:\n",
        "- You may still need to adjust the learning rate for the adapter layers. Since the base model is quantized (and thus uses less memory), you might be able to increase your effective batch size.\n",
        "\n",
        "Precision & Optimizer:\n",
        "- Even with 4-bit quantization, you’ll often use fp16 for the LoRA layers. Also, sometimes a specific optimizer (e.g., \"adamw_torch\") is preferred.\n",
        "\n",
        "- Example snippet:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zkfxu2ElYZuL"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForTokenClassification\n",
        "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n",
        "from transformers import BitsAndBytesConfig\n",
        "\n",
        "# Define a quantization configuration (this API may vary slightly)\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=\"float16\",  # or torch.float16\n",
        "    bnb_4bit_quant_type=\"nf4\",          # choose appropriate quantization type\n",
        ")\n",
        "\n",
        "# Load the model with 4-bit quantization\n",
        "model = AutoModelForTokenClassification.from_pretrained(\n",
        "    model_ckpt,\n",
        "    quantization_config=quantization_config,\n",
        "    num_labels=num_labels,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# Prepare model for k-bit training\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "# Apply LoRA on the quantized model\n",
        "lora_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.1,\n",
        "    target_modules=[\"query\", \"value\"],\n",
        "    bias=\"none\",\n",
        "    task_type=\"TOKEN_CLS\",\n",
        ")\n",
        "qlora_model = get_peft_model(model, lora_config)\n",
        "\n",
        "training_args_qlora = TrainingArguments(\n",
        "    output_dir=\"./xlm-roberta-ner-qlora\",\n",
        "    num_train_epochs=2,\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=16,\n",
        "    learning_rate=2e-4,\n",
        "    weight_decay=0.001,\n",
        "    fp16=True,\n",
        "    logging_steps=1,\n",
        "    lr_scheduler_type=\"cosine\",\n",
        "    warmup_ratio=0.03,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=qlora_model,\n",
        "    args=training_args_qlora,\n",
        "    train_dataset=tokenized_dataset[\"train\"],\n",
        "    eval_dataset=tokenized_dataset[\"validation\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59C2OFB5Yjlw"
      },
      "source": [
        "### 3. Key Points to Ensure You're Using the Correct Method"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bmSAyaVsYm6w"
      },
      "source": [
        "#### LoRA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0PRSRLlsYnzu"
      },
      "source": [
        "- Load the model normally (no quantization).\n",
        "- Wrap with get_peft_model() after defining your LoRA configuration.\n",
        "- Use training arguments tuned for LoRA (often a higher learning rate and smaller per-device batch size with gradient accumulation)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJrPOJaIYp9L"
      },
      "source": [
        "#### QLoRA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RtlTZTJcYq4s"
      },
      "source": [
        "- Load with 4-bit quantization:\n",
        "- Include a quantization config in your from_pretrained call.\n",
        "\n",
        "Prepare the model:\n",
        "- Run it through prepare_model_for_kbit_training() before applying LoRA.\n",
        "Wrap with get_peft_model() as usual.\n",
        "\n",
        "TrainingArguments:\n",
        "- Often similar in structure to LoRA’s but ensure you're using fp16 (or bf16 if supported) and other hyperparameters might be slightly adjusted to reflect the quantized setup.\n",
        "\n",
        "Double-check BitsAndBytes:\n",
        "- Make sure you have the latest version of the bitsandbytes library installed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6auI7X4wYv1U"
      },
      "source": [
        "### 4. Summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- The Trainer and TrainingArguments structure remains similar for both methods.\n",
        "- The major differences lie in the model loading and preparation steps.\n",
        "- For LoRA, you load normally and apply adapter layers.\n",
        "- For QLoRA, you load with quantization settings, prepare the model for k-bit training, then apply the adapter layers.\n",
        "\n",
        "Hyperparameters might also need tuning:\n",
        "- LoRA and QLoRA often benefit from a higher learning rate for the adapter layers, smaller effective batch sizes (or gradient accumulation to simulate larger batches), and potentially different scheduler settings.\n",
        "- By ensuring these steps, you can be confident that your code is running the intended method—whether it’s LoRA or QLoRA."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
